{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\Anaconda3\\lib\\site-packages\\pydub\\utils.py:165: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import *\n",
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "from PIL import Image,ImageTk\n",
    "import os\n",
    "import pyttsx3\n",
    "from pyttsx3 import voice\n",
    "import speech_recognition as sr\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import soundfile as sf\n",
    "from pyAudioAnalysis import audioSegmentation as aS\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1107 15:44:40.992156 22232 deprecation_wrapper.py:119] From C:\\Users\\kumar\\Anaconda3\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "W1107 15:44:40.994580 22232 deprecation_wrapper.py:119] From C:\\Users\\kumar\\Anaconda3\\lib\\site-packages\\tflearn\\helpers\\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "W1107 15:44:41.014549 22232 deprecation_wrapper.py:119] From C:\\Users\\kumar\\Anaconda3\\lib\\site-packages\\tflearn\\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W1107 15:44:41.028537 22232 deprecation_wrapper.py:119] From C:\\Users\\kumar\\Anaconda3\\lib\\site-packages\\tflearn\\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "W1107 15:44:41.040934 22232 deprecation_wrapper.py:119] From C:\\Users\\kumar\\Anaconda3\\lib\\site-packages\\tflearn\\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "W1107 15:44:41.041932 22232 deprecation_wrapper.py:119] From C:\\Users\\kumar\\Anaconda3\\lib\\site-packages\\tflearn\\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#It contains text processing libraries for tokenization, \n",
    "#parsing, classification, stemming, tagging and semantic reasoning.\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "#import curses\n",
    "import string\n",
    "import numpy\n",
    "import numpy as np\n",
    "import tflearn\n",
    "\n",
    "#TFlearn is a modular and transparent deep learning library built on top of Tensorflow. \n",
    "#It was designed to provide a higher-level API to TensorFlow \n",
    "#in order to facilitate and speed-up experimentations, while remaining fully transparent and compatible with it.\n",
    "import tensorflow as tf\n",
    "import random\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# import our chat-bot intents file\n",
    "import json\n",
    "#open the exact location   of CORPUS \n",
    "with open('docman.json') as jd:\n",
    "    intents = json.load(jd)\n",
    "import speech_recognition as sr\n",
    "r=sr.Recognizer()\n",
    "# importing the pyttsx library\n",
    "import pyttsx3\n",
    "# initialisation \n",
    "engine = pyttsx3.init() \n",
    "engine.setProperty('rate', 160)\n",
    "voices=engine.getProperty('voices')\n",
    "engine.setProperty('voice', voices[1].id)\n",
    "def bot_speaking(message): \n",
    "    # testing \n",
    "    engine.say(message)\n",
    "    engine.runAndWait() \n",
    "def get_input():\n",
    "    with sr.Microphone() as source:\n",
    "        #print(\"Say something!!!\");\n",
    "        output.config(text=\"Listening...\")\n",
    "        bot_speaking(\"Speak now\")\n",
    "        \n",
    "        audio=r.listen(source,timeout=0)\n",
    "        #print(\"Perfect, Thanks!\")\n",
    "        bot_speaking(\"Perfect\")\n",
    "    try:\n",
    "        msg=r.recognize_google(audio)\n",
    "        output.config(text=\"YOU: \"+msg); #r.recognize(audio,language='hi-IN')\n",
    "        #bot_speaking(\"you said \"+msg)\n",
    "        return msg\n",
    "    except:\n",
    "        #print(\"Dude it's not working :(\")\n",
    "        bot_speaking(\"Sorry mate  check your Internet\")\n",
    "        output.config(tetx=\"WARNING : Check your Internet and Try again!!\")\n",
    "        pass;\n",
    "nltk.download('punkt')\n",
    "#Punkt Sentence Tokenizer. This tokenizer divides a text into a list of sentences,\n",
    "#by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.\n",
    "#It must be trained on a large collection of plaintext in the target language before it can be used.\n",
    "#Create words, classes and documents\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?','!']\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']: # PATTERN IS USER QUESTION\n",
    "      \n",
    "        # tokenize each word in the sentence\n",
    "        \n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # add to our words list\n",
    "        words.extend(w)\n",
    "        # add to documents in our corpus        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "\n",
    "            documents.append((w, intent['tag']))\n",
    "            classes.append(intent['tag'])\n",
    "words = sorted(list(set([stemmer.stem(w.lower()) for w in words if w not in ignore_words])))\n",
    "\n",
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []    #bag of words which is a representaion of text that describes the occurence of wordss\n",
    "    #with in a doccument\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words: #if the word appears in the pattern put 1 else 0 \n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    print(bag)\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "random.shuffle(training)\n",
    "training = np.array(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: B9I1XH\n",
      "Log directory: tflearn_logs/\n",
      "---------------------------------\n",
      "Training samples: 11\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1  | time: 0.216s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 11/11\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m2.15810\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 002 | loss: 2.15810 - acc: 0.0000 -- iter: 11/11\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m2.35420\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 003 | loss: 2.35420 - acc: 0.2975 -- iter: 11/11\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m2.38679\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 004 | loss: 2.38679 - acc: 0.4153 -- iter: 11/11\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m2.39422\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 005 | loss: 2.39422 - acc: 0.3795 -- iter: 11/11\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m2.39626\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 006 | loss: 2.39626 - acc: 0.3109 -- iter: 11/11\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m2.39685\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 007 | loss: 2.39685 - acc: 0.2880 -- iter: 11/11\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m2.39698\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 008 | loss: 2.39698 - acc: 0.4328 -- iter: 11/11\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m2.39693\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 009 | loss: 2.39693 - acc: 0.4443 -- iter: 11/11\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m2.39680\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 010 | loss: 2.39680 - acc: 0.4494 -- iter: 11/11\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m2.39663\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 011 | loss: 2.39663 - acc: 0.4519 -- iter: 11/11\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m2.39642\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 012 | loss: 2.39642 - acc: 0.4531 -- iter: 11/11\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m2.39617\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 013 | loss: 2.39617 - acc: 0.5316 -- iter: 11/11\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m2.39590\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 014 | loss: 2.39590 - acc: 0.6117 -- iter: 11/11\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m2.39558\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 015 | loss: 2.39558 - acc: 0.6925 -- iter: 11/11\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m2.39523\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 016 | loss: 2.39523 - acc: 0.7737 -- iter: 11/11\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m2.39483\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 017 | loss: 2.39483 - acc: 0.8552 -- iter: 11/11\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m2.39439\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 018 | loss: 2.39439 - acc: 0.9053 -- iter: 11/11\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m2.39390\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 019 | loss: 2.39390 - acc: 0.9066 -- iter: 11/11\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m2.39335\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 020 | loss: 2.39335 - acc: 0.8197 -- iter: 11/11\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m2.39274\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 021 | loss: 2.39274 - acc: 0.7346 -- iter: 11/11\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m2.39207\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 022 | loss: 2.39207 - acc: 0.6506 -- iter: 11/11\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m2.39133\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 023 | loss: 2.39133 - acc: 0.5937 -- iter: 11/11\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m2.39051\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 024 | loss: 2.39051 - acc: 0.5545 -- iter: 11/11\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m2.38961\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 025 | loss: 2.38961 - acc: 0.5273 -- iter: 11/11\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m2.38862\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 026 | loss: 2.38862 - acc: 0.5561 -- iter: 11/11\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m2.38754\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 027 | loss: 2.38754 - acc: 0.6002 -- iter: 11/11\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m2.38635\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 028 | loss: 2.38635 - acc: 0.6319 -- iter: 11/11\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m2.38505\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 029 | loss: 2.38505 - acc: 0.6551 -- iter: 11/11\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m2.38364\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 030 | loss: 2.38364 - acc: 0.6507 -- iter: 11/11\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m2.38675\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 031 | loss: 2.38675 - acc: 0.5425 -- iter: 11/11\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m2.38411\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 032 | loss: 2.38411 - acc: 0.5022 -- iter: 11/11\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m2.38164\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 033 | loss: 2.38164 - acc: 0.4718 -- iter: 11/11\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m2.37924\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 034 | loss: 2.37924 - acc: 0.4486 -- iter: 11/11\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m2.37685\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 035 | loss: 2.37685 - acc: 0.4308 -- iter: 11/11\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m2.37442\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 036 | loss: 2.37442 - acc: 0.4357 -- iter: 11/11\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m2.37191\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 037 | loss: 2.37191 - acc: 0.4395 -- iter: 11/11\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m2.36928\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 038 | loss: 2.36928 - acc: 0.4424 -- iter: 11/11\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m2.36650\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 039 | loss: 2.36650 - acc: 0.4447 -- iter: 11/11\n",
      "--\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m2.36356\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 040 | loss: 2.36356 - acc: 0.4466 -- iter: 11/11\n",
      "--\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m2.36044\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 041 | loss: 2.36044 - acc: 0.4480 -- iter: 11/11\n",
      "--\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m2.35711\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 042 | loss: 2.35711 - acc: 0.4492 -- iter: 11/11\n",
      "--\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m2.35355\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 043 | loss: 2.35355 - acc: 0.4502 -- iter: 11/11\n",
      "--\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m2.35946\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 044 | loss: 2.35946 - acc: 0.4037 -- iter: 11/11\n",
      "--\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m2.35390\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 045 | loss: 2.35390 - acc: 0.4123 -- iter: 11/11\n",
      "--\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m2.35846\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 046 | loss: 2.35846 - acc: 0.3891 -- iter: 11/11\n",
      "--\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m2.35157\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 047 | loss: 2.35157 - acc: 0.3849 -- iter: 11/11\n",
      "--\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m2.34503\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 048 | loss: 2.34503 - acc: 0.3815 -- iter: 11/11\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m2.33869\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 049 | loss: 2.33869 - acc: 0.3787 -- iter: 11/11\n",
      "--\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m2.33244\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 050 | loss: 2.33244 - acc: 0.3763 -- iter: 11/11\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m2.32620\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 051 | loss: 2.32620 - acc: 0.3744 -- iter: 11/11\n",
      "--\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m2.31989\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 052 | loss: 2.31989 - acc: 0.3728 -- iter: 11/11\n",
      "--\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m2.31345\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 053 | loss: 2.31345 - acc: 0.3714 -- iter: 11/11\n",
      "--\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m2.30683\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 054 | loss: 2.30683 - acc: 0.3703 -- iter: 11/11\n",
      "--\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m2.30000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 055 | loss: 2.30000 - acc: 0.3694 -- iter: 11/11\n",
      "--\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m2.29292\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 056 | loss: 2.29292 - acc: 0.3685 -- iter: 11/11\n",
      "--\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m2.28557\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 057 | loss: 2.28557 - acc: 0.3679 -- iter: 11/11\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m2.27792\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 058 | loss: 2.27792 - acc: 0.3673 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m2.26995\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 059 | loss: 2.26995 - acc: 0.3668 -- iter: 11/11\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m2.26166\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 060 | loss: 2.26166 - acc: 0.3664 -- iter: 11/11\n",
      "--\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m2.25302\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 061 | loss: 2.25302 - acc: 0.3660 -- iter: 11/11\n",
      "--\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m2.24404\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 062 | loss: 2.24404 - acc: 0.3657 -- iter: 11/11\n",
      "--\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m2.23470\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 063 | loss: 2.23470 - acc: 0.3655 -- iter: 11/11\n",
      "--\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m2.22501\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 064 | loss: 2.22501 - acc: 0.3652 -- iter: 11/11\n",
      "--\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m2.21496\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 065 | loss: 2.21496 - acc: 0.3650 -- iter: 11/11\n",
      "--\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m2.20456\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 066 | loss: 2.20456 - acc: 0.3649 -- iter: 11/11\n",
      "--\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m2.19381\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 067 | loss: 2.19381 - acc: 0.3647 -- iter: 11/11\n",
      "--\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m2.18272\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 068 | loss: 2.18272 - acc: 0.3646 -- iter: 11/11\n",
      "--\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m2.17130\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 069 | loss: 2.17130 - acc: 0.3645 -- iter: 11/11\n",
      "--\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m2.15956\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 070 | loss: 2.15956 - acc: 0.3749 -- iter: 11/11\n",
      "--\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m2.14752\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 071 | loss: 2.14752 - acc: 0.3839 -- iter: 11/11\n",
      "--\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m2.13520\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 072 | loss: 2.13520 - acc: 0.3919 -- iter: 11/11\n",
      "--\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m2.12262\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 073 | loss: 2.12262 - acc: 0.3988 -- iter: 11/11\n",
      "--\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m2.10980\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 074 | loss: 2.10980 - acc: 0.4050 -- iter: 11/11\n",
      "--\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m2.09676\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 075 | loss: 2.09676 - acc: 0.4103 -- iter: 11/11\n",
      "--\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m2.08353\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 076 | loss: 2.08353 - acc: 0.4151 -- iter: 11/11\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m2.07014\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 077 | loss: 2.07014 - acc: 0.4193 -- iter: 11/11\n",
      "--\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m2.05662\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 078 | loss: 2.05662 - acc: 0.4229 -- iter: 11/11\n",
      "--\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m2.04300\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 079 | loss: 2.04300 - acc: 0.4262 -- iter: 11/11\n",
      "--\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m2.02931\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 080 | loss: 2.02931 - acc: 0.4291 -- iter: 11/11\n",
      "--\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m2.01557\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 081 | loss: 2.01557 - acc: 0.4317 -- iter: 11/11\n",
      "--\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m2.00182\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 082 | loss: 2.00182 - acc: 0.4340 -- iter: 11/11\n",
      "--\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m1.98795\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 083 | loss: 1.98795 - acc: 0.4360 -- iter: 11/11\n",
      "--\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m1.97398\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 084 | loss: 1.97398 - acc: 0.4379 -- iter: 11/11\n",
      "--\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m1.95996\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 085 | loss: 1.95996 - acc: 0.4395 -- iter: 11/11\n",
      "--\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m1.94594\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 086 | loss: 1.94594 - acc: 0.4410 -- iter: 11/11\n",
      "--\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m1.93195\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 087 | loss: 1.93195 - acc: 0.4333 -- iter: 11/11\n",
      "--\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m1.91803\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 088 | loss: 1.91803 - acc: 0.4263 -- iter: 11/11\n",
      "--\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m1.90422\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 089 | loss: 1.90422 - acc: 0.4201 -- iter: 11/11\n",
      "--\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m1.89053\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 090 | loss: 1.89053 - acc: 0.4144 -- iter: 11/11\n",
      "--\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m1.87699\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 091 | loss: 1.87699 - acc: 0.4093 -- iter: 11/11\n",
      "--\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m1.86364\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 092 | loss: 1.86364 - acc: 0.4048 -- iter: 11/11\n",
      "--\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m1.85049\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 093 | loss: 1.85049 - acc: 0.4007 -- iter: 11/11\n",
      "--\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m1.83755\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 094 | loss: 1.83755 - acc: 0.3970 -- iter: 11/11\n",
      "--\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m1.82485\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 095 | loss: 1.82485 - acc: 0.3845 -- iter: 11/11\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m1.81241\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 096 | loss: 1.81241 - acc: 0.3734 -- iter: 11/11\n",
      "--\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m1.80022\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 097 | loss: 1.80022 - acc: 0.3633 -- iter: 11/11\n",
      "--\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m1.78830\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 098 | loss: 1.78830 - acc: 0.3542 -- iter: 11/11\n",
      "--\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m1.77666\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 099 | loss: 1.77666 - acc: 0.3461 -- iter: 11/11\n",
      "--\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m1.76530\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 100 | loss: 1.76530 - acc: 0.3387 -- iter: 11/11\n",
      "--\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m1.75423\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 101 | loss: 1.75423 - acc: 0.3321 -- iter: 11/11\n",
      "--\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m1.74345\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 102 | loss: 1.74345 - acc: 0.3262 -- iter: 11/11\n",
      "--\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m1.73297\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 103 | loss: 1.73297 - acc: 0.3209 -- iter: 11/11\n",
      "--\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m1.72278\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 104 | loss: 1.72278 - acc: 0.3160 -- iter: 11/11\n",
      "--\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m1.71289\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 105 | loss: 1.71289 - acc: 0.3117 -- iter: 11/11\n",
      "--\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m1.70329\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 106 | loss: 1.70329 - acc: 0.3078 -- iter: 11/11\n",
      "--\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m1.69398\u001b[0m\u001b[0m | time: 0.332s\n",
      "| Adam | epoch: 107 | loss: 1.69398 - acc: 0.3043 -- iter: 11/11\n",
      "--\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m1.92411\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 108 | loss: 1.92411 - acc: 0.2739 -- iter: 11/11\n",
      "--\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m1.89179\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 109 | loss: 1.89179 - acc: 0.2738 -- iter: 11/11\n",
      "--\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m1.86242\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 110 | loss: 1.86242 - acc: 0.2737 -- iter: 11/11\n",
      "--\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m1.83570\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 111 | loss: 1.83570 - acc: 0.2827 -- iter: 11/11\n",
      "--\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m1.81135\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 112 | loss: 1.81135 - acc: 0.2998 -- iter: 11/11\n",
      "--\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m1.78912\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 113 | loss: 1.78912 - acc: 0.3153 -- iter: 11/11\n",
      "--\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m1.76878\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 114 | loss: 1.76878 - acc: 0.3292 -- iter: 11/11\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m1.75015\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 115 | loss: 1.75015 - acc: 0.3418 -- iter: 11/11\n",
      "--\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m1.73303\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 116 | loss: 1.73303 - acc: 0.3440 -- iter: 11/11\n",
      "--\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m1.71727\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 117 | loss: 1.71727 - acc: 0.3459 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m1.70272\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 118 | loss: 1.70272 - acc: 0.3568 -- iter: 11/11\n",
      "--\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m1.68926\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 119 | loss: 1.68926 - acc: 0.3666 -- iter: 11/11\n",
      "--\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m1.67677\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 120 | loss: 1.67677 - acc: 0.3754 -- iter: 11/11\n",
      "--\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m1.66514\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 121 | loss: 1.66514 - acc: 0.3833 -- iter: 11/11\n",
      "--\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m1.65429\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 122 | loss: 1.65429 - acc: 0.3904 -- iter: 11/11\n",
      "--\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m1.64413\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 123 | loss: 1.64413 - acc: 0.4059 -- iter: 11/11\n",
      "--\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m1.63459\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 124 | loss: 1.63459 - acc: 0.4199 -- iter: 11/11\n",
      "--\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m1.62561\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 125 | loss: 1.62561 - acc: 0.4324 -- iter: 11/11\n",
      "--\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m1.78637\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 126 | loss: 1.78637 - acc: 0.3892 -- iter: 11/11\n",
      "--\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m1.76161\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 127 | loss: 1.76161 - acc: 0.4048 -- iter: 11/11\n",
      "--\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m1.73910\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 128 | loss: 1.73910 - acc: 0.4189 -- iter: 11/11\n",
      "--\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m1.71861\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 129 | loss: 1.71861 - acc: 0.4315 -- iter: 11/11\n",
      "--\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m1.69992\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 130 | loss: 1.69992 - acc: 0.4429 -- iter: 11/11\n",
      "--\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m1.68283\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 131 | loss: 1.68283 - acc: 0.4623 -- iter: 11/11\n",
      "--\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m1.66718\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 132 | loss: 1.66718 - acc: 0.4797 -- iter: 11/11\n",
      "--\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m1.65280\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 133 | loss: 1.65280 - acc: 0.4953 -- iter: 11/11\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m1.63956\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 134 | loss: 1.63956 - acc: 0.5094 -- iter: 11/11\n",
      "--\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m1.62733\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 135 | loss: 1.62733 - acc: 0.5312 -- iter: 11/11\n",
      "--\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m1.61600\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 136 | loss: 1.61600 - acc: 0.5508 -- iter: 11/11\n",
      "--\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m1.60547\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 137 | loss: 1.60547 - acc: 0.5685 -- iter: 11/11\n",
      "--\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m1.59565\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 138 | loss: 1.59565 - acc: 0.5844 -- iter: 11/11\n",
      "--\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m1.58646\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 139 | loss: 1.58646 - acc: 0.5986 -- iter: 11/11\n",
      "--\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m1.57782\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 140 | loss: 1.57782 - acc: 0.6115 -- iter: 11/11\n",
      "--\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m1.56967\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 141 | loss: 1.56967 - acc: 0.6231 -- iter: 11/11\n",
      "--\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m1.56196\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 142 | loss: 1.56196 - acc: 0.6335 -- iter: 11/11\n",
      "--\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m1.55463\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 143 | loss: 1.55463 - acc: 0.6429 -- iter: 11/11\n",
      "--\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m1.54763\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 144 | loss: 1.54763 - acc: 0.6513 -- iter: 11/11\n",
      "--\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m1.54093\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 145 | loss: 1.54093 - acc: 0.6589 -- iter: 11/11\n",
      "--\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m1.53448\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 146 | loss: 1.53448 - acc: 0.6658 -- iter: 11/11\n",
      "--\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m1.52825\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 147 | loss: 1.52825 - acc: 0.6628 -- iter: 11/11\n",
      "--\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m1.52220\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 148 | loss: 1.52220 - acc: 0.6602 -- iter: 11/11\n",
      "--\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m1.51633\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 149 | loss: 1.51633 - acc: 0.6578 -- iter: 11/11\n",
      "--\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m1.51058\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 150 | loss: 1.51058 - acc: 0.6556 -- iter: 11/11\n",
      "--\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m1.50496\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 151 | loss: 1.50496 - acc: 0.6537 -- iter: 11/11\n",
      "--\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m1.72196\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 152 | loss: 1.72196 - acc: 0.5974 -- iter: 11/11\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m1.69448\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 153 | loss: 1.69448 - acc: 0.6013 -- iter: 11/11\n",
      "--\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m1.87622\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 154 | loss: 1.87622 - acc: 0.5594 -- iter: 11/11\n",
      "--\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m1.83293\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 155 | loss: 1.83293 - acc: 0.5671 -- iter: 11/11\n",
      "--\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m1.79384\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 156 | loss: 1.79384 - acc: 0.5740 -- iter: 11/11\n",
      "--\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m1.75850\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 157 | loss: 1.75850 - acc: 0.5712 -- iter: 11/11\n",
      "--\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m1.96971\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 158 | loss: 1.96971 - acc: 0.5140 -- iter: 11/11\n",
      "--\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m1.91657\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 159 | loss: 1.91657 - acc: 0.5172 -- iter: 11/11\n",
      "--\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m1.86868\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 160 | loss: 1.86868 - acc: 0.5109 -- iter: 11/11\n",
      "--\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m1.82548\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 161 | loss: 1.82548 - acc: 0.5144 -- iter: 11/11\n",
      "--\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m1.78648\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 162 | loss: 1.78648 - acc: 0.5266 -- iter: 11/11\n",
      "--\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m1.75121\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 163 | loss: 1.75121 - acc: 0.5375 -- iter: 11/11\n",
      "--\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m1.86160\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 164 | loss: 1.86160 - acc: 0.4929 -- iter: 11/11\n",
      "--\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m1.81854\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 165 | loss: 1.81854 - acc: 0.5072 -- iter: 11/11\n",
      "--\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m1.95028\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 166 | loss: 1.95028 - acc: 0.4656 -- iter: 11/11\n",
      "--\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m1.89820\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 167 | loss: 1.89820 - acc: 0.4827 -- iter: 11/11\n",
      "--\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m1.85126\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 168 | loss: 1.85126 - acc: 0.4980 -- iter: 11/11\n",
      "--\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m1.80890\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 169 | loss: 1.80890 - acc: 0.5119 -- iter: 11/11\n",
      "--\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m1.77064\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 170 | loss: 1.77064 - acc: 0.5243 -- iter: 11/11\n",
      "--\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m1.73601\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 171 | loss: 1.73601 - acc: 0.5355 -- iter: 11/11\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m1.70462\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 172 | loss: 1.70462 - acc: 0.5456 -- iter: 11/11\n",
      "--\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m1.67611\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 173 | loss: 1.67611 - acc: 0.5547 -- iter: 11/11\n",
      "--\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m1.65016\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 174 | loss: 1.65016 - acc: 0.5629 -- iter: 11/11\n",
      "--\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m1.62649\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 175 | loss: 1.62649 - acc: 0.5702 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m1.60484\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 176 | loss: 1.60484 - acc: 0.5768 -- iter: 11/11\n",
      "--\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m1.58497\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 177 | loss: 1.58497 - acc: 0.5828 -- iter: 11/11\n",
      "--\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m1.56670\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 178 | loss: 1.56670 - acc: 0.5881 -- iter: 11/11\n",
      "--\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m1.54984\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 179 | loss: 1.54984 - acc: 0.5930 -- iter: 11/11\n",
      "--\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m1.53423\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 180 | loss: 1.53423 - acc: 0.6064 -- iter: 11/11\n",
      "--\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m1.51972\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 181 | loss: 1.51972 - acc: 0.6185 -- iter: 11/11\n",
      "--\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m1.50618\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 182 | loss: 1.50618 - acc: 0.6294 -- iter: 11/11\n",
      "--\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m1.49352\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 183 | loss: 1.49352 - acc: 0.6391 -- iter: 11/11\n",
      "--\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m1.48161\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 184 | loss: 1.48161 - acc: 0.6571 -- iter: 11/11\n",
      "--\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m1.47038\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 185 | loss: 1.47038 - acc: 0.6732 -- iter: 11/11\n",
      "--\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m1.45974\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 186 | loss: 1.45974 - acc: 0.6877 -- iter: 11/11\n",
      "--\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m1.44962\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 187 | loss: 1.44962 - acc: 0.7007 -- iter: 11/11\n",
      "--\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m1.43995\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 188 | loss: 1.43995 - acc: 0.7125 -- iter: 11/11\n",
      "--\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m1.43069\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 189 | loss: 1.43069 - acc: 0.7230 -- iter: 11/11\n",
      "--\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m1.42178\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 190 | loss: 1.42178 - acc: 0.7326 -- iter: 11/11\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m1.41318\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 191 | loss: 1.41318 - acc: 0.7411 -- iter: 11/11\n",
      "--\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m1.58637\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 192 | loss: 1.58637 - acc: 0.6852 -- iter: 11/11\n",
      "--\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m1.56034\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 193 | loss: 1.56034 - acc: 0.6985 -- iter: 11/11\n",
      "--\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m1.78431\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 194 | loss: 1.78431 - acc: 0.6559 -- iter: 11/11\n",
      "--\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m1.73792\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 195 | loss: 1.73792 - acc: 0.6721 -- iter: 11/11\n",
      "--\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m1.69595\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 196 | loss: 1.69595 - acc: 0.6776 -- iter: 11/11\n",
      "--\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m1.65792\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 197 | loss: 1.65792 - acc: 0.6826 -- iter: 11/11\n",
      "--\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m1.62340\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 198 | loss: 1.62340 - acc: 0.6871 -- iter: 11/11\n",
      "--\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m1.59202\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 199 | loss: 1.59202 - acc: 0.6911 -- iter: 11/11\n",
      "--\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m1.56341\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 200 | loss: 1.56341 - acc: 0.6947 -- iter: 11/11\n",
      "--\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m1.53728\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 201 | loss: 1.53728 - acc: 0.6980 -- iter: 11/11\n",
      "--\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m1.51336\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 202 | loss: 1.51336 - acc: 0.7009 -- iter: 11/11\n",
      "--\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m1.49139\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 203 | loss: 1.49139 - acc: 0.7035 -- iter: 11/11\n",
      "--\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m1.47116\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 204 | loss: 1.47116 - acc: 0.7059 -- iter: 11/11\n",
      "--\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m1.45247\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 205 | loss: 1.45247 - acc: 0.7080 -- iter: 11/11\n",
      "--\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m1.43515\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 206 | loss: 1.43515 - acc: 0.7100 -- iter: 11/11\n",
      "--\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m1.41905\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 207 | loss: 1.41905 - acc: 0.7117 -- iter: 11/11\n",
      "--\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m1.40402\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 208 | loss: 1.40402 - acc: 0.7133 -- iter: 11/11\n",
      "--\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m1.38995\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 209 | loss: 1.38995 - acc: 0.7147 -- iter: 11/11\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m1.37673\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 210 | loss: 1.37673 - acc: 0.7159 -- iter: 11/11\n",
      "--\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m1.36425\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 211 | loss: 1.36425 - acc: 0.7171 -- iter: 11/11\n",
      "--\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m1.55320\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 212 | loss: 1.55320 - acc: 0.6544 -- iter: 11/11\n",
      "--\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m1.52213\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 213 | loss: 1.52213 - acc: 0.6617 -- iter: 11/11\n",
      "--\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m1.49376\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 214 | loss: 1.49376 - acc: 0.6683 -- iter: 11/11\n",
      "--\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m1.46782\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 215 | loss: 1.46782 - acc: 0.6742 -- iter: 11/11\n",
      "--\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m1.44402\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 216 | loss: 1.44402 - acc: 0.6795 -- iter: 11/11\n",
      "--\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m1.42213\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 217 | loss: 1.42213 - acc: 0.6843 -- iter: 11/11\n",
      "--\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m1.40195\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 218 | loss: 1.40195 - acc: 0.6886 -- iter: 11/11\n",
      "--\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m1.38328\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 219 | loss: 1.38328 - acc: 0.6924 -- iter: 11/11\n",
      "--\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m1.36595\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 220 | loss: 1.36595 - acc: 0.6959 -- iter: 11/11\n",
      "--\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m1.34981\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 221 | loss: 1.34981 - acc: 0.6900 -- iter: 11/11\n",
      "--\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m1.33473\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 222 | loss: 1.33473 - acc: 0.6846 -- iter: 11/11\n",
      "--\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m1.32059\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 223 | loss: 1.32059 - acc: 0.6798 -- iter: 11/11\n",
      "--\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m1.55500\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 224 | loss: 1.55500 - acc: 0.6118 -- iter: 11/11\n",
      "--\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m1.51789\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 225 | loss: 1.51789 - acc: 0.6324 -- iter: 11/11\n",
      "--\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m1.48410\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 226 | loss: 1.48410 - acc: 0.6510 -- iter: 11/11\n",
      "--\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m1.45327\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 227 | loss: 1.45327 - acc: 0.6677 -- iter: 11/11\n",
      "--\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m1.42509\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 228 | loss: 1.42509 - acc: 0.6828 -- iter: 11/11\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m1.39927\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 229 | loss: 1.39927 - acc: 0.7054 -- iter: 11/11\n",
      "--\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m1.37555\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 230 | loss: 1.37555 - acc: 0.7258 -- iter: 11/11\n",
      "--\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m1.35372\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 231 | loss: 1.35372 - acc: 0.7532 -- iter: 11/11\n",
      "--\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m1.33356\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 232 | loss: 1.33356 - acc: 0.7779 -- iter: 11/11\n",
      "--\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m1.31490\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 233 | loss: 1.31490 - acc: 0.7910 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m1.29756\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 234 | loss: 1.29756 - acc: 0.8028 -- iter: 11/11\n",
      "--\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m1.28142\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 235 | loss: 1.28142 - acc: 0.8134 -- iter: 11/11\n",
      "--\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m1.26633\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 236 | loss: 1.26633 - acc: 0.8230 -- iter: 11/11\n",
      "--\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m1.25219\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 237 | loss: 1.25219 - acc: 0.8316 -- iter: 11/11\n",
      "--\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m1.23890\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 238 | loss: 1.23890 - acc: 0.8394 -- iter: 11/11\n",
      "--\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m1.22635\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 239 | loss: 1.22635 - acc: 0.8463 -- iter: 11/11\n",
      "--\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m1.21448\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 240 | loss: 1.21448 - acc: 0.8526 -- iter: 11/11\n",
      "--\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m1.20320\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 241 | loss: 1.20320 - acc: 0.8583 -- iter: 11/11\n",
      "--\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m1.19246\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 242 | loss: 1.19246 - acc: 0.8633 -- iter: 11/11\n",
      "--\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m1.18220\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 243 | loss: 1.18220 - acc: 0.8679 -- iter: 11/11\n",
      "--\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m1.17237\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 244 | loss: 1.17237 - acc: 0.8720 -- iter: 11/11\n",
      "--\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m1.16292\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 245 | loss: 1.16292 - acc: 0.8757 -- iter: 11/11\n",
      "--\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m1.15382\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 246 | loss: 1.15382 - acc: 0.8882 -- iter: 11/11\n",
      "--\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m1.14502\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 247 | loss: 1.14502 - acc: 0.8993 -- iter: 11/11\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m1.13651\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 248 | loss: 1.13651 - acc: 0.9094 -- iter: 11/11\n",
      "--\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m1.12824\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 249 | loss: 1.12824 - acc: 0.9185 -- iter: 11/11\n",
      "--\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m1.12020\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 250 | loss: 1.12020 - acc: 0.9175 -- iter: 11/11\n",
      "--\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m1.11236\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 251 | loss: 1.11236 - acc: 0.9167 -- iter: 11/11\n",
      "--\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m1.10471\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 252 | loss: 1.10471 - acc: 0.9159 -- iter: 11/11\n",
      "--\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m1.09722\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 253 | loss: 1.09722 - acc: 0.9152 -- iter: 11/11\n",
      "--\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m1.08989\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 254 | loss: 1.08989 - acc: 0.9146 -- iter: 11/11\n",
      "--\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m1.08270\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 255 | loss: 1.08270 - acc: 0.9141 -- iter: 11/11\n",
      "--\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m1.07564\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 256 | loss: 1.07564 - acc: 0.9136 -- iter: 11/11\n",
      "--\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m1.06869\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 257 | loss: 1.06869 - acc: 0.9131 -- iter: 11/11\n",
      "--\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m1.06185\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 258 | loss: 1.06185 - acc: 0.9127 -- iter: 11/11\n",
      "--\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m1.05512\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 259 | loss: 1.05512 - acc: 0.9124 -- iter: 11/11\n",
      "--\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m1.04847\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 260 | loss: 1.04847 - acc: 0.9120 -- iter: 11/11\n",
      "--\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m1.04191\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 261 | loss: 1.04191 - acc: 0.9117 -- iter: 11/11\n",
      "--\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m1.03543\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 262 | loss: 1.03543 - acc: 0.9115 -- iter: 11/11\n",
      "--\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m1.02902\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 263 | loss: 1.02902 - acc: 0.9112 -- iter: 11/11\n",
      "--\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m1.02269\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 264 | loss: 1.02269 - acc: 0.9110 -- iter: 11/11\n",
      "--\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m1.01642\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 265 | loss: 1.01642 - acc: 0.9108 -- iter: 11/11\n",
      "--\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m1.01021\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 266 | loss: 1.01021 - acc: 0.9107 -- iter: 11/11\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m1.00406\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 267 | loss: 1.00406 - acc: 0.9105 -- iter: 11/11\n",
      "--\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.99796\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 268 | loss: 0.99796 - acc: 0.9104 -- iter: 11/11\n",
      "--\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.99192\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 269 | loss: 0.99192 - acc: 0.9102 -- iter: 11/11\n",
      "--\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.98592\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 270 | loss: 0.98592 - acc: 0.9101 -- iter: 11/11\n",
      "--\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.97998\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 271 | loss: 0.97998 - acc: 0.9100 -- iter: 11/11\n",
      "--\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.97408\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 272 | loss: 0.97408 - acc: 0.9099 -- iter: 11/11\n",
      "--\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.96822\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 273 | loss: 0.96822 - acc: 0.9098 -- iter: 11/11\n",
      "--\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m1.18927\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 274 | loss: 1.18927 - acc: 0.8370 -- iter: 11/11\n",
      "--\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m1.16098\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 275 | loss: 1.16098 - acc: 0.8442 -- iter: 11/11\n",
      "--\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m1.13513\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 276 | loss: 1.13513 - acc: 0.8507 -- iter: 11/11\n",
      "--\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m1.11148\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 277 | loss: 1.11148 - acc: 0.8566 -- iter: 11/11\n",
      "--\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m1.35856\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 278 | loss: 1.35856 - acc: 0.7891 -- iter: 11/11\n",
      "--\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m1.31196\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 279 | loss: 1.31196 - acc: 0.7920 -- iter: 11/11\n",
      "--\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m1.51677\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 280 | loss: 1.51677 - acc: 0.7401 -- iter: 11/11\n",
      "--\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m1.45401\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 281 | loss: 1.45401 - acc: 0.7479 -- iter: 11/11\n",
      "--\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m1.39739\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 282 | loss: 1.39739 - acc: 0.7549 -- iter: 11/11\n",
      "--\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m1.34627\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 283 | loss: 1.34627 - acc: 0.7612 -- iter: 11/11\n",
      "--\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m1.81685\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 284 | loss: 1.81685 - acc: 0.6851 -- iter: 11/11\n",
      "--\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m1.72373\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 285 | loss: 1.72373 - acc: 0.6984 -- iter: 11/11\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m1.64001\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 286 | loss: 1.64001 - acc: 0.7104 -- iter: 11/11\n",
      "--\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m1.56471\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 287 | loss: 1.56471 - acc: 0.7303 -- iter: 11/11\n",
      "--\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m1.89063\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 288 | loss: 1.89063 - acc: 0.6572 -- iter: 11/11\n",
      "--\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m1.79048\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 289 | loss: 1.79048 - acc: 0.6824 -- iter: 11/11\n",
      "--\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m2.04142\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 290 | loss: 2.04142 - acc: 0.6324 -- iter: 11/11\n",
      "--\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m1.92664\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 291 | loss: 1.92664 - acc: 0.6691 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m2.21072\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 292 | loss: 2.21072 - acc: 0.6113 -- iter: 11/11\n",
      "--\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m2.07969\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 293 | loss: 2.07969 - acc: 0.6411 -- iter: 11/11\n",
      "--\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m1.96211\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 294 | loss: 1.96211 - acc: 0.6679 -- iter: 11/11\n",
      "--\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m1.85659\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 295 | loss: 1.85659 - acc: 0.6920 -- iter: 11/11\n",
      "--\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m1.76183\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 296 | loss: 1.76183 - acc: 0.7137 -- iter: 11/11\n",
      "--\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m1.67669\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 297 | loss: 1.67669 - acc: 0.7333 -- iter: 11/11\n",
      "--\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m1.60016\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 298 | loss: 1.60016 - acc: 0.7508 -- iter: 11/11\n",
      "--\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m1.53130\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 299 | loss: 1.53130 - acc: 0.7667 -- iter: 11/11\n",
      "--\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m1.46931\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 300 | loss: 1.46931 - acc: 0.7809 -- iter: 11/11\n",
      "--\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m1.41343\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 301 | loss: 1.41343 - acc: 0.7937 -- iter: 11/11\n",
      "--\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m1.36302\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 302 | loss: 1.36302 - acc: 0.8053 -- iter: 11/11\n",
      "--\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m1.31748\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 303 | loss: 1.31748 - acc: 0.8156 -- iter: 11/11\n",
      "--\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m1.27630\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 304 | loss: 1.27630 - acc: 0.8250 -- iter: 11/11\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m1.23899\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 305 | loss: 1.23899 - acc: 0.8334 -- iter: 11/11\n",
      "--\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m1.20515\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 306 | loss: 1.20515 - acc: 0.8410 -- iter: 11/11\n",
      "--\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m1.17440\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 307 | loss: 1.17440 - acc: 0.8478 -- iter: 11/11\n",
      "--\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m1.14641\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 308 | loss: 1.14641 - acc: 0.8539 -- iter: 11/11\n",
      "--\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m1.12088\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 309 | loss: 1.12088 - acc: 0.8594 -- iter: 11/11\n",
      "--\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m1.09754\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 310 | loss: 1.09754 - acc: 0.8644 -- iter: 11/11\n",
      "--\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m1.07616\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 311 | loss: 1.07616 - acc: 0.8689 -- iter: 11/11\n",
      "--\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m1.05654\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 312 | loss: 1.05654 - acc: 0.8729 -- iter: 11/11\n",
      "--\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m1.03847\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 313 | loss: 1.03847 - acc: 0.8765 -- iter: 11/11\n",
      "--\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m1.02180\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 314 | loss: 1.02180 - acc: 0.8798 -- iter: 11/11\n",
      "--\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m1.00638\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 315 | loss: 1.00638 - acc: 0.8827 -- iter: 11/11\n",
      "--\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.99208\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 316 | loss: 0.99208 - acc: 0.8853 -- iter: 11/11\n",
      "--\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.97877\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 317 | loss: 0.97877 - acc: 0.8877 -- iter: 11/11\n",
      "--\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.96635\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 318 | loss: 0.96635 - acc: 0.8899 -- iter: 11/11\n",
      "--\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.95474\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 319 | loss: 0.95474 - acc: 0.8918 -- iter: 11/11\n",
      "--\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.94383\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 320 | loss: 0.94383 - acc: 0.8935 -- iter: 11/11\n",
      "--\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.93357\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 321 | loss: 0.93357 - acc: 0.8951 -- iter: 11/11\n",
      "--\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m1.29272\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 322 | loss: 1.29272 - acc: 0.8146 -- iter: 11/11\n",
      "--\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m1.24693\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 323 | loss: 1.24693 - acc: 0.8332 -- iter: 11/11\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m1.20549\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 324 | loss: 1.20549 - acc: 0.8499 -- iter: 11/11\n",
      "--\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m1.16796\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 325 | loss: 1.16796 - acc: 0.8558 -- iter: 11/11\n",
      "--\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m1.13392\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 326 | loss: 1.13392 - acc: 0.8611 -- iter: 11/11\n",
      "--\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m1.10300\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 327 | loss: 1.10300 - acc: 0.8659 -- iter: 11/11\n",
      "--\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m1.07487\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 328 | loss: 1.07487 - acc: 0.8702 -- iter: 11/11\n",
      "--\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m1.04924\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 329 | loss: 1.04924 - acc: 0.8741 -- iter: 11/11\n",
      "--\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m1.02584\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 330 | loss: 1.02584 - acc: 0.8776 -- iter: 11/11\n",
      "--\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m1.00443\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 331 | loss: 1.00443 - acc: 0.8808 -- iter: 11/11\n",
      "--\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.98481\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 332 | loss: 0.98481 - acc: 0.8836 -- iter: 11/11\n",
      "--\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.96678\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 333 | loss: 0.96678 - acc: 0.8861 -- iter: 11/11\n",
      "--\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m1.17752\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 334 | loss: 1.17752 - acc: 0.8157 -- iter: 11/11\n",
      "--\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m1.13959\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 335 | loss: 1.13959 - acc: 0.8251 -- iter: 11/11\n",
      "--\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m1.10519\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 336 | loss: 1.10519 - acc: 0.8335 -- iter: 11/11\n",
      "--\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m1.07394\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 337 | loss: 1.07394 - acc: 0.8410 -- iter: 11/11\n",
      "--\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m1.04551\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 338 | loss: 1.04551 - acc: 0.8478 -- iter: 11/11\n",
      "--\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m1.01961\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 339 | loss: 1.01961 - acc: 0.8540 -- iter: 11/11\n",
      "--\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.99597\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 340 | loss: 0.99597 - acc: 0.8595 -- iter: 11/11\n",
      "--\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.97435\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 341 | loss: 0.97435 - acc: 0.8644 -- iter: 11/11\n",
      "--\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.95455\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 342 | loss: 0.95455 - acc: 0.8689 -- iter: 11/11\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.93636\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 343 | loss: 0.93636 - acc: 0.8729 -- iter: 11/11\n",
      "--\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.91963\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 344 | loss: 0.91963 - acc: 0.8765 -- iter: 11/11\n",
      "--\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.90420\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 345 | loss: 0.90420 - acc: 0.8798 -- iter: 11/11\n",
      "--\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.88993\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 346 | loss: 0.88993 - acc: 0.8827 -- iter: 11/11\n",
      "--\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.87671\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 347 | loss: 0.87671 - acc: 0.8854 -- iter: 11/11\n",
      "--\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.86442\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 348 | loss: 0.86442 - acc: 0.8877 -- iter: 11/11\n",
      "--\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.85296\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 349 | loss: 0.85296 - acc: 0.8899 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m1.38096\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 350 | loss: 1.38096 - acc: 0.8009 -- iter: 11/11\n",
      "--\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m1.31737\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 351 | loss: 1.31737 - acc: 0.8117 -- iter: 11/11\n",
      "--\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m1.26002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 352 | loss: 1.26002 - acc: 0.8214 -- iter: 11/11\n",
      "--\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m1.20827\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 353 | loss: 1.20827 - acc: 0.8302 -- iter: 11/11\n",
      "--\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m1.54791\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 354 | loss: 1.54791 - acc: 0.7472 -- iter: 11/11\n",
      "--\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m1.46722\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 355 | loss: 1.46722 - acc: 0.7634 -- iter: 11/11\n",
      "--\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m1.39460\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 356 | loss: 1.39460 - acc: 0.7779 -- iter: 11/11\n",
      "--\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m1.32919\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 357 | loss: 1.32919 - acc: 0.7911 -- iter: 11/11\n",
      "--\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m1.27025\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 358 | loss: 1.27025 - acc: 0.8029 -- iter: 11/11\n",
      "--\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m1.21709\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 359 | loss: 1.21709 - acc: 0.8226 -- iter: 11/11\n",
      "--\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m1.70204\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 360 | loss: 1.70204 - acc: 0.7403 -- iter: 11/11\n",
      "--\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m1.60571\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 361 | loss: 1.60571 - acc: 0.7663 -- iter: 11/11\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m1.51911\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 362 | loss: 1.51911 - acc: 0.7897 -- iter: 11/11\n",
      "--\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m1.44123\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 363 | loss: 1.44123 - acc: 0.8107 -- iter: 11/11\n",
      "--\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m1.37115\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 364 | loss: 1.37115 - acc: 0.8296 -- iter: 11/11\n",
      "--\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m1.30805\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 365 | loss: 1.30805 - acc: 0.8467 -- iter: 11/11\n",
      "--\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m1.25119\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 366 | loss: 1.25119 - acc: 0.8620 -- iter: 11/11\n",
      "--\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m1.19992\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 367 | loss: 1.19992 - acc: 0.8758 -- iter: 11/11\n",
      "--\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m1.42379\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 368 | loss: 1.42379 - acc: 0.8064 -- iter: 11/11\n",
      "--\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m1.35512\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 369 | loss: 1.35512 - acc: 0.8258 -- iter: 11/11\n",
      "--\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m1.29329\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 370 | loss: 1.29329 - acc: 0.8341 -- iter: 11/11\n",
      "--\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m1.23756\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 371 | loss: 1.23756 - acc: 0.8416 -- iter: 11/11\n",
      "--\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m1.18729\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 372 | loss: 1.18729 - acc: 0.8483 -- iter: 11/11\n",
      "--\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m1.14191\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 373 | loss: 1.14191 - acc: 0.8544 -- iter: 11/11\n",
      "--\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m1.10089\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 374 | loss: 1.10089 - acc: 0.8599 -- iter: 11/11\n",
      "--\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m1.06377\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 375 | loss: 1.06377 - acc: 0.8648 -- iter: 11/11\n",
      "--\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m1.33839\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 376 | loss: 1.33839 - acc: 0.7965 -- iter: 11/11\n",
      "--\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m1.27725\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 377 | loss: 1.27725 - acc: 0.8078 -- iter: 11/11\n",
      "--\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m1.62536\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 378 | loss: 1.62536 - acc: 0.7270 -- iter: 11/11\n",
      "--\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m1.53558\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 379 | loss: 1.53558 - acc: 0.7452 -- iter: 11/11\n",
      "--\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m1.45488\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 380 | loss: 1.45488 - acc: 0.7616 -- iter: 11/11\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m1.38230\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 381 | loss: 1.38230 - acc: 0.7763 -- iter: 11/11\n",
      "--\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m1.31697\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 382 | loss: 1.31697 - acc: 0.7896 -- iter: 11/11\n",
      "--\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m1.25813\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 383 | loss: 1.25813 - acc: 0.8016 -- iter: 11/11\n",
      "--\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m1.72757\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 384 | loss: 1.72757 - acc: 0.7305 -- iter: 11/11\n",
      "--\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m1.62779\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 385 | loss: 1.62779 - acc: 0.7484 -- iter: 11/11\n",
      "--\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m1.53813\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 386 | loss: 1.53813 - acc: 0.7644 -- iter: 11/11\n",
      "--\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m1.45753\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 387 | loss: 1.45753 - acc: 0.7789 -- iter: 11/11\n",
      "--\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m1.38501\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 388 | loss: 1.38501 - acc: 0.7919 -- iter: 11/11\n",
      "--\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m1.31972\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 389 | loss: 1.31972 - acc: 0.8036 -- iter: 11/11\n",
      "--\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m1.26090\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 390 | loss: 1.26090 - acc: 0.8142 -- iter: 11/11\n",
      "--\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m1.20786\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 391 | loss: 1.20786 - acc: 0.8237 -- iter: 11/11\n",
      "--\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m1.15998\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 392 | loss: 1.15998 - acc: 0.8322 -- iter: 11/11\n",
      "--\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m1.11671\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 393 | loss: 1.11671 - acc: 0.8399 -- iter: 11/11\n",
      "--\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m1.07757\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 394 | loss: 1.07757 - acc: 0.8468 -- iter: 11/11\n",
      "--\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m1.04211\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 395 | loss: 1.04211 - acc: 0.8530 -- iter: 11/11\n",
      "--\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m1.00993\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 396 | loss: 1.00993 - acc: 0.8587 -- iter: 11/11\n",
      "--\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.98070\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 397 | loss: 0.98070 - acc: 0.8637 -- iter: 11/11\n",
      "--\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m1.16889\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 398 | loss: 1.16889 - acc: 0.7955 -- iter: 11/11\n",
      "--\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m1.12331\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 399 | loss: 1.12331 - acc: 0.8069 -- iter: 11/11\n",
      "--\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m1.08210\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 400 | loss: 1.08210 - acc: 0.8171 -- iter: 11/11\n",
      "--\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m1.04480\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 401 | loss: 1.04480 - acc: 0.8263 -- iter: 11/11\n",
      "--\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m1.01099\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 402 | loss: 1.01099 - acc: 0.8346 -- iter: 11/11\n",
      "--\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.98031\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 403 | loss: 0.98031 - acc: 0.8420 -- iter: 11/11\n",
      "--\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m1.40542\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 404 | loss: 1.40542 - acc: 0.7669 -- iter: 11/11\n",
      "--\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m1.33501\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 405 | loss: 1.33501 - acc: 0.7811 -- iter: 11/11\n",
      "--\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m1.27158\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 406 | loss: 1.27158 - acc: 0.7939 -- iter: 11/11\n",
      "--\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m1.21440\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 407 | loss: 1.21440 - acc: 0.8054 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m1.16282\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 408 | loss: 1.16282 - acc: 0.8158 -- iter: 11/11\n",
      "--\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m1.11624\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 409 | loss: 1.11624 - acc: 0.8251 -- iter: 11/11\n",
      "--\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m1.07413\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 410 | loss: 1.07413 - acc: 0.8335 -- iter: 11/11\n",
      "--\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m1.03603\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 411 | loss: 1.03603 - acc: 0.8411 -- iter: 11/11\n",
      "--\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m1.00151\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 412 | loss: 1.00151 - acc: 0.8479 -- iter: 11/11\n",
      "--\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.97019\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 413 | loss: 0.97019 - acc: 0.8540 -- iter: 11/11\n",
      "--\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.94173\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 414 | loss: 0.94173 - acc: 0.8595 -- iter: 11/11\n",
      "--\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.91583\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 415 | loss: 0.91583 - acc: 0.8645 -- iter: 11/11\n",
      "--\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.89222\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 416 | loss: 0.89222 - acc: 0.8689 -- iter: 11/11\n",
      "--\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.87066\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 417 | loss: 0.87066 - acc: 0.8730 -- iter: 11/11\n",
      "--\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m1.34003\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 418 | loss: 1.34003 - acc: 0.8038 -- iter: 11/11\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m1.27330\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 419 | loss: 1.27330 - acc: 0.8144 -- iter: 11/11\n",
      "--\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m1.74200\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 420 | loss: 1.74200 - acc: 0.7420 -- iter: 11/11\n",
      "--\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m1.63514\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 421 | loss: 1.63514 - acc: 0.7587 -- iter: 11/11\n",
      "--\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m2.10107\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 422 | loss: 2.10107 - acc: 0.6919 -- iter: 11/11\n",
      "--\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m1.95879\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 423 | loss: 1.95879 - acc: 0.7137 -- iter: 11/11\n",
      "--\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m1.83104\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 424 | loss: 1.83104 - acc: 0.7332 -- iter: 11/11\n",
      "--\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m1.71630\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 425 | loss: 1.71630 - acc: 0.7599 -- iter: 11/11\n",
      "--\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m1.61321\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 426 | loss: 1.61321 - acc: 0.7839 -- iter: 11/11\n",
      "--\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m1.52055\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 427 | loss: 1.52055 - acc: 0.8055 -- iter: 11/11\n",
      "--\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m1.43721\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 428 | loss: 1.43721 - acc: 0.8250 -- iter: 11/11\n",
      "--\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m1.36222\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 429 | loss: 1.36222 - acc: 0.8425 -- iter: 11/11\n",
      "--\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m1.29469\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 430 | loss: 1.29469 - acc: 0.8582 -- iter: 11/11\n",
      "--\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m1.23383\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 431 | loss: 1.23383 - acc: 0.8724 -- iter: 11/11\n",
      "--\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m1.17894\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 432 | loss: 1.17894 - acc: 0.8852 -- iter: 11/11\n",
      "--\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m1.12938\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 433 | loss: 1.12938 - acc: 0.8966 -- iter: 11/11\n",
      "--\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m1.08459\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 434 | loss: 1.08459 - acc: 0.8979 -- iter: 11/11\n",
      "--\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m1.04406\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 435 | loss: 1.04406 - acc: 0.8990 -- iter: 11/11\n",
      "--\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m1.00735\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 436 | loss: 1.00735 - acc: 0.9000 -- iter: 11/11\n",
      "--\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.97405\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 437 | loss: 0.97405 - acc: 0.9009 -- iter: 11/11\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.94379\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 438 | loss: 0.94379 - acc: 0.9017 -- iter: 11/11\n",
      "--\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.91627\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 439 | loss: 0.91627 - acc: 0.9025 -- iter: 11/11\n",
      "--\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.89118\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 440 | loss: 0.89118 - acc: 0.9031 -- iter: 11/11\n",
      "--\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.86827\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 441 | loss: 0.86827 - acc: 0.9037 -- iter: 11/11\n",
      "--\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m1.09562\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 442 | loss: 1.09562 - acc: 0.8315 -- iter: 11/11\n",
      "--\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m1.05173\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 443 | loss: 1.05173 - acc: 0.8484 -- iter: 11/11\n",
      "--\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m1.01200\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 444 | loss: 1.01200 - acc: 0.8635 -- iter: 11/11\n",
      "--\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.97600\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 445 | loss: 0.97600 - acc: 0.8772 -- iter: 11/11\n",
      "--\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.94334\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 446 | loss: 0.94334 - acc: 0.8895 -- iter: 11/11\n",
      "--\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.91366\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 447 | loss: 0.91366 - acc: 0.9005 -- iter: 11/11\n",
      "--\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.88666\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 448 | loss: 0.88666 - acc: 0.9105 -- iter: 11/11\n",
      "--\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.86206\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 449 | loss: 0.86206 - acc: 0.9194 -- iter: 11/11\n",
      "--\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m1.15437\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 450 | loss: 1.15437 - acc: 0.8366 -- iter: 11/11\n",
      "--\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m1.10255\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 451 | loss: 1.10255 - acc: 0.8529 -- iter: 11/11\n",
      "--\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m1.05575\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 452 | loss: 1.05575 - acc: 0.8676 -- iter: 11/11\n",
      "--\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m1.01344\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 453 | loss: 1.01344 - acc: 0.8809 -- iter: 11/11\n",
      "--\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m1.44999\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 454 | loss: 1.44999 - acc: 0.8019 -- iter: 11/11\n",
      "--\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m1.36810\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 455 | loss: 1.36810 - acc: 0.8217 -- iter: 11/11\n",
      "--\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m1.29442\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 456 | loss: 1.29442 - acc: 0.8395 -- iter: 11/11\n",
      "--\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m1.22807\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 457 | loss: 1.22807 - acc: 0.8556 -- iter: 11/11\n",
      "--\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m1.16829\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 458 | loss: 1.16829 - acc: 0.8700 -- iter: 11/11\n",
      "--\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m1.11439\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 459 | loss: 1.11439 - acc: 0.8830 -- iter: 11/11\n",
      "--\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m1.06575\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 460 | loss: 1.06575 - acc: 0.8947 -- iter: 11/11\n",
      "--\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m1.02181\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 461 | loss: 1.02181 - acc: 0.9052 -- iter: 11/11\n",
      "--\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.98207\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 462 | loss: 0.98207 - acc: 0.9147 -- iter: 11/11\n",
      "--\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.94610\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 463 | loss: 0.94610 - acc: 0.9232 -- iter: 11/11\n",
      "--\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.91350\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 464 | loss: 0.91350 - acc: 0.9309 -- iter: 11/11\n",
      "--\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.88390\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 465 | loss: 0.88390 - acc: 0.9378 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.85700\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 466 | loss: 0.85700 - acc: 0.9440 -- iter: 11/11\n",
      "--\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.83251\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 467 | loss: 0.83251 - acc: 0.9496 -- iter: 11/11\n",
      "--\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.81017\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 468 | loss: 0.81017 - acc: 0.9547 -- iter: 11/11\n",
      "--\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.78975\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 469 | loss: 0.78975 - acc: 0.9592 -- iter: 11/11\n",
      "--\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.77106\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 470 | loss: 0.77106 - acc: 0.9633 -- iter: 11/11\n",
      "--\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.75391\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 471 | loss: 0.75391 - acc: 0.9670 -- iter: 11/11\n",
      "--\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m1.14107\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 472 | loss: 1.14107 - acc: 0.8884 -- iter: 11/11\n",
      "--\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m1.08647\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 473 | loss: 1.08647 - acc: 0.8996 -- iter: 11/11\n",
      "--\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m1.45782\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 474 | loss: 1.45782 - acc: 0.8278 -- iter: 11/11\n",
      "--\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m1.37149\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 475 | loss: 1.37149 - acc: 0.8450 -- iter: 11/11\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m1.29385\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 476 | loss: 1.29385 - acc: 0.8605 -- iter: 11/11\n",
      "--\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m1.22398\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 477 | loss: 1.22398 - acc: 0.8745 -- iter: 11/11\n",
      "--\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m1.16107\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 478 | loss: 1.16107 - acc: 0.8870 -- iter: 11/11\n",
      "--\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m1.10439\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 479 | loss: 1.10439 - acc: 0.8983 -- iter: 11/11\n",
      "--\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m1.05327\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 480 | loss: 1.05327 - acc: 0.9085 -- iter: 11/11\n",
      "--\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m1.00713\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 481 | loss: 1.00713 - acc: 0.9176 -- iter: 11/11\n",
      "--\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.96545\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 482 | loss: 0.96545 - acc: 0.9259 -- iter: 11/11\n",
      "--\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.92774\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 483 | loss: 0.92774 - acc: 0.9333 -- iter: 11/11\n",
      "--\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.89359\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 484 | loss: 0.89359 - acc: 0.9400 -- iter: 11/11\n",
      "--\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.86262\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 485 | loss: 0.86262 - acc: 0.9460 -- iter: 11/11\n",
      "--\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.83449\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 486 | loss: 0.83449 - acc: 0.9514 -- iter: 11/11\n",
      "--\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.80889\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 487 | loss: 0.80889 - acc: 0.9562 -- iter: 11/11\n",
      "--\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.78557\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 488 | loss: 0.78557 - acc: 0.9606 -- iter: 11/11\n",
      "--\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.76427\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 489 | loss: 0.76427 - acc: 0.9645 -- iter: 11/11\n",
      "--\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.74479\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 490 | loss: 0.74479 - acc: 0.9681 -- iter: 11/11\n",
      "--\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.72693\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 491 | loss: 0.72693 - acc: 0.9713 -- iter: 11/11\n",
      "--\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m1.18570\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 492 | loss: 1.18570 - acc: 0.8832 -- iter: 11/11\n",
      "--\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m1.12328\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 493 | loss: 1.12328 - acc: 0.8949 -- iter: 11/11\n",
      "--\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m1.06695\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 494 | loss: 1.06695 - acc: 0.9054 -- iter: 11/11\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m1.01607\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 495 | loss: 1.01607 - acc: 0.9149 -- iter: 11/11\n",
      "--\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.97008\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 496 | loss: 0.97008 - acc: 0.9234 -- iter: 11/11\n",
      "--\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.92847\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 497 | loss: 0.92847 - acc: 0.9311 -- iter: 11/11\n",
      "--\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m1.29720\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 498 | loss: 1.29720 - acc: 0.8470 -- iter: 11/11\n",
      "--\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m1.22263\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 499 | loss: 1.22263 - acc: 0.8623 -- iter: 11/11\n",
      "--\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m1.15545\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 500 | loss: 1.15545 - acc: 0.8761 -- iter: 11/11\n",
      "--\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m1.09491\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 501 | loss: 1.09491 - acc: 0.8885 -- iter: 11/11\n",
      "--\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m1.04029\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 502 | loss: 1.04029 - acc: 0.8996 -- iter: 11/11\n",
      "--\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.99099\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 503 | loss: 0.99099 - acc: 0.9097 -- iter: 11/11\n",
      "--\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.94644\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 504 | loss: 0.94644 - acc: 0.9187 -- iter: 11/11\n",
      "--\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.90615\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 505 | loss: 0.90615 - acc: 0.9268 -- iter: 11/11\n",
      "--\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.86966\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 506 | loss: 0.86966 - acc: 0.9342 -- iter: 11/11\n",
      "--\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.83659\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 507 | loss: 0.83659 - acc: 0.9407 -- iter: 11/11\n",
      "--\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m1.14167\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 508 | loss: 1.14167 - acc: 0.8467 -- iter: 11/11\n",
      "--\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m1.08105\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 509 | loss: 1.08105 - acc: 0.8620 -- iter: 11/11\n",
      "--\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m1.02638\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 510 | loss: 1.02638 - acc: 0.8758 -- iter: 11/11\n",
      "--\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.97703\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 511 | loss: 0.97703 - acc: 0.8882 -- iter: 11/11\n",
      "--\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.93245\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 512 | loss: 0.93245 - acc: 0.8994 -- iter: 11/11\n",
      "--\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.89214\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 513 | loss: 0.89214 - acc: 0.9095 -- iter: 11/11\n",
      "--\n",
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.85564\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 514 | loss: 0.85564 - acc: 0.9185 -- iter: 11/11\n",
      "--\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.82257\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 515 | loss: 0.82257 - acc: 0.9267 -- iter: 11/11\n",
      "--\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.79256\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 516 | loss: 0.79256 - acc: 0.9340 -- iter: 11/11\n",
      "--\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.76528\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 517 | loss: 0.76528 - acc: 0.9406 -- iter: 11/11\n",
      "--\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.74047\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 518 | loss: 0.74047 - acc: 0.9465 -- iter: 11/11\n",
      "--\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.71785\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 519 | loss: 0.71785 - acc: 0.9519 -- iter: 11/11\n",
      "--\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.69720\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 520 | loss: 0.69720 - acc: 0.9567 -- iter: 11/11\n",
      "--\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.67831\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 521 | loss: 0.67831 - acc: 0.9610 -- iter: 11/11\n",
      "--\n",
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.66100\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 522 | loss: 0.66100 - acc: 0.9649 -- iter: 11/11\n",
      "--\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.64511\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 523 | loss: 0.64511 - acc: 0.9684 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m1.00370\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 524 | loss: 1.00370 - acc: 0.8807 -- iter: 11/11\n",
      "--\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.95307\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 525 | loss: 0.95307 - acc: 0.8926 -- iter: 11/11\n",
      "--\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m1.43795\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 526 | loss: 1.43795 - acc: 0.8033 -- iter: 11/11\n",
      "--\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m1.34380\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 527 | loss: 1.34380 - acc: 0.8230 -- iter: 11/11\n",
      "--\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m1.25909\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 528 | loss: 1.25909 - acc: 0.8407 -- iter: 11/11\n",
      "--\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m1.18284\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 529 | loss: 1.18284 - acc: 0.8566 -- iter: 11/11\n",
      "--\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m1.52470\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 530 | loss: 1.52470 - acc: 0.7710 -- iter: 11/11\n",
      "--\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m1.42198\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 531 | loss: 1.42198 - acc: 0.7939 -- iter: 11/11\n",
      "--\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m1.32963\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 532 | loss: 1.32963 - acc: 0.8145 -- iter: 11/11\n",
      "--\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m1.24657\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 533 | loss: 1.24657 - acc: 0.8330 -- iter: 11/11\n",
      "--\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m1.17183\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 534 | loss: 1.17183 - acc: 0.8497 -- iter: 11/11\n",
      "--\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m1.10454\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 535 | loss: 1.10454 - acc: 0.8648 -- iter: 11/11\n",
      "--\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m1.04391\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 536 | loss: 1.04391 - acc: 0.8783 -- iter: 11/11\n",
      "--\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.98926\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 537 | loss: 0.98926 - acc: 0.8905 -- iter: 11/11\n",
      "--\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.93996\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 538 | loss: 0.93996 - acc: 0.9014 -- iter: 11/11\n",
      "--\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.89544\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 539 | loss: 0.89544 - acc: 0.9113 -- iter: 11/11\n",
      "--\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.85520\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 540 | loss: 0.85520 - acc: 0.9201 -- iter: 11/11\n",
      "--\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.81880\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 541 | loss: 0.81880 - acc: 0.9281 -- iter: 11/11\n",
      "--\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.78582\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 542 | loss: 0.78582 - acc: 0.9353 -- iter: 11/11\n",
      "--\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.75591\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 543 | loss: 0.75591 - acc: 0.9418 -- iter: 11/11\n",
      "--\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.72875\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 544 | loss: 0.72875 - acc: 0.9476 -- iter: 11/11\n",
      "--\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.70405\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 545 | loss: 0.70405 - acc: 0.9528 -- iter: 11/11\n",
      "--\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.68154\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 546 | loss: 0.68154 - acc: 0.9576 -- iter: 11/11\n",
      "--\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.66101\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 547 | loss: 0.66101 - acc: 0.9618 -- iter: 11/11\n",
      "--\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.64223\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 548 | loss: 0.64223 - acc: 0.9656 -- iter: 11/11\n",
      "--\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.62504\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 549 | loss: 0.62504 - acc: 0.9691 -- iter: 11/11\n",
      "--\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.60926\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 550 | loss: 0.60926 - acc: 0.9722 -- iter: 11/11\n",
      "--\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.59474\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 551 | loss: 0.59474 - acc: 0.9749 -- iter: 11/11\n",
      "--\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.58136\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 552 | loss: 0.58136 - acc: 0.9774 -- iter: 11/11\n",
      "--\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.56900\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 553 | loss: 0.56900 - acc: 0.9797 -- iter: 11/11\n",
      "--\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.55756\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 554 | loss: 0.55756 - acc: 0.9817 -- iter: 11/11\n",
      "--\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.54693\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 555 | loss: 0.54693 - acc: 0.9836 -- iter: 11/11\n",
      "--\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.53704\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 556 | loss: 0.53704 - acc: 0.9852 -- iter: 11/11\n",
      "--\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.52781\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 557 | loss: 0.52781 - acc: 0.9867 -- iter: 11/11\n",
      "--\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.51918\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 558 | loss: 0.51918 - acc: 0.9880 -- iter: 11/11\n",
      "--\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.51108\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 559 | loss: 0.51108 - acc: 0.9892 -- iter: 11/11\n",
      "--\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.50346\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 560 | loss: 0.50346 - acc: 0.9903 -- iter: 11/11\n",
      "--\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.49628\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 561 | loss: 0.49628 - acc: 0.9913 -- iter: 11/11\n",
      "--\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.48948\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 562 | loss: 0.48948 - acc: 0.9921 -- iter: 11/11\n",
      "--\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.48304\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 563 | loss: 0.48304 - acc: 0.9929 -- iter: 11/11\n",
      "--\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.47692\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 564 | loss: 0.47692 - acc: 0.9936 -- iter: 11/11\n",
      "--\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.47109\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 565 | loss: 0.47109 - acc: 0.9943 -- iter: 11/11\n",
      "--\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.46552\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 566 | loss: 0.46552 - acc: 0.9948 -- iter: 11/11\n",
      "--\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.46018\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 567 | loss: 0.46018 - acc: 0.9954 -- iter: 11/11\n",
      "--\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.45506\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 568 | loss: 0.45506 - acc: 0.9958 -- iter: 11/11\n",
      "--\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.45013\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 569 | loss: 0.45013 - acc: 0.9962 -- iter: 11/11\n",
      "--\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.44538\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 570 | loss: 0.44538 - acc: 0.9966 -- iter: 11/11\n",
      "--\n",
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.44079\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 571 | loss: 0.44079 - acc: 0.9970 -- iter: 11/11\n",
      "--\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.43634\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 572 | loss: 0.43634 - acc: 0.9973 -- iter: 11/11\n",
      "--\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.43203\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 573 | loss: 0.43203 - acc: 0.9975 -- iter: 11/11\n",
      "--\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.42785\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 574 | loss: 0.42785 - acc: 0.9978 -- iter: 11/11\n",
      "--\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.42377\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 575 | loss: 0.42377 - acc: 0.9980 -- iter: 11/11\n",
      "--\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.41980\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 576 | loss: 0.41980 - acc: 0.9982 -- iter: 11/11\n",
      "--\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.41592\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 577 | loss: 0.41592 - acc: 0.9984 -- iter: 11/11\n",
      "--\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m1.02846\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 578 | loss: 1.02846 - acc: 0.9076 -- iter: 11/11\n",
      "--\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.96335\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 579 | loss: 0.96335 - acc: 0.9169 -- iter: 11/11\n",
      "--\n",
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.90467\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 580 | loss: 0.90467 - acc: 0.9252 -- iter: 11/11\n",
      "--\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.85177\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 581 | loss: 0.85177 - acc: 0.9327 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m1.41709\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 582 | loss: 1.41709 - acc: 0.8394 -- iter: 11/11\n",
      "--\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m1.31292\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 583 | loss: 1.31292 - acc: 0.8555 -- iter: 11/11\n",
      "--\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m1.21924\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 584 | loss: 1.21924 - acc: 0.8699 -- iter: 11/11\n",
      "--\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m1.13496\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 585 | loss: 1.13496 - acc: 0.8829 -- iter: 11/11\n",
      "--\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m1.05911\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 586 | loss: 1.05911 - acc: 0.8946 -- iter: 11/11\n",
      "--\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.99083\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 587 | loss: 0.99083 - acc: 0.9052 -- iter: 11/11\n",
      "--\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.92934\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 588 | loss: 0.92934 - acc: 0.9146 -- iter: 11/11\n",
      "--\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.87392\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 589 | loss: 0.87392 - acc: 0.9232 -- iter: 11/11\n",
      "--\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.82396\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 590 | loss: 0.82396 - acc: 0.9309 -- iter: 11/11\n",
      "--\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.77888\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 591 | loss: 0.77888 - acc: 0.9378 -- iter: 11/11\n",
      "--\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m1.32512\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 592 | loss: 1.32512 - acc: 0.8440 -- iter: 11/11\n",
      "--\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m1.22990\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 593 | loss: 1.22990 - acc: 0.8596 -- iter: 11/11\n",
      "--\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m1.57391\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 594 | loss: 1.57391 - acc: 0.7827 -- iter: 11/11\n",
      "--\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m1.45407\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 595 | loss: 1.45407 - acc: 0.8045 -- iter: 11/11\n",
      "--\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m1.34637\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 596 | loss: 1.34637 - acc: 0.8240 -- iter: 11/11\n",
      "--\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m1.24955\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 597 | loss: 1.24955 - acc: 0.8416 -- iter: 11/11\n",
      "--\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m1.16249\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 598 | loss: 1.16249 - acc: 0.8575 -- iter: 11/11\n",
      "--\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m1.08417\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 599 | loss: 1.08417 - acc: 0.8717 -- iter: 11/11\n",
      "--\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m1.01368\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 600 | loss: 1.01368 - acc: 0.8845 -- iter: 11/11\n",
      "--\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.95022\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 601 | loss: 0.95022 - acc: 0.8961 -- iter: 11/11\n",
      "--\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.89305\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 602 | loss: 0.89305 - acc: 0.9065 -- iter: 11/11\n",
      "--\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.84151\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 603 | loss: 0.84151 - acc: 0.9158 -- iter: 11/11\n",
      "--\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.79503\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 604 | loss: 0.79503 - acc: 0.9242 -- iter: 11/11\n",
      "--\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.75307\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 605 | loss: 0.75307 - acc: 0.9318 -- iter: 11/11\n",
      "--\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.71516\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 606 | loss: 0.71516 - acc: 0.9386 -- iter: 11/11\n",
      "--\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.68088\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 607 | loss: 0.68088 - acc: 0.9448 -- iter: 11/11\n",
      "--\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.64986\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 608 | loss: 0.64986 - acc: 0.9503 -- iter: 11/11\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.62174\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 609 | loss: 0.62174 - acc: 0.9553 -- iter: 11/11\n",
      "--\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.59624\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 610 | loss: 0.59624 - acc: 0.9597 -- iter: 11/11\n",
      "--\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.57308\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 611 | loss: 0.57308 - acc: 0.9638 -- iter: 11/11\n",
      "--\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.55201\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 612 | loss: 0.55201 - acc: 0.9674 -- iter: 11/11\n",
      "--\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.53282\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 613 | loss: 0.53282 - acc: 0.9707 -- iter: 11/11\n",
      "--\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.51531\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 614 | loss: 0.51531 - acc: 0.9736 -- iter: 11/11\n",
      "--\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.49931\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 615 | loss: 0.49931 - acc: 0.9762 -- iter: 11/11\n",
      "--\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.97530\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 616 | loss: 0.97530 - acc: 0.8786 -- iter: 11/11\n",
      "--\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.91302\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 617 | loss: 0.91302 - acc: 0.8907 -- iter: 11/11\n",
      "--\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m1.34889\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 618 | loss: 1.34889 - acc: 0.8108 -- iter: 11/11\n",
      "--\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m1.24933\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 619 | loss: 1.24933 - acc: 0.8297 -- iter: 11/11\n",
      "--\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m1.15980\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 620 | loss: 1.15980 - acc: 0.8467 -- iter: 11/11\n",
      "--\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m1.07928\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 621 | loss: 1.07928 - acc: 0.8620 -- iter: 11/11\n",
      "--\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m1.00683\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 622 | loss: 1.00683 - acc: 0.8758 -- iter: 11/11\n",
      "--\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.94161\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 623 | loss: 0.94161 - acc: 0.8883 -- iter: 11/11\n",
      "--\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m1.34289\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 624 | loss: 1.34289 - acc: 0.8085 -- iter: 11/11\n",
      "--\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m1.24417\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 625 | loss: 1.24417 - acc: 0.8277 -- iter: 11/11\n",
      "--\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m1.15544\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 626 | loss: 1.15544 - acc: 0.8449 -- iter: 11/11\n",
      "--\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m1.07565\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 627 | loss: 1.07565 - acc: 0.8604 -- iter: 11/11\n",
      "--\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m1.42776\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 628 | loss: 1.42776 - acc: 0.8016 -- iter: 11/11\n",
      "--\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m1.32096\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 629 | loss: 1.32096 - acc: 0.8215 -- iter: 11/11\n",
      "--\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m1.22497\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 630 | loss: 1.22497 - acc: 0.8393 -- iter: 11/11\n",
      "--\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m1.13868\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 631 | loss: 1.13868 - acc: 0.8554 -- iter: 11/11\n",
      "--\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m1.06106\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 632 | loss: 1.06106 - acc: 0.8699 -- iter: 11/11\n",
      "--\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.99123\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 633 | loss: 0.99123 - acc: 0.8829 -- iter: 11/11\n",
      "--\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.92836\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 634 | loss: 0.92836 - acc: 0.8946 -- iter: 11/11\n",
      "--\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.87172\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 635 | loss: 0.87172 - acc: 0.9051 -- iter: 11/11\n",
      "--\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.82067\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 636 | loss: 0.82067 - acc: 0.9146 -- iter: 11/11\n",
      "--\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.77462\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 637 | loss: 0.77462 - acc: 0.9232 -- iter: 11/11\n",
      "--\n",
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m1.10810\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 638 | loss: 1.10810 - acc: 0.8399 -- iter: 11/11\n",
      "--\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m1.03322\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 639 | loss: 1.03322 - acc: 0.8559 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.96582\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 640 | loss: 0.96582 - acc: 0.8703 -- iter: 11/11\n",
      "--\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.90514\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 641 | loss: 0.90514 - acc: 0.8833 -- iter: 11/11\n",
      "--\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.85046\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 642 | loss: 0.85046 - acc: 0.8950 -- iter: 11/11\n",
      "--\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.80115\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 643 | loss: 0.80115 - acc: 0.9055 -- iter: 11/11\n",
      "--\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m1.23887\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 644 | loss: 1.23887 - acc: 0.8240 -- iter: 11/11\n",
      "--\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m1.15068\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 645 | loss: 1.15068 - acc: 0.8416 -- iter: 11/11\n",
      "--\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m1.07135\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 646 | loss: 1.07135 - acc: 0.8575 -- iter: 11/11\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.99994\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 647 | loss: 0.99994 - acc: 0.8717 -- iter: 11/11\n",
      "--\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.93564\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 648 | loss: 0.93564 - acc: 0.8845 -- iter: 11/11\n",
      "--\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.87770\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 649 | loss: 0.87770 - acc: 0.8961 -- iter: 11/11\n",
      "--\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m1.49982\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 650 | loss: 1.49982 - acc: 0.8065 -- iter: 11/11\n",
      "--\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m1.38555\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 651 | loss: 1.38555 - acc: 0.8258 -- iter: 11/11\n",
      "--\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m1.87658\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 652 | loss: 1.87658 - acc: 0.7432 -- iter: 11/11\n",
      "--\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m1.72513\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 653 | loss: 1.72513 - acc: 0.7689 -- iter: 11/11\n",
      "--\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m1.58913\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 654 | loss: 1.58913 - acc: 0.7920 -- iter: 11/11\n",
      "--\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m1.46698\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 655 | loss: 1.46698 - acc: 0.8128 -- iter: 11/11\n",
      "--\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m1.35723\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 656 | loss: 1.35723 - acc: 0.8315 -- iter: 11/11\n",
      "--\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m1.25858\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 657 | loss: 1.25858 - acc: 0.8484 -- iter: 11/11\n",
      "--\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m1.16989\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 658 | loss: 1.16989 - acc: 0.8636 -- iter: 11/11\n",
      "--\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m1.09011\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 659 | loss: 1.09011 - acc: 0.8772 -- iter: 11/11\n",
      "--\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m1.45793\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 660 | loss: 1.45793 - acc: 0.8077 -- iter: 11/11\n",
      "--\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m1.34952\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 661 | loss: 1.34952 - acc: 0.8269 -- iter: 11/11\n",
      "--\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m1.25207\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 662 | loss: 1.25207 - acc: 0.8442 -- iter: 11/11\n",
      "--\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m1.16443\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 663 | loss: 1.16443 - acc: 0.8598 -- iter: 11/11\n",
      "--\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m1.08559\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 664 | loss: 1.08559 - acc: 0.8738 -- iter: 11/11\n",
      "--\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m1.01462\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 665 | loss: 1.01462 - acc: 0.8864 -- iter: 11/11\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.95070\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 666 | loss: 0.95070 - acc: 0.8978 -- iter: 11/11\n",
      "--\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.89308\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 667 | loss: 0.89308 - acc: 0.9080 -- iter: 11/11\n",
      "--\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.84112\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 668 | loss: 0.84112 - acc: 0.9172 -- iter: 11/11\n",
      "--\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.79422\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 669 | loss: 0.79422 - acc: 0.9255 -- iter: 11/11\n",
      "--\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.75185\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 670 | loss: 0.75185 - acc: 0.9329 -- iter: 11/11\n",
      "--\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.71354\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 671 | loss: 0.71354 - acc: 0.9396 -- iter: 11/11\n",
      "--\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.67885\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 672 | loss: 0.67885 - acc: 0.9457 -- iter: 11/11\n",
      "--\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.64743\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 673 | loss: 0.64743 - acc: 0.9511 -- iter: 11/11\n",
      "--\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.61892\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 674 | loss: 0.61892 - acc: 0.9560 -- iter: 11/11\n",
      "--\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.59302\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 675 | loss: 0.59302 - acc: 0.9604 -- iter: 11/11\n",
      "--\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.56946\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 676 | loss: 0.56946 - acc: 0.9644 -- iter: 11/11\n",
      "--\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.54800\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 677 | loss: 0.54800 - acc: 0.9679 -- iter: 11/11\n",
      "--\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.52842\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 678 | loss: 0.52842 - acc: 0.9711 -- iter: 11/11\n",
      "--\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.51054\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 679 | loss: 0.51054 - acc: 0.9740 -- iter: 11/11\n",
      "--\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.49416\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 680 | loss: 0.49416 - acc: 0.9766 -- iter: 11/11\n",
      "--\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.47914\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 681 | loss: 0.47914 - acc: 0.9790 -- iter: 11/11\n",
      "--\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.93978\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 682 | loss: 0.93978 - acc: 0.8901 -- iter: 11/11\n",
      "--\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.87985\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 683 | loss: 0.87985 - acc: 0.9011 -- iter: 11/11\n",
      "--\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m1.37834\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 684 | loss: 1.37834 - acc: 0.8110 -- iter: 11/11\n",
      "--\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m1.27456\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 685 | loss: 1.27456 - acc: 0.8299 -- iter: 11/11\n",
      "--\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m1.18122\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 686 | loss: 1.18122 - acc: 0.8469 -- iter: 11/11\n",
      "--\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m1.09724\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 687 | loss: 1.09724 - acc: 0.8622 -- iter: 11/11\n",
      "--\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m1.52637\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 688 | loss: 1.52637 - acc: 0.7851 -- iter: 11/11\n",
      "--\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m1.40808\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 689 | loss: 1.40808 - acc: 0.8066 -- iter: 11/11\n",
      "--\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m1.30176\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 690 | loss: 1.30176 - acc: 0.8259 -- iter: 11/11\n",
      "--\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m1.20618\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 691 | loss: 1.20618 - acc: 0.8433 -- iter: 11/11\n",
      "--\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m1.12021\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 692 | loss: 1.12021 - acc: 0.8590 -- iter: 11/11\n",
      "--\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m1.04287\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 693 | loss: 1.04287 - acc: 0.8731 -- iter: 11/11\n",
      "--\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.97326\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 694 | loss: 0.97326 - acc: 0.8858 -- iter: 11/11\n",
      "--\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.91056\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 695 | loss: 0.91056 - acc: 0.8972 -- iter: 11/11\n",
      "--\n",
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.85406\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 696 | loss: 0.85406 - acc: 0.9075 -- iter: 11/11\n",
      "--\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.80311\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 697 | loss: 0.80311 - acc: 0.9167 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.75713\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 698 | loss: 0.75713 - acc: 0.9251 -- iter: 11/11\n",
      "--\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.71561\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 699 | loss: 0.71561 - acc: 0.9326 -- iter: 11/11\n",
      "--\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.67807\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 700 | loss: 0.67807 - acc: 0.9393 -- iter: 11/11\n",
      "--\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.64411\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 701 | loss: 0.64411 - acc: 0.9454 -- iter: 11/11\n",
      "--\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.61335\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 702 | loss: 0.61335 - acc: 0.9508 -- iter: 11/11\n",
      "--\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.58545\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 703 | loss: 0.58545 - acc: 0.9558 -- iter: 11/11\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.56013\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 704 | loss: 0.56013 - acc: 0.9602 -- iter: 11/11\n",
      "--\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.53710\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 705 | loss: 0.53710 - acc: 0.9642 -- iter: 11/11\n",
      "--\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.51614\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 706 | loss: 0.51614 - acc: 0.9677 -- iter: 11/11\n",
      "--\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.49704\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 707 | loss: 0.49704 - acc: 0.9710 -- iter: 11/11\n",
      "--\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.47959\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 708 | loss: 0.47959 - acc: 0.9739 -- iter: 11/11\n",
      "--\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.46363\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 709 | loss: 0.46363 - acc: 0.9765 -- iter: 11/11\n",
      "--\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.73713\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 710 | loss: 0.73713 - acc: 0.9061 -- iter: 11/11\n",
      "--\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.69502\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 711 | loss: 0.69502 - acc: 0.9155 -- iter: 11/11\n",
      "--\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m1.16630\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 712 | loss: 1.16630 - acc: 0.8330 -- iter: 11/11\n",
      "--\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m1.08116\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 713 | loss: 1.08116 - acc: 0.8497 -- iter: 11/11\n",
      "--\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m1.00455\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 714 | loss: 1.00455 - acc: 0.8648 -- iter: 11/11\n",
      "--\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.93559\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 715 | loss: 0.93559 - acc: 0.8783 -- iter: 11/11\n",
      "--\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.87349\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 716 | loss: 0.87349 - acc: 0.8905 -- iter: 11/11\n",
      "--\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.81754\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 717 | loss: 0.81754 - acc: 0.9014 -- iter: 11/11\n",
      "--\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.76711\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 718 | loss: 0.76711 - acc: 0.9113 -- iter: 11/11\n",
      "--\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.72163\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 719 | loss: 0.72163 - acc: 0.9201 -- iter: 11/11\n",
      "--\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.68057\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 720 | loss: 0.68057 - acc: 0.9281 -- iter: 11/11\n",
      "--\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.64349\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 721 | loss: 0.64349 - acc: 0.9353 -- iter: 11/11\n",
      "--\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.60996\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 722 | loss: 0.60996 - acc: 0.9418 -- iter: 11/11\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.57963\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 723 | loss: 0.57963 - acc: 0.9476 -- iter: 11/11\n",
      "--\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.55216\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 724 | loss: 0.55216 - acc: 0.9528 -- iter: 11/11\n",
      "--\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.52725\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 725 | loss: 0.52725 - acc: 0.9576 -- iter: 11/11\n",
      "--\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.50463\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 726 | loss: 0.50463 - acc: 0.9618 -- iter: 11/11\n",
      "--\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.48408\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 727 | loss: 0.48408 - acc: 0.9656 -- iter: 11/11\n",
      "--\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.46538\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 728 | loss: 0.46538 - acc: 0.9691 -- iter: 11/11\n",
      "--\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.44833\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 729 | loss: 0.44833 - acc: 0.9722 -- iter: 11/11\n",
      "--\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.43277\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 730 | loss: 0.43277 - acc: 0.9749 -- iter: 11/11\n",
      "--\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.41854\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 731 | loss: 0.41854 - acc: 0.9774 -- iter: 11/11\n",
      "--\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.40551\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 732 | loss: 0.40551 - acc: 0.9797 -- iter: 11/11\n",
      "--\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.39355\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 733 | loss: 0.39355 - acc: 0.9817 -- iter: 11/11\n",
      "--\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.38256\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 734 | loss: 0.38256 - acc: 0.9836 -- iter: 11/11\n",
      "--\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.37243\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 735 | loss: 0.37243 - acc: 0.9852 -- iter: 11/11\n",
      "--\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.85048\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 736 | loss: 0.85048 - acc: 0.8958 -- iter: 11/11\n",
      "--\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.79327\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 737 | loss: 0.79327 - acc: 0.9062 -- iter: 11/11\n",
      "--\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.74171\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 738 | loss: 0.74171 - acc: 0.9156 -- iter: 11/11\n",
      "--\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.69522\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 739 | loss: 0.69522 - acc: 0.9240 -- iter: 11/11\n",
      "--\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.65327\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 740 | loss: 0.65327 - acc: 0.9316 -- iter: 11/11\n",
      "--\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.61541\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 741 | loss: 0.61541 - acc: 0.9385 -- iter: 11/11\n",
      "--\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m1.11192\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 742 | loss: 1.11192 - acc: 0.8537 -- iter: 11/11\n",
      "--\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m1.02813\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 743 | loss: 1.02813 - acc: 0.8683 -- iter: 11/11\n",
      "--\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.95275\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 744 | loss: 0.95275 - acc: 0.8815 -- iter: 11/11\n",
      "--\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.88491\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 745 | loss: 0.88491 - acc: 0.8933 -- iter: 11/11\n",
      "--\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.82385\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 746 | loss: 0.82385 - acc: 0.9040 -- iter: 11/11\n",
      "--\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.76885\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 747 | loss: 0.76885 - acc: 0.9136 -- iter: 11/11\n",
      "--\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.71929\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 748 | loss: 0.71929 - acc: 0.9223 -- iter: 11/11\n",
      "--\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.67461\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 749 | loss: 0.67461 - acc: 0.9300 -- iter: 11/11\n",
      "--\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.63430\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 750 | loss: 0.63430 - acc: 0.9370 -- iter: 11/11\n",
      "--\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.59791\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 751 | loss: 0.59791 - acc: 0.9433 -- iter: 11/11\n",
      "--\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.56504\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 752 | loss: 0.56504 - acc: 0.9490 -- iter: 11/11\n",
      "--\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.53532\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 753 | loss: 0.53532 - acc: 0.9541 -- iter: 11/11\n",
      "--\n",
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m1.00010\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 754 | loss: 1.00010 - acc: 0.8678 -- iter: 11/11\n",
      "--\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.92677\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 755 | loss: 0.92677 - acc: 0.8810 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.86079\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 756 | loss: 0.86079 - acc: 0.8929 -- iter: 11/11\n",
      "--\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.80140\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 757 | loss: 0.80140 - acc: 0.9036 -- iter: 11/11\n",
      "--\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.74793\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 758 | loss: 0.74793 - acc: 0.9132 -- iter: 11/11\n",
      "--\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.69974\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 759 | loss: 0.69974 - acc: 0.9219 -- iter: 11/11\n",
      "--\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.65631\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 760 | loss: 0.65631 - acc: 0.9297 -- iter: 11/11\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.61713\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 761 | loss: 0.61713 - acc: 0.9368 -- iter: 11/11\n",
      "--\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m1.14268\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 762 | loss: 1.14268 - acc: 0.8522 -- iter: 11/11\n",
      "--\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m1.05487\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 763 | loss: 1.05487 - acc: 0.8670 -- iter: 11/11\n",
      "--\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m1.56681\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 764 | loss: 1.56681 - acc: 0.7803 -- iter: 11/11\n",
      "--\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m1.43693\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 765 | loss: 1.43693 - acc: 0.8022 -- iter: 11/11\n",
      "--\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m1.32028\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 766 | loss: 1.32028 - acc: 0.8220 -- iter: 11/11\n",
      "--\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m1.21547\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 767 | loss: 1.21547 - acc: 0.8398 -- iter: 11/11\n",
      "--\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m1.12129\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 768 | loss: 1.12129 - acc: 0.8558 -- iter: 11/11\n",
      "--\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m1.03664\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 769 | loss: 1.03664 - acc: 0.8702 -- iter: 11/11\n",
      "--\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.96052\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 770 | loss: 0.96052 - acc: 0.8832 -- iter: 11/11\n",
      "--\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.89204\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 771 | loss: 0.89204 - acc: 0.8949 -- iter: 11/11\n",
      "--\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.83042\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 772 | loss: 0.83042 - acc: 0.9054 -- iter: 11/11\n",
      "--\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.77493\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 773 | loss: 0.77493 - acc: 0.9149 -- iter: 11/11\n",
      "--\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.72495\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 774 | loss: 0.72495 - acc: 0.9234 -- iter: 11/11\n",
      "--\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.67990\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 775 | loss: 0.67990 - acc: 0.9310 -- iter: 11/11\n",
      "--\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.63926\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 776 | loss: 0.63926 - acc: 0.9379 -- iter: 11/11\n",
      "--\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.60258\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 777 | loss: 0.60258 - acc: 0.9441 -- iter: 11/11\n",
      "--\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.56945\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 778 | loss: 0.56945 - acc: 0.9497 -- iter: 11/11\n",
      "--\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.53949\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 779 | loss: 0.53949 - acc: 0.9548 -- iter: 11/11\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.51238\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 780 | loss: 0.51238 - acc: 0.9593 -- iter: 11/11\n",
      "--\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.48782\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 781 | loss: 0.48782 - acc: 0.9634 -- iter: 11/11\n",
      "--\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.46555\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 782 | loss: 0.46555 - acc: 0.9670 -- iter: 11/11\n",
      "--\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.44533\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 783 | loss: 0.44533 - acc: 0.9703 -- iter: 11/11\n",
      "--\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.42694\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 784 | loss: 0.42694 - acc: 0.9733 -- iter: 11/11\n",
      "--\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.41021\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 785 | loss: 0.41021 - acc: 0.9760 -- iter: 11/11\n",
      "--\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.39496\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 786 | loss: 0.39496 - acc: 0.9784 -- iter: 11/11\n",
      "--\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.38103\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 787 | loss: 0.38103 - acc: 0.9805 -- iter: 11/11\n",
      "--\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.36830\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 788 | loss: 0.36830 - acc: 0.9825 -- iter: 11/11\n",
      "--\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.35664\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 789 | loss: 0.35664 - acc: 0.9842 -- iter: 11/11\n",
      "--\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.34594\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 790 | loss: 0.34594 - acc: 0.9858 -- iter: 11/11\n",
      "--\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.33611\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 791 | loss: 0.33611 - acc: 0.9872 -- iter: 11/11\n",
      "--\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.32706\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 792 | loss: 0.32706 - acc: 0.9885 -- iter: 11/11\n",
      "--\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.31870\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 793 | loss: 0.31870 - acc: 0.9896 -- iter: 11/11\n",
      "--\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.31098\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 794 | loss: 0.31098 - acc: 0.9907 -- iter: 11/11\n",
      "--\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.30382\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 795 | loss: 0.30382 - acc: 0.9916 -- iter: 11/11\n",
      "--\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.29717\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 796 | loss: 0.29717 - acc: 0.9925 -- iter: 11/11\n",
      "--\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.29098\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 797 | loss: 0.29098 - acc: 0.9932 -- iter: 11/11\n",
      "--\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.28521\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 798 | loss: 0.28521 - acc: 0.9939 -- iter: 11/11\n",
      "--\n",
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.27981\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 799 | loss: 0.27981 - acc: 0.9945 -- iter: 11/11\n",
      "--\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.27475\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 800 | loss: 0.27475 - acc: 0.9950 -- iter: 11/11\n",
      "--\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.27000\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 801 | loss: 0.27000 - acc: 0.9955 -- iter: 11/11\n",
      "--\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.26553\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 802 | loss: 0.26553 - acc: 0.9960 -- iter: 11/11\n",
      "--\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.26130\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 803 | loss: 0.26130 - acc: 0.9964 -- iter: 11/11\n",
      "--\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.25730\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 804 | loss: 0.25730 - acc: 0.9968 -- iter: 11/11\n",
      "--\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.25351\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 805 | loss: 0.25351 - acc: 0.9971 -- iter: 11/11\n",
      "--\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.24991\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 806 | loss: 0.24991 - acc: 0.9974 -- iter: 11/11\n",
      "--\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.24647\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 807 | loss: 0.24647 - acc: 0.9976 -- iter: 11/11\n",
      "--\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.24319\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 808 | loss: 0.24319 - acc: 0.9979 -- iter: 11/11\n",
      "--\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.24006\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 809 | loss: 0.24006 - acc: 0.9981 -- iter: 11/11\n",
      "--\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.23705\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 810 | loss: 0.23705 - acc: 0.9983 -- iter: 11/11\n",
      "--\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.23416\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 811 | loss: 0.23416 - acc: 0.9984 -- iter: 11/11\n",
      "--\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.89552\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 812 | loss: 0.89552 - acc: 0.8986 -- iter: 11/11\n",
      "--\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.82661\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 813 | loss: 0.82661 - acc: 0.9087 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.76460\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 814 | loss: 0.76460 - acc: 0.9179 -- iter: 11/11\n",
      "--\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.70877\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 815 | loss: 0.70877 - acc: 0.9261 -- iter: 11/11\n",
      "--\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.65849\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 816 | loss: 0.65849 - acc: 0.9335 -- iter: 11/11\n",
      "--\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.61321\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 817 | loss: 0.61321 - acc: 0.9401 -- iter: 11/11\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.57239\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 818 | loss: 0.57239 - acc: 0.9461 -- iter: 11/11\n",
      "--\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.53560\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 819 | loss: 0.53560 - acc: 0.9515 -- iter: 11/11\n",
      "--\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.50241\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 820 | loss: 0.50241 - acc: 0.9564 -- iter: 11/11\n",
      "--\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.47245\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 821 | loss: 0.47245 - acc: 0.9607 -- iter: 11/11\n",
      "--\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.44540\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 822 | loss: 0.44540 - acc: 0.9646 -- iter: 11/11\n",
      "--\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.42096\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 823 | loss: 0.42096 - acc: 0.9682 -- iter: 11/11\n",
      "--\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.39885\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 824 | loss: 0.39885 - acc: 0.9714 -- iter: 11/11\n",
      "--\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.37885\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 825 | loss: 0.37885 - acc: 0.9742 -- iter: 11/11\n",
      "--\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.36073\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 826 | loss: 0.36073 - acc: 0.9768 -- iter: 11/11\n",
      "--\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.34430\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 827 | loss: 0.34430 - acc: 0.9791 -- iter: 11/11\n",
      "--\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.32939\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 828 | loss: 0.32939 - acc: 0.9812 -- iter: 11/11\n",
      "--\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.31584\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 829 | loss: 0.31584 - acc: 0.9831 -- iter: 11/11\n",
      "--\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.30352\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 830 | loss: 0.30352 - acc: 0.9848 -- iter: 11/11\n",
      "--\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.29230\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 831 | loss: 0.29230 - acc: 0.9863 -- iter: 11/11\n",
      "--\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.28206\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 832 | loss: 0.28206 - acc: 0.9877 -- iter: 11/11\n",
      "--\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.27272\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 833 | loss: 0.27272 - acc: 0.9889 -- iter: 11/11\n",
      "--\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.26416\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 834 | loss: 0.26416 - acc: 0.9900 -- iter: 11/11\n",
      "--\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.25633\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 835 | loss: 0.25633 - acc: 0.9910 -- iter: 11/11\n",
      "--\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.24914\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 836 | loss: 0.24914 - acc: 0.9919 -- iter: 11/11\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.24253\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 837 | loss: 0.24253 - acc: 0.9927 -- iter: 11/11\n",
      "--\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.91358\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 838 | loss: 0.91358 - acc: 0.8934 -- iter: 11/11\n",
      "--\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.84043\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 839 | loss: 0.84043 - acc: 0.9041 -- iter: 11/11\n",
      "--\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.77461\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 840 | loss: 0.77461 - acc: 0.9137 -- iter: 11/11\n",
      "--\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.71539\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 841 | loss: 0.71539 - acc: 0.9223 -- iter: 11/11\n",
      "--\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.66209\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 842 | loss: 0.66209 - acc: 0.9301 -- iter: 11/11\n",
      "--\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.61410\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 843 | loss: 0.61410 - acc: 0.9371 -- iter: 11/11\n",
      "--\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m1.32984\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 844 | loss: 1.32984 - acc: 0.8434 -- iter: 11/11\n",
      "--\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m1.21524\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 845 | loss: 1.21524 - acc: 0.8590 -- iter: 11/11\n",
      "--\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m1.11225\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 846 | loss: 1.11225 - acc: 0.8731 -- iter: 11/11\n",
      "--\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m1.01968\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 847 | loss: 1.01968 - acc: 0.8858 -- iter: 11/11\n",
      "--\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.93647\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 848 | loss: 0.93647 - acc: 0.8972 -- iter: 11/11\n",
      "--\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.86166\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 849 | loss: 0.86166 - acc: 0.9075 -- iter: 11/11\n",
      "--\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.79439\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 850 | loss: 0.79439 - acc: 0.9168 -- iter: 11/11\n",
      "--\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.73387\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 851 | loss: 0.73387 - acc: 0.9251 -- iter: 11/11\n",
      "--\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m1.41098\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 852 | loss: 1.41098 - acc: 0.8326 -- iter: 11/11\n",
      "--\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m1.28904\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 853 | loss: 1.28904 - acc: 0.8493 -- iter: 11/11\n",
      "--\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m1.17947\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 854 | loss: 1.17947 - acc: 0.8644 -- iter: 11/11\n",
      "--\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m1.08102\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 855 | loss: 1.08102 - acc: 0.8779 -- iter: 11/11\n",
      "--\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.99253\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 856 | loss: 0.99253 - acc: 0.8902 -- iter: 11/11\n",
      "--\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.91299\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 857 | loss: 0.91299 - acc: 0.9011 -- iter: 11/11\n",
      "--\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m1.21404\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 858 | loss: 1.21404 - acc: 0.8201 -- iter: 11/11\n",
      "--\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m1.11258\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 859 | loss: 1.11258 - acc: 0.8381 -- iter: 11/11\n",
      "--\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m1.02141\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 860 | loss: 1.02141 - acc: 0.8543 -- iter: 11/11\n",
      "--\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.93947\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 861 | loss: 0.93947 - acc: 0.8689 -- iter: 11/11\n",
      "--\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m1.23329\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 862 | loss: 1.23329 - acc: 0.8183 -- iter: 11/11\n",
      "--\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m1.13041\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 863 | loss: 1.13041 - acc: 0.8365 -- iter: 11/11\n",
      "--\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m1.03796\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 864 | loss: 1.03796 - acc: 0.8529 -- iter: 11/11\n",
      "--\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.95486\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 865 | loss: 0.95486 - acc: 0.8676 -- iter: 11/11\n",
      "--\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.88015\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 866 | loss: 0.88015 - acc: 0.8808 -- iter: 11/11\n",
      "--\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.81296\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 867 | loss: 0.81296 - acc: 0.8927 -- iter: 11/11\n",
      "--\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m1.22757\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 868 | loss: 1.22757 - acc: 0.8126 -- iter: 11/11\n",
      "--\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m1.12582\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 869 | loss: 1.12582 - acc: 0.8313 -- iter: 11/11\n",
      "--\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m1.03436\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 870 | loss: 1.03436 - acc: 0.8482 -- iter: 11/11\n",
      "--\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.95213\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 871 | loss: 0.95213 - acc: 0.8633 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m1.33289\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 872 | loss: 1.33289 - acc: 0.7770 -- iter: 11/11\n",
      "--\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m1.22106\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 873 | loss: 1.22106 - acc: 0.7993 -- iter: 11/11\n",
      "--\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m1.12057\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 874 | loss: 1.12057 - acc: 0.8194 -- iter: 11/11\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m1.03026\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 875 | loss: 1.03026 - acc: 0.8374 -- iter: 11/11\n",
      "--\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m1.59830\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 876 | loss: 1.59830 - acc: 0.7628 -- iter: 11/11\n",
      "--\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m1.46059\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 877 | loss: 1.46059 - acc: 0.7865 -- iter: 11/11\n",
      "--\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m1.33688\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 878 | loss: 1.33688 - acc: 0.8079 -- iter: 11/11\n",
      "--\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m1.22573\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 879 | loss: 1.22573 - acc: 0.8271 -- iter: 11/11\n",
      "--\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m1.73416\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 880 | loss: 1.73416 - acc: 0.7444 -- iter: 11/11\n",
      "--\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m1.58375\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 881 | loss: 1.58375 - acc: 0.7699 -- iter: 11/11\n",
      "--\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m1.44866\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 882 | loss: 1.44866 - acc: 0.7929 -- iter: 11/11\n",
      "--\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m1.32731\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 883 | loss: 1.32731 - acc: 0.8136 -- iter: 11/11\n",
      "--\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m1.21827\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 884 | loss: 1.21827 - acc: 0.8323 -- iter: 11/11\n",
      "--\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m1.12028\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 885 | loss: 1.12028 - acc: 0.8491 -- iter: 11/11\n",
      "--\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m1.58032\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 886 | loss: 1.58032 - acc: 0.7641 -- iter: 11/11\n",
      "--\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m1.44652\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 887 | loss: 1.44652 - acc: 0.7877 -- iter: 11/11\n",
      "--\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m1.32632\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 888 | loss: 1.32632 - acc: 0.8090 -- iter: 11/11\n",
      "--\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m1.21834\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 889 | loss: 1.21834 - acc: 0.8281 -- iter: 11/11\n",
      "--\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m1.12129\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 890 | loss: 1.12129 - acc: 0.8453 -- iter: 11/11\n",
      "--\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m1.03407\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 891 | loss: 1.03407 - acc: 0.8607 -- iter: 11/11\n",
      "--\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.95564\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 892 | loss: 0.95564 - acc: 0.8747 -- iter: 11/11\n",
      "--\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.88510\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 893 | loss: 0.88510 - acc: 0.8872 -- iter: 11/11\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.82163\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 894 | loss: 0.82163 - acc: 0.8985 -- iter: 11/11\n",
      "--\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.76450\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 895 | loss: 0.76450 - acc: 0.9086 -- iter: 11/11\n",
      "--\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m1.35948\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 896 | loss: 1.35948 - acc: 0.8178 -- iter: 11/11\n",
      "--\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m1.24871\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 897 | loss: 1.24871 - acc: 0.8360 -- iter: 11/11\n",
      "--\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m1.14917\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 898 | loss: 1.14917 - acc: 0.8524 -- iter: 11/11\n",
      "--\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m1.05968\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 899 | loss: 1.05968 - acc: 0.8671 -- iter: 11/11\n",
      "--\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.97922\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 900 | loss: 0.97922 - acc: 0.8804 -- iter: 11/11\n",
      "--\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.90685\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 901 | loss: 0.90685 - acc: 0.8924 -- iter: 11/11\n",
      "--\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m1.35575\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 902 | loss: 1.35575 - acc: 0.8032 -- iter: 11/11\n",
      "--\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m1.24591\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 903 | loss: 1.24591 - acc: 0.8228 -- iter: 11/11\n",
      "--\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m1.14720\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 904 | loss: 1.14720 - acc: 0.8406 -- iter: 11/11\n",
      "--\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m1.05846\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 905 | loss: 1.05846 - acc: 0.8565 -- iter: 11/11\n",
      "--\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m1.55155\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 906 | loss: 1.55155 - acc: 0.7799 -- iter: 11/11\n",
      "--\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m1.42271\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 907 | loss: 1.42271 - acc: 0.8019 -- iter: 11/11\n",
      "--\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m1.30696\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 908 | loss: 1.30696 - acc: 0.8218 -- iter: 11/11\n",
      "--\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m1.20296\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 909 | loss: 1.20296 - acc: 0.8396 -- iter: 11/11\n",
      "--\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m1.65394\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 910 | loss: 1.65394 - acc: 0.7556 -- iter: 11/11\n",
      "--\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m1.51568\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 911 | loss: 1.51568 - acc: 0.7801 -- iter: 11/11\n",
      "--\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m1.78774\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 912 | loss: 1.78774 - acc: 0.7384 -- iter: 11/11\n",
      "--\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m1.63672\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 913 | loss: 1.63672 - acc: 0.7646 -- iter: 11/11\n",
      "--\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m1.92252\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 914 | loss: 1.92252 - acc: 0.6972 -- iter: 11/11\n",
      "--\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m1.75879\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 915 | loss: 1.75879 - acc: 0.7275 -- iter: 11/11\n",
      "--\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m1.61183\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 916 | loss: 1.61183 - acc: 0.7547 -- iter: 11/11\n",
      "--\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m1.47989\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 917 | loss: 1.47989 - acc: 0.7793 -- iter: 11/11\n",
      "--\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m1.36143\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 918 | loss: 1.36143 - acc: 0.8013 -- iter: 11/11\n",
      "--\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m1.25502\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 919 | loss: 1.25502 - acc: 0.8212 -- iter: 11/11\n",
      "--\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m1.15942\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 920 | loss: 1.15942 - acc: 0.8391 -- iter: 11/11\n",
      "--\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m1.07350\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 921 | loss: 1.07350 - acc: 0.8552 -- iter: 11/11\n",
      "--\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.99625\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 922 | loss: 0.99625 - acc: 0.8697 -- iter: 11/11\n",
      "--\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.92676\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 923 | loss: 0.92676 - acc: 0.8827 -- iter: 11/11\n",
      "--\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.86421\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 924 | loss: 0.86421 - acc: 0.8944 -- iter: 11/11\n",
      "--\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.80788\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 925 | loss: 0.80788 - acc: 0.9050 -- iter: 11/11\n",
      "--\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.75713\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 926 | loss: 0.75713 - acc: 0.9145 -- iter: 11/11\n",
      "--\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.71136\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 927 | loss: 0.71136 - acc: 0.9230 -- iter: 11/11\n",
      "--\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.67005\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 928 | loss: 0.67005 - acc: 0.9307 -- iter: 11/11\n",
      "--\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.63274\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 929 | loss: 0.63274 - acc: 0.9377 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.59900\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 930 | loss: 0.59900 - acc: 0.9439 -- iter: 11/11\n",
      "--\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.56847\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 931 | loss: 0.56847 - acc: 0.9495 -- iter: 11/11\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.54082\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 932 | loss: 0.54082 - acc: 0.9546 -- iter: 11/11\n",
      "--\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.51573\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 933 | loss: 0.51573 - acc: 0.9591 -- iter: 11/11\n",
      "--\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.49295\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 934 | loss: 0.49295 - acc: 0.9632 -- iter: 11/11\n",
      "--\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.47223\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 935 | loss: 0.47223 - acc: 0.9669 -- iter: 11/11\n",
      "--\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.45337\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 936 | loss: 0.45337 - acc: 0.9702 -- iter: 11/11\n",
      "--\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.43617\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 937 | loss: 0.43617 - acc: 0.9732 -- iter: 11/11\n",
      "--\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.97726\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 938 | loss: 0.97726 - acc: 0.8758 -- iter: 11/11\n",
      "--\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.90744\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 939 | loss: 0.90744 - acc: 0.8883 -- iter: 11/11\n",
      "--\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.84456\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 940 | loss: 0.84456 - acc: 0.8994 -- iter: 11/11\n",
      "--\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.78791\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 941 | loss: 0.78791 - acc: 0.9095 -- iter: 11/11\n",
      "--\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.73684\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 942 | loss: 0.73684 - acc: 0.9185 -- iter: 11/11\n",
      "--\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.69078\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 943 | loss: 0.69078 - acc: 0.9267 -- iter: 11/11\n",
      "--\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.64921\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 944 | loss: 0.64921 - acc: 0.9340 -- iter: 11/11\n",
      "--\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.61167\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 945 | loss: 0.61167 - acc: 0.9406 -- iter: 11/11\n",
      "--\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.57775\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 946 | loss: 0.57775 - acc: 0.9466 -- iter: 11/11\n",
      "--\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.54706\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 947 | loss: 0.54706 - acc: 0.9519 -- iter: 11/11\n",
      "--\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.51927\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 948 | loss: 0.51927 - acc: 0.9567 -- iter: 11/11\n",
      "--\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.49410\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 949 | loss: 0.49410 - acc: 0.9610 -- iter: 11/11\n",
      "--\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.75540\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 950 | loss: 0.75540 - acc: 0.9104 -- iter: 11/11\n",
      "--\n",
      "Training Step: 951  | total loss: \u001b[1m\u001b[32m0.70640\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 951 | loss: 0.70640 - acc: 0.9194 -- iter: 11/11\n",
      "--\n",
      "Training Step: 952  | total loss: \u001b[1m\u001b[32m0.66223\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 952 | loss: 0.66223 - acc: 0.9274 -- iter: 11/11\n",
      "--\n",
      "Training Step: 953  | total loss: \u001b[1m\u001b[32m0.62241\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 953 | loss: 0.62241 - acc: 0.9347 -- iter: 11/11\n",
      "--\n",
      "Training Step: 954  | total loss: \u001b[1m\u001b[32m0.58647\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 954 | loss: 0.58647 - acc: 0.9412 -- iter: 11/11\n",
      "--\n",
      "Training Step: 955  | total loss: \u001b[1m\u001b[32m0.55402\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 955 | loss: 0.55402 - acc: 0.9471 -- iter: 11/11\n",
      "--\n",
      "Training Step: 956  | total loss: \u001b[1m\u001b[32m0.52469\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 956 | loss: 0.52469 - acc: 0.9524 -- iter: 11/11\n",
      "--\n",
      "Training Step: 957  | total loss: \u001b[1m\u001b[32m0.49816\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 957 | loss: 0.49816 - acc: 0.9571 -- iter: 11/11\n",
      "--\n",
      "Training Step: 958  | total loss: \u001b[1m\u001b[32m0.93838\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 958 | loss: 0.93838 - acc: 0.8796 -- iter: 11/11\n",
      "--\n",
      "Training Step: 959  | total loss: \u001b[1m\u001b[32m0.87035\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 959 | loss: 0.87035 - acc: 0.8916 -- iter: 11/11\n",
      "--\n",
      "Training Step: 960  | total loss: \u001b[1m\u001b[32m0.80912\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 960 | loss: 0.80912 - acc: 0.9025 -- iter: 11/11\n",
      "--\n",
      "Training Step: 961  | total loss: \u001b[1m\u001b[32m0.75397\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 961 | loss: 0.75397 - acc: 0.9122 -- iter: 11/11\n",
      "--\n",
      "Training Step: 962  | total loss: \u001b[1m\u001b[32m0.70429\u001b[0m\u001b[0m | time: 0.026s\n",
      "| Adam | epoch: 962 | loss: 0.70429 - acc: 0.9210 -- iter: 11/11\n",
      "--\n",
      "Training Step: 963  | total loss: \u001b[1m\u001b[32m0.65951\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 963 | loss: 0.65951 - acc: 0.9289 -- iter: 11/11\n",
      "--\n",
      "Training Step: 964  | total loss: \u001b[1m\u001b[32m0.61912\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 964 | loss: 0.61912 - acc: 0.9360 -- iter: 11/11\n",
      "--\n",
      "Training Step: 965  | total loss: \u001b[1m\u001b[32m0.58268\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 965 | loss: 0.58268 - acc: 0.9424 -- iter: 11/11\n",
      "--\n",
      "Training Step: 966  | total loss: \u001b[1m\u001b[32m0.54976\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 966 | loss: 0.54976 - acc: 0.9482 -- iter: 11/11\n",
      "--\n",
      "Training Step: 967  | total loss: \u001b[1m\u001b[32m0.52001\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 967 | loss: 0.52001 - acc: 0.9534 -- iter: 11/11\n",
      "--\n",
      "Training Step: 968  | total loss: \u001b[1m\u001b[32m0.49309\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 968 | loss: 0.49309 - acc: 0.9580 -- iter: 11/11\n",
      "--\n",
      "Training Step: 969  | total loss: \u001b[1m\u001b[32m0.46873\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 969 | loss: 0.46873 - acc: 0.9622 -- iter: 11/11\n",
      "--\n",
      "Training Step: 970  | total loss: \u001b[1m\u001b[32m0.44665\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 970 | loss: 0.44665 - acc: 0.9660 -- iter: 11/11\n",
      "--\n",
      "Training Step: 971  | total loss: \u001b[1m\u001b[32m0.42662\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 971 | loss: 0.42662 - acc: 0.9694 -- iter: 11/11\n",
      "--\n",
      "Training Step: 972  | total loss: \u001b[1m\u001b[32m0.40843\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 972 | loss: 0.40843 - acc: 0.9725 -- iter: 11/11\n",
      "--\n",
      "Training Step: 973  | total loss: \u001b[1m\u001b[32m0.39188\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 973 | loss: 0.39188 - acc: 0.9752 -- iter: 11/11\n",
      "--\n",
      "Training Step: 974  | total loss: \u001b[1m\u001b[32m0.82550\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 974 | loss: 0.82550 - acc: 0.8959 -- iter: 11/11\n",
      "--\n",
      "Training Step: 975  | total loss: \u001b[1m\u001b[32m0.76707\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 975 | loss: 0.76707 - acc: 0.9063 -- iter: 11/11\n",
      "--\n",
      "Training Step: 976  | total loss: \u001b[1m\u001b[32m0.71446\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 976 | loss: 0.71446 - acc: 0.9157 -- iter: 11/11\n",
      "--\n",
      "Training Step: 977  | total loss: \u001b[1m\u001b[32m0.66707\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 977 | loss: 0.66707 - acc: 0.9241 -- iter: 11/11\n",
      "--\n",
      "Training Step: 978  | total loss: \u001b[1m\u001b[32m0.62436\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 978 | loss: 0.62436 - acc: 0.9317 -- iter: 11/11\n",
      "--\n",
      "Training Step: 979  | total loss: \u001b[1m\u001b[32m0.58585\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 979 | loss: 0.58585 - acc: 0.9385 -- iter: 11/11\n",
      "--\n",
      "Training Step: 980  | total loss: \u001b[1m\u001b[32m0.55110\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 980 | loss: 0.55110 - acc: 0.9447 -- iter: 11/11\n",
      "--\n",
      "Training Step: 981  | total loss: \u001b[1m\u001b[32m0.51973\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 981 | loss: 0.51973 - acc: 0.9502 -- iter: 11/11\n",
      "--\n",
      "Training Step: 982  | total loss: \u001b[1m\u001b[32m0.49138\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 982 | loss: 0.49138 - acc: 0.9552 -- iter: 11/11\n",
      "--\n",
      "Training Step: 983  | total loss: \u001b[1m\u001b[32m0.46575\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 983 | loss: 0.46575 - acc: 0.9597 -- iter: 11/11\n",
      "--\n",
      "Training Step: 984  | total loss: \u001b[1m\u001b[32m0.44255\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 984 | loss: 0.44255 - acc: 0.9637 -- iter: 11/11\n",
      "--\n",
      "Training Step: 985  | total loss: \u001b[1m\u001b[32m0.42154\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 985 | loss: 0.42154 - acc: 0.9673 -- iter: 11/11\n",
      "--\n",
      "Training Step: 986  | total loss: \u001b[1m\u001b[32m0.40248\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 986 | loss: 0.40248 - acc: 0.9706 -- iter: 11/11\n",
      "--\n",
      "Training Step: 987  | total loss: \u001b[1m\u001b[32m0.38518\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 987 | loss: 0.38518 - acc: 0.9735 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 988  | total loss: \u001b[1m\u001b[32m0.36946\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 988 | loss: 0.36946 - acc: 0.9762 -- iter: 11/11\n",
      "--\n",
      "Training Step: 989  | total loss: \u001b[1m\u001b[32m0.35515\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 989 | loss: 0.35515 - acc: 0.9786 -- iter: 11/11\n",
      "--\n",
      "Training Step: 990  | total loss: \u001b[1m\u001b[32m0.98057\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 990 | loss: 0.98057 - acc: 0.8807 -- iter: 11/11\n",
      "--\n",
      "Training Step: 991  | total loss: \u001b[1m\u001b[32m0.90503\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 991 | loss: 0.90503 - acc: 0.8926 -- iter: 11/11\n",
      "--\n",
      "Training Step: 992  | total loss: \u001b[1m\u001b[32m1.23567\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 992 | loss: 1.23567 - acc: 0.8216 -- iter: 11/11\n",
      "--\n",
      "Training Step: 993  | total loss: \u001b[1m\u001b[32m1.13479\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 993 | loss: 1.13479 - acc: 0.8394 -- iter: 11/11\n",
      "--\n",
      "Training Step: 994  | total loss: \u001b[1m\u001b[32m1.04411\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 994 | loss: 1.04411 - acc: 0.8555 -- iter: 11/11\n",
      "--\n",
      "Training Step: 995  | total loss: \u001b[1m\u001b[32m0.96257\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 995 | loss: 0.96257 - acc: 0.8699 -- iter: 11/11\n",
      "--\n",
      "Training Step: 996  | total loss: \u001b[1m\u001b[32m0.88924\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 996 | loss: 0.88924 - acc: 0.8829 -- iter: 11/11\n",
      "--\n",
      "Training Step: 997  | total loss: \u001b[1m\u001b[32m0.82326\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 997 | loss: 0.82326 - acc: 0.8946 -- iter: 11/11\n",
      "--\n",
      "Training Step: 998  | total loss: \u001b[1m\u001b[32m1.31594\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 998 | loss: 1.31594 - acc: 0.8052 -- iter: 11/11\n",
      "--\n",
      "Training Step: 999  | total loss: \u001b[1m\u001b[32m1.20744\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 999 | loss: 1.20744 - acc: 0.8246 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1000  | total loss: \u001b[1m\u001b[32m1.10990\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1000 | loss: 1.10990 - acc: 0.8422 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1001  | total loss: \u001b[1m\u001b[32m1.02219\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1001 | loss: 1.02219 - acc: 0.8580 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1002  | total loss: \u001b[1m\u001b[32m0.94330\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1002 | loss: 0.94330 - acc: 0.8722 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1003  | total loss: \u001b[1m\u001b[32m0.87233\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1003 | loss: 0.87233 - acc: 0.8850 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1004  | total loss: \u001b[1m\u001b[32m0.80846\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1004 | loss: 0.80846 - acc: 0.8965 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1005  | total loss: \u001b[1m\u001b[32m0.75096\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1005 | loss: 0.75096 - acc: 0.9068 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1006  | total loss: \u001b[1m\u001b[32m0.69917\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1006 | loss: 0.69917 - acc: 0.9161 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1007  | total loss: \u001b[1m\u001b[32m0.65251\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1007 | loss: 0.65251 - acc: 0.9245 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1008  | total loss: \u001b[1m\u001b[32m1.11897\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1008 | loss: 1.11897 - acc: 0.8502 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1009  | total loss: \u001b[1m\u001b[32m1.03037\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1009 | loss: 1.03037 - acc: 0.8652 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1010  | total loss: \u001b[1m\u001b[32m0.95071\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1010 | loss: 0.95071 - acc: 0.8787 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1011  | total loss: \u001b[1m\u001b[32m0.87907\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1011 | loss: 0.87907 - acc: 0.8908 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1012  | total loss: \u001b[1m\u001b[32m0.81462\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1012 | loss: 0.81462 - acc: 0.9017 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1013  | total loss: \u001b[1m\u001b[32m0.75661\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1013 | loss: 0.75661 - acc: 0.9116 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1014  | total loss: \u001b[1m\u001b[32m0.70439\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1014 | loss: 0.70439 - acc: 0.9204 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1015  | total loss: \u001b[1m\u001b[32m0.65736\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1015 | loss: 0.65736 - acc: 0.9284 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1016  | total loss: \u001b[1m\u001b[32m1.14027\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1016 | loss: 1.14027 - acc: 0.8355 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1017  | total loss: \u001b[1m\u001b[32m1.04970\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1017 | loss: 1.04970 - acc: 0.8520 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1018  | total loss: \u001b[1m\u001b[32m0.96827\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1018 | loss: 0.96827 - acc: 0.8668 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1019  | total loss: \u001b[1m\u001b[32m0.89504\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1019 | loss: 0.89504 - acc: 0.8801 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1020  | total loss: \u001b[1m\u001b[32m0.82916\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1020 | loss: 0.82916 - acc: 0.8921 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1021  | total loss: \u001b[1m\u001b[32m0.76987\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1021 | loss: 0.76987 - acc: 0.9029 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1022  | total loss: \u001b[1m\u001b[32m1.21015\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1022 | loss: 1.21015 - acc: 0.8308 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1023  | total loss: \u001b[1m\u001b[32m1.11286\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1023 | loss: 1.11286 - acc: 0.8477 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1024  | total loss: \u001b[1m\u001b[32m1.02538\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1024 | loss: 1.02538 - acc: 0.8629 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1025  | total loss: \u001b[1m\u001b[32m0.94671\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1025 | loss: 0.94671 - acc: 0.8766 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1026  | total loss: \u001b[1m\u001b[32m0.87594\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1026 | loss: 0.87594 - acc: 0.8890 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1027  | total loss: \u001b[1m\u001b[32m0.81225\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1027 | loss: 0.81225 - acc: 0.9001 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1028  | total loss: \u001b[1m\u001b[32m1.26728\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1028 | loss: 1.26728 - acc: 0.8101 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1029  | total loss: \u001b[1m\u001b[32m1.16459\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1029 | loss: 1.16459 - acc: 0.8291 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1030  | total loss: \u001b[1m\u001b[32m1.07227\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1030 | loss: 1.07227 - acc: 0.8462 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1031  | total loss: \u001b[1m\u001b[32m0.98927\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1031 | loss: 0.98927 - acc: 0.8615 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1032  | total loss: \u001b[1m\u001b[32m0.91462\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1032 | loss: 0.91462 - acc: 0.8754 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1033  | total loss: \u001b[1m\u001b[32m0.84747\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1033 | loss: 0.84747 - acc: 0.8878 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1034  | total loss: \u001b[1m\u001b[32m0.78703\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1034 | loss: 0.78703 - acc: 0.8991 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1035  | total loss: \u001b[1m\u001b[32m0.73261\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1035 | loss: 0.73261 - acc: 0.9092 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1036  | total loss: \u001b[1m\u001b[32m0.68359\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1036 | loss: 0.68359 - acc: 0.9182 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1037  | total loss: \u001b[1m\u001b[32m0.63942\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1037 | loss: 0.63942 - acc: 0.9264 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1038  | total loss: \u001b[1m\u001b[32m0.59959\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1038 | loss: 0.59959 - acc: 0.9338 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1039  | total loss: \u001b[1m\u001b[32m0.56366\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1039 | loss: 0.56366 - acc: 0.9404 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1040  | total loss: \u001b[1m\u001b[32m1.06076\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1040 | loss: 1.06076 - acc: 0.8554 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1041  | total loss: \u001b[1m\u001b[32m0.97867\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1041 | loss: 0.97867 - acc: 0.8699 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1042  | total loss: \u001b[1m\u001b[32m1.51028\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1042 | loss: 1.51028 - acc: 0.7829 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1043  | total loss: \u001b[1m\u001b[32m1.38351\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1043 | loss: 1.38351 - acc: 0.8046 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1044  | total loss: \u001b[1m\u001b[32m1.26959\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1044 | loss: 1.26959 - acc: 0.8242 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1045  | total loss: \u001b[1m\u001b[32m1.16721\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1045 | loss: 1.16721 - acc: 0.8417 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1046  | total loss: \u001b[1m\u001b[32m1.66170\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1046 | loss: 1.66170 - acc: 0.7576 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1047  | total loss: \u001b[1m\u001b[32m1.52049\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1047 | loss: 1.52049 - acc: 0.7818 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1048  | total loss: \u001b[1m\u001b[32m1.39362\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1048 | loss: 1.39362 - acc: 0.8036 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1049  | total loss: \u001b[1m\u001b[32m1.27962\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1049 | loss: 1.27962 - acc: 0.8233 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1050  | total loss: \u001b[1m\u001b[32m1.72093\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1050 | loss: 1.72093 - acc: 0.7409 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1051  | total loss: \u001b[1m\u001b[32m1.57467\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1051 | loss: 1.57467 - acc: 0.7668 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1052  | total loss: \u001b[1m\u001b[32m1.44331\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1052 | loss: 1.44331 - acc: 0.7902 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1053  | total loss: \u001b[1m\u001b[32m1.32531\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1053 | loss: 1.32531 - acc: 0.8111 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1054  | total loss: \u001b[1m\u001b[32m1.21930\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1054 | loss: 1.21930 - acc: 0.8300 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1055  | total loss: \u001b[1m\u001b[32m1.12402\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1055 | loss: 1.12402 - acc: 0.8470 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1056  | total loss: \u001b[1m\u001b[32m1.43561\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1056 | loss: 1.43561 - acc: 0.7714 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1057  | total loss: \u001b[1m\u001b[32m1.31905\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1057 | loss: 1.31905 - acc: 0.7943 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1058  | total loss: \u001b[1m\u001b[32m1.21433\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1058 | loss: 1.21433 - acc: 0.8148 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1059  | total loss: \u001b[1m\u001b[32m1.12024\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1059 | loss: 1.12024 - acc: 0.8334 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1060  | total loss: \u001b[1m\u001b[32m1.03566\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1060 | loss: 1.03566 - acc: 0.8500 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1061  | total loss: \u001b[1m\u001b[32m0.95961\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1061 | loss: 0.95961 - acc: 0.8650 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1062  | total loss: \u001b[1m\u001b[32m0.89121\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1062 | loss: 0.89121 - acc: 0.8785 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1063  | total loss: \u001b[1m\u001b[32m0.82966\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1063 | loss: 0.82966 - acc: 0.8907 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1064  | total loss: \u001b[1m\u001b[32m0.77424\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1064 | loss: 0.77424 - acc: 0.9016 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1065  | total loss: \u001b[1m\u001b[32m0.72433\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1065 | loss: 0.72433 - acc: 0.9114 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1066  | total loss: \u001b[1m\u001b[32m0.67933\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1066 | loss: 0.67933 - acc: 0.9203 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1067  | total loss: \u001b[1m\u001b[32m0.63875\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1067 | loss: 0.63875 - acc: 0.9283 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1068  | total loss: \u001b[1m\u001b[32m0.60213\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1068 | loss: 0.60213 - acc: 0.9354 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1069  | total loss: \u001b[1m\u001b[32m0.56905\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1069 | loss: 0.56905 - acc: 0.9419 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1070  | total loss: \u001b[1m\u001b[32m0.53914\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1070 | loss: 0.53914 - acc: 0.9477 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1071  | total loss: \u001b[1m\u001b[32m0.51208\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1071 | loss: 0.51208 - acc: 0.9529 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1072  | total loss: \u001b[1m\u001b[32m0.92040\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1072 | loss: 0.92040 - acc: 0.8576 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1073  | total loss: \u001b[1m\u001b[32m0.85505\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1073 | loss: 0.85505 - acc: 0.8719 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1074  | total loss: \u001b[1m\u001b[32m0.79621\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1074 | loss: 0.79621 - acc: 0.8847 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1075  | total loss: \u001b[1m\u001b[32m0.74320\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1075 | loss: 0.74320 - acc: 0.8962 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1076  | total loss: \u001b[1m\u001b[32m0.69543\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1076 | loss: 0.69543 - acc: 0.9066 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1077  | total loss: \u001b[1m\u001b[32m0.65235\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1077 | loss: 0.65235 - acc: 0.9159 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1078  | total loss: \u001b[1m\u001b[32m0.61347\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1078 | loss: 0.61347 - acc: 0.9243 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1079  | total loss: \u001b[1m\u001b[32m0.57836\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1079 | loss: 0.57836 - acc: 0.9319 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1080  | total loss: \u001b[1m\u001b[32m0.54664\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1080 | loss: 0.54664 - acc: 0.9387 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1081  | total loss: \u001b[1m\u001b[32m0.51796\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1081 | loss: 0.51796 - acc: 0.9448 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1082  | total loss: \u001b[1m\u001b[32m0.49199\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1082 | loss: 0.49199 - acc: 0.9504 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1083  | total loss: \u001b[1m\u001b[32m0.46846\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1083 | loss: 0.46846 - acc: 0.9553 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1084  | total loss: \u001b[1m\u001b[32m0.77695\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1084 | loss: 0.77695 - acc: 0.8871 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1085  | total loss: \u001b[1m\u001b[32m0.72474\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1085 | loss: 0.72474 - acc: 0.8984 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1086  | total loss: \u001b[1m\u001b[32m0.67770\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1086 | loss: 0.67770 - acc: 0.9085 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1087  | total loss: \u001b[1m\u001b[32m0.63530\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1087 | loss: 0.63530 - acc: 0.9177 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1088  | total loss: \u001b[1m\u001b[32m0.59705\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1088 | loss: 0.59705 - acc: 0.9259 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1089  | total loss: \u001b[1m\u001b[32m0.56254\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1089 | loss: 0.56254 - acc: 0.9333 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1090  | total loss: \u001b[1m\u001b[32m0.53137\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1090 | loss: 0.53137 - acc: 0.9400 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1091  | total loss: \u001b[1m\u001b[32m0.50320\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1091 | loss: 0.50320 - acc: 0.9460 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1092  | total loss: \u001b[1m\u001b[32m0.47772\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1092 | loss: 0.47772 - acc: 0.9514 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1093  | total loss: \u001b[1m\u001b[32m0.45464\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1093 | loss: 0.45464 - acc: 0.9562 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1094  | total loss: \u001b[1m\u001b[32m0.43373\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1094 | loss: 0.43373 - acc: 0.9606 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1095  | total loss: \u001b[1m\u001b[32m0.41475\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1095 | loss: 0.41475 - acc: 0.9646 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1096  | total loss: \u001b[1m\u001b[32m0.39751\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1096 | loss: 0.39751 - acc: 0.9681 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1097  | total loss: \u001b[1m\u001b[32m0.38184\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1097 | loss: 0.38184 - acc: 0.9713 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1098  | total loss: \u001b[1m\u001b[32m0.36755\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1098 | loss: 0.36755 - acc: 0.9742 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1099  | total loss: \u001b[1m\u001b[32m0.35453\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1099 | loss: 0.35453 - acc: 0.9767 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.34263\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1100 | loss: 0.34263 - acc: 0.9791 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1101  | total loss: \u001b[1m\u001b[32m0.33174\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1101 | loss: 0.33174 - acc: 0.9812 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1102  | total loss: \u001b[1m\u001b[32m0.89407\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1102 | loss: 0.89407 - acc: 0.8830 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1103  | total loss: \u001b[1m\u001b[32m0.82787\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1103 | loss: 0.82787 - acc: 0.8947 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1104  | total loss: \u001b[1m\u001b[32m0.76830\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1104 | loss: 0.76830 - acc: 0.9053 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1105  | total loss: \u001b[1m\u001b[32m0.71466\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1105 | loss: 0.71466 - acc: 0.9147 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1106  | total loss: \u001b[1m\u001b[32m0.66634\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1106 | loss: 0.66634 - acc: 0.9233 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1107  | total loss: \u001b[1m\u001b[32m0.62280\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1107 | loss: 0.62280 - acc: 0.9309 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1108  | total loss: \u001b[1m\u001b[32m0.58354\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1108 | loss: 0.58354 - acc: 0.9378 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1109  | total loss: \u001b[1m\u001b[32m0.54813\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1109 | loss: 0.54813 - acc: 0.9441 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1110  | total loss: \u001b[1m\u001b[32m0.51616\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1110 | loss: 0.51616 - acc: 0.9497 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1111  | total loss: \u001b[1m\u001b[32m0.48728\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1111 | loss: 0.48728 - acc: 0.9547 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1112  | total loss: \u001b[1m\u001b[32m1.16037\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1112 | loss: 1.16037 - acc: 0.8683 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1113  | total loss: \u001b[1m\u001b[32m1.06706\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1113 | loss: 1.06706 - acc: 0.8815 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1114  | total loss: \u001b[1m\u001b[32m0.98316\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1114 | loss: 0.98316 - acc: 0.8933 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1115  | total loss: \u001b[1m\u001b[32m0.90769\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1115 | loss: 0.90769 - acc: 0.9040 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1116  | total loss: \u001b[1m\u001b[32m0.83980\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1116 | loss: 0.83980 - acc: 0.9136 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1117  | total loss: \u001b[1m\u001b[32m0.77869\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1117 | loss: 0.77869 - acc: 0.9222 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1118  | total loss: \u001b[1m\u001b[32m0.72368\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1118 | loss: 0.72368 - acc: 0.9300 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1119  | total loss: \u001b[1m\u001b[32m0.67413\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1119 | loss: 0.67413 - acc: 0.9370 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1120  | total loss: \u001b[1m\u001b[32m1.09072\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1120 | loss: 1.09072 - acc: 0.8615 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1121  | total loss: \u001b[1m\u001b[32m1.00451\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1121 | loss: 1.00451 - acc: 0.8753 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1122  | total loss: \u001b[1m\u001b[32m0.92699\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1122 | loss: 0.92699 - acc: 0.8878 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1123  | total loss: \u001b[1m\u001b[32m0.85727\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1123 | loss: 0.85727 - acc: 0.8990 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1124  | total loss: \u001b[1m\u001b[32m0.79453\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1124 | loss: 0.79453 - acc: 0.9091 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1125  | total loss: \u001b[1m\u001b[32m0.73806\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1125 | loss: 0.73806 - acc: 0.9182 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1126  | total loss: \u001b[1m\u001b[32m1.30569\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1126 | loss: 1.30569 - acc: 0.8264 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1127  | total loss: \u001b[1m\u001b[32m1.19825\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1127 | loss: 1.19825 - acc: 0.8438 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1128  | total loss: \u001b[1m\u001b[32m1.10169\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1128 | loss: 1.10169 - acc: 0.8594 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1129  | total loss: \u001b[1m\u001b[32m1.01488\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1129 | loss: 1.01488 - acc: 0.8734 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1130  | total loss: \u001b[1m\u001b[32m1.40807\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1130 | loss: 1.40807 - acc: 0.7952 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1131  | total loss: \u001b[1m\u001b[32m1.29090\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1131 | loss: 1.29090 - acc: 0.8157 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1132  | total loss: \u001b[1m\u001b[32m1.71908\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1132 | loss: 1.71908 - acc: 0.7341 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1133  | total loss: \u001b[1m\u001b[32m1.57128\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1133 | loss: 1.57128 - acc: 0.7607 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1134  | total loss: \u001b[1m\u001b[32m1.43851\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1134 | loss: 1.43851 - acc: 0.7846 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1135  | total loss: \u001b[1m\u001b[32m1.31922\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1135 | loss: 1.31922 - acc: 0.8062 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1136  | total loss: \u001b[1m\u001b[32m1.21203\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1136 | loss: 1.21203 - acc: 0.8255 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1137  | total loss: \u001b[1m\u001b[32m1.11568\u001b[0m\u001b[0m | time: 0.025s\n",
      "| Adam | epoch: 1137 | loss: 1.11568 - acc: 0.8430 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1138  | total loss: \u001b[1m\u001b[32m1.02907\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1138 | loss: 1.02907 - acc: 0.8587 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1139  | total loss: \u001b[1m\u001b[32m0.95118\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1139 | loss: 0.95118 - acc: 0.8728 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1140  | total loss: \u001b[1m\u001b[32m0.88111\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1140 | loss: 0.88111 - acc: 0.8855 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1141  | total loss: \u001b[1m\u001b[32m0.81805\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1141 | loss: 0.81805 - acc: 0.8970 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1142  | total loss: \u001b[1m\u001b[32m0.76129\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1142 | loss: 0.76129 - acc: 0.9073 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1143  | total loss: \u001b[1m\u001b[32m0.71016\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1143 | loss: 0.71016 - acc: 0.9166 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1144  | total loss: \u001b[1m\u001b[32m0.66409\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1144 | loss: 0.66409 - acc: 0.9249 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1145  | total loss: \u001b[1m\u001b[32m0.62255\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1145 | loss: 0.62255 - acc: 0.9324 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1146  | total loss: \u001b[1m\u001b[32m0.58508\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1146 | loss: 0.58508 - acc: 0.9392 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1147  | total loss: \u001b[1m\u001b[32m0.55125\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1147 | loss: 0.55125 - acc: 0.9453 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1148  | total loss: \u001b[1m\u001b[32m1.02555\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1148 | loss: 1.02555 - acc: 0.8507 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1149  | total loss: \u001b[1m\u001b[32m0.94761\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1149 | loss: 0.94761 - acc: 0.8657 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1150  | total loss: \u001b[1m\u001b[32m0.87749\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1150 | loss: 0.87749 - acc: 0.8791 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1151  | total loss: \u001b[1m\u001b[32m0.81439\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1151 | loss: 0.81439 - acc: 0.8912 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1152  | total loss: \u001b[1m\u001b[32m0.75757\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1152 | loss: 0.75757 - acc: 0.9021 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1153  | total loss: \u001b[1m\u001b[32m0.70640\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1153 | loss: 0.70640 - acc: 0.9119 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1154  | total loss: \u001b[1m\u001b[32m0.66028\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1154 | loss: 0.66028 - acc: 0.9207 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1155  | total loss: \u001b[1m\u001b[32m0.61870\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1155 | loss: 0.61870 - acc: 0.9286 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1156  | total loss: \u001b[1m\u001b[32m0.58119\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1156 | loss: 0.58119 - acc: 0.9357 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1157  | total loss: \u001b[1m\u001b[32m0.54733\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1157 | loss: 0.54733 - acc: 0.9422 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1158  | total loss: \u001b[1m\u001b[32m0.51674\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1158 | loss: 0.51674 - acc: 0.9480 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1159  | total loss: \u001b[1m\u001b[32m0.48909\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1159 | loss: 0.48909 - acc: 0.9532 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1160  | total loss: \u001b[1m\u001b[32m0.46407\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1160 | loss: 0.46407 - acc: 0.9578 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1161  | total loss: \u001b[1m\u001b[32m0.44141\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1161 | loss: 0.44141 - acc: 0.9621 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1162  | total loss: \u001b[1m\u001b[32m0.42088\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1162 | loss: 0.42088 - acc: 0.9659 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1163  | total loss: \u001b[1m\u001b[32m0.40224\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1163 | loss: 0.40224 - acc: 0.9693 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1164  | total loss: \u001b[1m\u001b[32m0.38531\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1164 | loss: 0.38531 - acc: 0.9723 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1165  | total loss: \u001b[1m\u001b[32m0.36992\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1165 | loss: 0.36992 - acc: 0.9751 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1166  | total loss: \u001b[1m\u001b[32m0.35590\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1166 | loss: 0.35590 - acc: 0.9776 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1167  | total loss: \u001b[1m\u001b[32m0.34311\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1167 | loss: 0.34311 - acc: 0.9798 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1168  | total loss: \u001b[1m\u001b[32m0.86544\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1168 | loss: 0.86544 - acc: 0.8909 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1169  | total loss: \u001b[1m\u001b[32m0.80154\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1169 | loss: 0.80154 - acc: 0.9018 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1170  | total loss: \u001b[1m\u001b[32m0.74401\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1170 | loss: 0.74401 - acc: 0.9117 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1171  | total loss: \u001b[1m\u001b[32m0.69222\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1171 | loss: 0.69222 - acc: 0.9205 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1172  | total loss: \u001b[1m\u001b[32m0.64555\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1172 | loss: 0.64555 - acc: 0.9284 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1173  | total loss: \u001b[1m\u001b[32m0.60350\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1173 | loss: 0.60350 - acc: 0.9356 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1174  | total loss: \u001b[1m\u001b[32m0.56558\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1174 | loss: 0.56558 - acc: 0.9420 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1175  | total loss: \u001b[1m\u001b[32m0.53137\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1175 | loss: 0.53137 - acc: 0.9478 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1176  | total loss: \u001b[1m\u001b[32m1.20553\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1176 | loss: 1.20553 - acc: 0.8531 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1177  | total loss: \u001b[1m\u001b[32m1.10738\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1177 | loss: 1.10738 - acc: 0.8677 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1178  | total loss: \u001b[1m\u001b[32m1.01914\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1178 | loss: 1.01914 - acc: 0.8810 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1179  | total loss: \u001b[1m\u001b[32m0.93981\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1179 | loss: 0.93981 - acc: 0.8929 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1180  | total loss: \u001b[1m\u001b[32m0.86847\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1180 | loss: 0.86847 - acc: 0.9036 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1181  | total loss: \u001b[1m\u001b[32m0.80429\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1181 | loss: 0.80429 - acc: 0.9132 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1182  | total loss: \u001b[1m\u001b[32m0.74654\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1182 | loss: 0.74654 - acc: 0.9219 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1183  | total loss: \u001b[1m\u001b[32m0.69454\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1183 | loss: 0.69454 - acc: 0.9297 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1184  | total loss: \u001b[1m\u001b[32m1.30397\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1184 | loss: 1.30397 - acc: 0.8367 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1185  | total loss: \u001b[1m\u001b[32m1.19638\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1185 | loss: 1.19638 - acc: 0.8531 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1186  | total loss: \u001b[1m\u001b[32m1.09969\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1186 | loss: 1.09969 - acc: 0.8678 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1187  | total loss: \u001b[1m\u001b[32m1.01279\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1187 | loss: 1.01279 - acc: 0.8810 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1188  | total loss: \u001b[1m\u001b[32m0.93466\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1188 | loss: 0.93466 - acc: 0.8929 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1189  | total loss: \u001b[1m\u001b[32m0.86441\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1189 | loss: 0.86441 - acc: 0.9036 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1190  | total loss: \u001b[1m\u001b[32m0.80121\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1190 | loss: 0.80121 - acc: 0.9132 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1191  | total loss: \u001b[1m\u001b[32m0.74434\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1191 | loss: 0.74434 - acc: 0.9219 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1192  | total loss: \u001b[1m\u001b[32m0.69314\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1192 | loss: 0.69314 - acc: 0.9297 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1193  | total loss: \u001b[1m\u001b[32m0.64703\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1193 | loss: 0.64703 - acc: 0.9368 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1194  | total loss: \u001b[1m\u001b[32m0.60549\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1194 | loss: 0.60549 - acc: 0.9431 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1195  | total loss: \u001b[1m\u001b[32m0.56803\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1195 | loss: 0.56803 - acc: 0.9488 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1196  | total loss: \u001b[1m\u001b[32m0.53424\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1196 | loss: 0.53424 - acc: 0.9539 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1197  | total loss: \u001b[1m\u001b[32m0.50374\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1197 | loss: 0.50374 - acc: 0.9585 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1198  | total loss: \u001b[1m\u001b[32m1.04616\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1198 | loss: 1.04616 - acc: 0.8717 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1199  | total loss: \u001b[1m\u001b[32m0.96443\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1199 | loss: 0.96443 - acc: 0.8846 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1200  | total loss: \u001b[1m\u001b[32m0.89092\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1200 | loss: 0.89092 - acc: 0.8961 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1201  | total loss: \u001b[1m\u001b[32m0.82478\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1201 | loss: 0.82478 - acc: 0.9065 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1202  | total loss: \u001b[1m\u001b[32m1.17349\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1202 | loss: 1.17349 - acc: 0.8249 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1203  | total loss: \u001b[1m\u001b[32m1.07922\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1203 | loss: 1.07922 - acc: 0.8424 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1204  | total loss: \u001b[1m\u001b[32m0.99447\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1204 | loss: 0.99447 - acc: 0.8582 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1205  | total loss: \u001b[1m\u001b[32m0.91826\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1205 | loss: 0.91826 - acc: 0.8724 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1206  | total loss: \u001b[1m\u001b[32m0.84972\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1206 | loss: 0.84972 - acc: 0.8851 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1207  | total loss: \u001b[1m\u001b[32m0.78805\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1207 | loss: 0.78805 - acc: 0.8966 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1208  | total loss: \u001b[1m\u001b[32m1.27002\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1208 | loss: 1.27002 - acc: 0.8070 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1209  | total loss: \u001b[1m\u001b[32m1.16648\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1209 | loss: 1.16648 - acc: 0.8263 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1210  | total loss: \u001b[1m\u001b[32m1.07342\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1210 | loss: 1.07342 - acc: 0.8436 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1211  | total loss: \u001b[1m\u001b[32m0.98977\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1211 | loss: 0.98977 - acc: 0.8593 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1212  | total loss: \u001b[1m\u001b[32m0.91455\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1212 | loss: 0.91455 - acc: 0.8734 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1213  | total loss: \u001b[1m\u001b[32m0.84688\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1213 | loss: 0.84688 - acc: 0.8860 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1214  | total loss: \u001b[1m\u001b[32m0.78600\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1214 | loss: 0.78600 - acc: 0.8974 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1215  | total loss: \u001b[1m\u001b[32m0.73119\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1215 | loss: 0.73119 - acc: 0.9077 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1216  | total loss: \u001b[1m\u001b[32m0.68184\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1216 | loss: 0.68184 - acc: 0.9169 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1217  | total loss: \u001b[1m\u001b[32m0.63737\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1217 | loss: 0.63737 - acc: 0.9252 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1218  | total loss: \u001b[1m\u001b[32m0.59729\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1218 | loss: 0.59729 - acc: 0.9327 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1219  | total loss: \u001b[1m\u001b[32m0.56113\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1219 | loss: 0.56113 - acc: 0.9394 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1220  | total loss: \u001b[1m\u001b[32m0.52850\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1220 | loss: 0.52850 - acc: 0.9455 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1221  | total loss: \u001b[1m\u001b[32m0.49903\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1221 | loss: 0.49903 - acc: 0.9509 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1222  | total loss: \u001b[1m\u001b[32m0.47239\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1222 | loss: 0.47239 - acc: 0.9558 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1223  | total loss: \u001b[1m\u001b[32m0.44829\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1223 | loss: 0.44829 - acc: 0.9603 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1224  | total loss: \u001b[1m\u001b[32m0.42646\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1224 | loss: 0.42646 - acc: 0.9642 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1225  | total loss: \u001b[1m\u001b[32m0.40669\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1225 | loss: 0.40669 - acc: 0.9678 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1226  | total loss: \u001b[1m\u001b[32m0.38875\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1226 | loss: 0.38875 - acc: 0.9710 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1227  | total loss: \u001b[1m\u001b[32m0.37245\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1227 | loss: 0.37245 - acc: 0.9739 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1228  | total loss: \u001b[1m\u001b[32m0.35763\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1228 | loss: 0.35763 - acc: 0.9765 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1229  | total loss: \u001b[1m\u001b[32m0.34414\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1229 | loss: 0.34414 - acc: 0.9789 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1230  | total loss: \u001b[1m\u001b[32m1.01436\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1230 | loss: 1.01436 - acc: 0.8901 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1231  | total loss: \u001b[1m\u001b[32m0.93511\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1231 | loss: 0.93511 - acc: 0.9011 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1232  | total loss: \u001b[1m\u001b[32m0.86382\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1232 | loss: 0.86382 - acc: 0.9110 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1233  | total loss: \u001b[1m\u001b[32m0.79968\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1233 | loss: 0.79968 - acc: 0.9199 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1234  | total loss: \u001b[1m\u001b[32m1.23909\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1234 | loss: 1.23909 - acc: 0.8370 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1235  | total loss: \u001b[1m\u001b[32m1.13757\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1235 | loss: 1.13757 - acc: 0.8533 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1236  | total loss: \u001b[1m\u001b[32m1.41727\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1236 | loss: 1.41727 - acc: 0.7952 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1237  | total loss: \u001b[1m\u001b[32m1.29827\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1237 | loss: 1.29827 - acc: 0.8157 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1238  | total loss: \u001b[1m\u001b[32m1.19136\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1238 | loss: 1.19136 - acc: 0.8341 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1239  | total loss: \u001b[1m\u001b[32m1.09531\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1239 | loss: 1.09531 - acc: 0.8507 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1240  | total loss: \u001b[1m\u001b[32m1.00899\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1240 | loss: 1.00899 - acc: 0.8656 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1241  | total loss: \u001b[1m\u001b[32m0.93139\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1241 | loss: 0.93139 - acc: 0.8791 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1242  | total loss: \u001b[1m\u001b[32m0.86161\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1242 | loss: 0.86161 - acc: 0.8912 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1243  | total loss: \u001b[1m\u001b[32m0.79885\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1243 | loss: 0.79885 - acc: 0.9021 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1244  | total loss: \u001b[1m\u001b[32m0.74238\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1244 | loss: 0.74238 - acc: 0.9118 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1245  | total loss: \u001b[1m\u001b[32m0.69154\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1245 | loss: 0.69154 - acc: 0.9207 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1246  | total loss: \u001b[1m\u001b[32m0.64575\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1246 | loss: 0.64575 - acc: 0.9286 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1247  | total loss: \u001b[1m\u001b[32m0.60449\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1247 | loss: 0.60449 - acc: 0.9357 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1248  | total loss: \u001b[1m\u001b[32m0.99273\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1248 | loss: 0.99273 - acc: 0.8513 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1249  | total loss: \u001b[1m\u001b[32m0.91675\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1249 | loss: 0.91675 - acc: 0.8661 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1250  | total loss: \u001b[1m\u001b[32m0.84840\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1250 | loss: 0.84840 - acc: 0.8795 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1251  | total loss: \u001b[1m\u001b[32m0.78689\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1251 | loss: 0.78689 - acc: 0.8916 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1252  | total loss: \u001b[1m\u001b[32m0.73150\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1252 | loss: 0.73150 - acc: 0.9024 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1253  | total loss: \u001b[1m\u001b[32m0.68162\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1253 | loss: 0.68162 - acc: 0.9122 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1254  | total loss: \u001b[1m\u001b[32m0.63667\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1254 | loss: 0.63667 - acc: 0.9210 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1255  | total loss: \u001b[1m\u001b[32m0.59615\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1255 | loss: 0.59615 - acc: 0.9289 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1256  | total loss: \u001b[1m\u001b[32m0.55959\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1256 | loss: 0.55959 - acc: 0.9360 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1257  | total loss: \u001b[1m\u001b[32m0.52658\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1257 | loss: 0.52658 - acc: 0.9424 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1258  | total loss: \u001b[1m\u001b[32m0.49677\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1258 | loss: 0.49677 - acc: 0.9481 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1259  | total loss: \u001b[1m\u001b[32m0.46982\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1259 | loss: 0.46982 - acc: 0.9533 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1260  | total loss: \u001b[1m\u001b[32m0.44543\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1260 | loss: 0.44543 - acc: 0.9580 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1261  | total loss: \u001b[1m\u001b[32m0.42335\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1261 | loss: 0.42335 - acc: 0.9622 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1262  | total loss: \u001b[1m\u001b[32m0.40334\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1262 | loss: 0.40334 - acc: 0.9660 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1263  | total loss: \u001b[1m\u001b[32m0.38518\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1263 | loss: 0.38518 - acc: 0.9694 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1264  | total loss: \u001b[1m\u001b[32m0.36868\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1264 | loss: 0.36868 - acc: 0.9724 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1265  | total loss: \u001b[1m\u001b[32m0.35368\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1265 | loss: 0.35368 - acc: 0.9752 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1266  | total loss: \u001b[1m\u001b[32m0.34002\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1266 | loss: 0.34002 - acc: 0.9777 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1267  | total loss: \u001b[1m\u001b[32m0.32757\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1267 | loss: 0.32757 - acc: 0.9799 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1268  | total loss: \u001b[1m\u001b[32m0.31620\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1268 | loss: 0.31620 - acc: 0.9819 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1269  | total loss: \u001b[1m\u001b[32m0.30581\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1269 | loss: 0.30581 - acc: 0.9837 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1270  | total loss: \u001b[1m\u001b[32m0.29629\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1270 | loss: 0.29629 - acc: 0.9854 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1271  | total loss: \u001b[1m\u001b[32m0.28755\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1271 | loss: 0.28755 - acc: 0.9868 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1272  | total loss: \u001b[1m\u001b[32m0.84276\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1272 | loss: 0.84276 - acc: 0.8972 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1273  | total loss: \u001b[1m\u001b[32m0.77922\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1273 | loss: 0.77922 - acc: 0.9075 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1274  | total loss: \u001b[1m\u001b[32m0.72204\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1274 | loss: 0.72204 - acc: 0.9168 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1275  | total loss: \u001b[1m\u001b[32m0.67055\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1275 | loss: 0.67055 - acc: 0.9251 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1276  | total loss: \u001b[1m\u001b[32m0.62417\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1276 | loss: 0.62417 - acc: 0.9326 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1277  | total loss: \u001b[1m\u001b[32m0.58238\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1277 | loss: 0.58238 - acc: 0.9393 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1278  | total loss: \u001b[1m\u001b[32m0.54471\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1278 | loss: 0.54471 - acc: 0.9454 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1279  | total loss: \u001b[1m\u001b[32m0.51073\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1279 | loss: 0.51073 - acc: 0.9508 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1280  | total loss: \u001b[1m\u001b[32m0.93557\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1280 | loss: 0.93557 - acc: 0.8830 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1281  | total loss: \u001b[1m\u001b[32m0.86248\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1281 | loss: 0.86248 - acc: 0.8947 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1282  | total loss: \u001b[1m\u001b[32m0.79673\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1282 | loss: 0.79673 - acc: 0.9053 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1283  | total loss: \u001b[1m\u001b[32m0.73756\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1283 | loss: 0.73756 - acc: 0.9147 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1284  | total loss: \u001b[1m\u001b[32m0.68431\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1284 | loss: 0.68431 - acc: 0.9233 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1285  | total loss: \u001b[1m\u001b[32m0.63637\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1285 | loss: 0.63637 - acc: 0.9309 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1286  | total loss: \u001b[1m\u001b[32m1.15001\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1286 | loss: 1.15001 - acc: 0.8378 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1287  | total loss: \u001b[1m\u001b[32m1.05559\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1287 | loss: 1.05559 - acc: 0.8541 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1288  | total loss: \u001b[1m\u001b[32m0.97070\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1288 | loss: 0.97070 - acc: 0.8686 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1289  | total loss: \u001b[1m\u001b[32m0.89438\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1289 | loss: 0.89438 - acc: 0.8818 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1290  | total loss: \u001b[1m\u001b[32m1.48643\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1290 | loss: 1.48643 - acc: 0.7936 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1291  | total loss: \u001b[1m\u001b[32m1.35881\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1291 | loss: 1.35881 - acc: 0.8142 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1292  | total loss: \u001b[1m\u001b[32m1.24413\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1292 | loss: 1.24413 - acc: 0.8328 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1293  | total loss: \u001b[1m\u001b[32m1.14107\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1293 | loss: 1.14107 - acc: 0.8495 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1294  | total loss: \u001b[1m\u001b[32m1.04843\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1294 | loss: 1.04843 - acc: 0.8646 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1295  | total loss: \u001b[1m\u001b[32m0.96514\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1295 | loss: 0.96514 - acc: 0.8781 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1296  | total loss: \u001b[1m\u001b[32m0.89024\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1296 | loss: 0.89024 - acc: 0.8903 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1297  | total loss: \u001b[1m\u001b[32m0.82287\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1297 | loss: 0.82287 - acc: 0.9013 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1298  | total loss: \u001b[1m\u001b[32m1.36782\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1298 | loss: 1.36782 - acc: 0.8202 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1299  | total loss: \u001b[1m\u001b[32m1.25291\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1299 | loss: 1.25291 - acc: 0.8382 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1300  | total loss: \u001b[1m\u001b[32m1.14967\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1300 | loss: 1.14967 - acc: 0.8544 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1301  | total loss: \u001b[1m\u001b[32m1.05689\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1301 | loss: 1.05689 - acc: 0.8690 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1302  | total loss: \u001b[1m\u001b[32m1.42774\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1302 | loss: 1.42774 - acc: 0.7912 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1303  | total loss: \u001b[1m\u001b[32m1.30748\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1303 | loss: 1.30748 - acc: 0.8120 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1304  | total loss: \u001b[1m\u001b[32m1.19944\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1304 | loss: 1.19944 - acc: 0.8308 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1305  | total loss: \u001b[1m\u001b[32m1.10234\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1305 | loss: 1.10234 - acc: 0.8478 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1306  | total loss: \u001b[1m\u001b[32m1.01506\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1306 | loss: 1.01506 - acc: 0.8630 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1307  | total loss: \u001b[1m\u001b[32m0.93659\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1307 | loss: 0.93659 - acc: 0.8767 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1308  | total loss: \u001b[1m\u001b[32m1.41254\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1308 | loss: 1.41254 - acc: 0.7890 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1309  | total loss: \u001b[1m\u001b[32m1.29460\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1309 | loss: 1.29460 - acc: 0.8101 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1310  | total loss: \u001b[1m\u001b[32m1.18863\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1310 | loss: 1.18863 - acc: 0.8291 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1311  | total loss: \u001b[1m\u001b[32m1.09340\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1311 | loss: 1.09340 - acc: 0.8462 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1312  | total loss: \u001b[1m\u001b[32m1.54327\u001b[0m\u001b[0m | time: 0.025s\n",
      "| Adam | epoch: 1312 | loss: 1.54327 - acc: 0.7707 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1313  | total loss: \u001b[1m\u001b[32m1.41297\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1313 | loss: 1.41297 - acc: 0.7936 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1314  | total loss: \u001b[1m\u001b[32m1.29594\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1314 | loss: 1.29594 - acc: 0.8142 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1315  | total loss: \u001b[1m\u001b[32m1.19080\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1315 | loss: 1.19080 - acc: 0.8328 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1316  | total loss: \u001b[1m\u001b[32m1.09632\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1316 | loss: 1.09632 - acc: 0.8495 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1317  | total loss: \u001b[1m\u001b[32m1.01140\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1317 | loss: 1.01140 - acc: 0.8646 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1318  | total loss: \u001b[1m\u001b[32m0.93504\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1318 | loss: 0.93504 - acc: 0.8781 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1319  | total loss: \u001b[1m\u001b[32m0.86636\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1319 | loss: 0.86636 - acc: 0.8903 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1320  | total loss: \u001b[1m\u001b[32m0.80456\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1320 | loss: 0.80456 - acc: 0.9013 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1321  | total loss: \u001b[1m\u001b[32m0.74892\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1321 | loss: 0.74892 - acc: 0.9111 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1322  | total loss: \u001b[1m\u001b[32m0.69881\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1322 | loss: 0.69881 - acc: 0.9200 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1323  | total loss: \u001b[1m\u001b[32m0.65365\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1323 | loss: 0.65365 - acc: 0.9280 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1324  | total loss: \u001b[1m\u001b[32m1.10181\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1324 | loss: 1.10181 - acc: 0.8443 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1325  | total loss: \u001b[1m\u001b[32m1.01637\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1325 | loss: 1.01637 - acc: 0.8599 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1326  | total loss: \u001b[1m\u001b[32m0.93955\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1326 | loss: 0.93955 - acc: 0.8739 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1327  | total loss: \u001b[1m\u001b[32m0.87045\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1327 | loss: 0.87045 - acc: 0.8865 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1328  | total loss: \u001b[1m\u001b[32m0.80826\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1328 | loss: 0.80826 - acc: 0.8979 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1329  | total loss: \u001b[1m\u001b[32m0.75228\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1329 | loss: 0.75228 - acc: 0.9081 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1330  | total loss: \u001b[1m\u001b[32m0.70186\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1330 | loss: 0.70186 - acc: 0.9173 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1331  | total loss: \u001b[1m\u001b[32m0.65641\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1331 | loss: 0.65641 - acc: 0.9255 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1332  | total loss: \u001b[1m\u001b[32m1.15190\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1332 | loss: 1.15190 - acc: 0.8330 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1333  | total loss: \u001b[1m\u001b[32m1.06147\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1333 | loss: 1.06147 - acc: 0.8497 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1334  | total loss: \u001b[1m\u001b[32m1.57911\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1334 | loss: 1.57911 - acc: 0.7647 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1335  | total loss: \u001b[1m\u001b[32m1.44629\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1335 | loss: 1.44629 - acc: 0.7882 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1336  | total loss: \u001b[1m\u001b[32m1.32697\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1336 | loss: 1.32697 - acc: 0.8094 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1337  | total loss: \u001b[1m\u001b[32m1.21975\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1337 | loss: 1.21975 - acc: 0.8285 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1338  | total loss: \u001b[1m\u001b[32m1.12338\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1338 | loss: 1.12338 - acc: 0.8456 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1339  | total loss: \u001b[1m\u001b[32m1.03675\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1339 | loss: 1.03675 - acc: 0.8611 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1340  | total loss: \u001b[1m\u001b[32m0.95883\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1340 | loss: 0.95883 - acc: 0.8750 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1341  | total loss: \u001b[1m\u001b[32m0.88873\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1341 | loss: 0.88873 - acc: 0.8875 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1342  | total loss: \u001b[1m\u001b[32m0.82564\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1342 | loss: 0.82564 - acc: 0.8987 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1343  | total loss: \u001b[1m\u001b[32m0.76883\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1343 | loss: 0.76883 - acc: 0.9088 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1344  | total loss: \u001b[1m\u001b[32m1.27638\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1344 | loss: 1.27638 - acc: 0.8180 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1345  | total loss: \u001b[1m\u001b[32m1.17459\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1345 | loss: 1.17459 - acc: 0.8362 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1346  | total loss: \u001b[1m\u001b[32m1.81446\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1346 | loss: 1.81446 - acc: 0.7525 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1347  | total loss: \u001b[1m\u001b[32m1.65929\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1347 | loss: 1.65929 - acc: 0.7773 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1348  | total loss: \u001b[1m\u001b[32m1.51991\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1348 | loss: 1.51991 - acc: 0.7996 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1349  | total loss: \u001b[1m\u001b[32m1.39469\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1349 | loss: 1.39469 - acc: 0.8196 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1350  | total loss: \u001b[1m\u001b[32m1.28216\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1350 | loss: 1.28216 - acc: 0.8376 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1351  | total loss: \u001b[1m\u001b[32m1.18101\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1351 | loss: 1.18101 - acc: 0.8539 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1352  | total loss: \u001b[1m\u001b[32m1.09007\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1352 | loss: 1.09007 - acc: 0.8685 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1353  | total loss: \u001b[1m\u001b[32m1.00828\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1353 | loss: 1.00828 - acc: 0.8816 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1354  | total loss: \u001b[1m\u001b[32m0.93469\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1354 | loss: 0.93469 - acc: 0.8935 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1355  | total loss: \u001b[1m\u001b[32m0.86845\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1355 | loss: 0.86845 - acc: 0.9041 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1356  | total loss: \u001b[1m\u001b[32m1.32175\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1356 | loss: 1.32175 - acc: 0.8137 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1357  | total loss: \u001b[1m\u001b[32m1.21690\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1357 | loss: 1.21690 - acc: 0.8323 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1358  | total loss: \u001b[1m\u001b[32m1.66506\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1358 | loss: 1.66506 - acc: 0.7491 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1359  | total loss: \u001b[1m\u001b[32m1.52623\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1359 | loss: 1.52623 - acc: 0.7742 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1360  | total loss: \u001b[1m\u001b[32m1.40150\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1360 | loss: 1.40150 - acc: 0.7968 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1361  | total loss: \u001b[1m\u001b[32m1.28940\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1361 | loss: 1.28940 - acc: 0.8171 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1362  | total loss: \u001b[1m\u001b[32m1.18864\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1362 | loss: 1.18864 - acc: 0.8354 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1363  | total loss: \u001b[1m\u001b[32m1.09804\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1363 | loss: 1.09804 - acc: 0.8519 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1364  | total loss: \u001b[1m\u001b[32m1.01655\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1364 | loss: 1.01655 - acc: 0.8667 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1365  | total loss: \u001b[1m\u001b[32m0.94323\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1365 | loss: 0.94323 - acc: 0.8800 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1366  | total loss: \u001b[1m\u001b[32m0.87723\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1366 | loss: 0.87723 - acc: 0.8920 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1367  | total loss: \u001b[1m\u001b[32m0.81779\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1367 | loss: 0.81779 - acc: 0.9028 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1368  | total loss: \u001b[1m\u001b[32m0.76424\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1368 | loss: 0.76424 - acc: 0.9125 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1369  | total loss: \u001b[1m\u001b[32m0.71595\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1369 | loss: 0.71595 - acc: 0.9213 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1370  | total loss: \u001b[1m\u001b[32m0.67240\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1370 | loss: 0.67240 - acc: 0.9291 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1371  | total loss: \u001b[1m\u001b[32m0.63309\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1371 | loss: 0.63309 - acc: 0.9362 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1372  | total loss: \u001b[1m\u001b[32m0.59757\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1372 | loss: 0.59757 - acc: 0.9426 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1373  | total loss: \u001b[1m\u001b[32m0.56546\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1373 | loss: 0.56546 - acc: 0.9483 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1374  | total loss: \u001b[1m\u001b[32m0.53641\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1374 | loss: 0.53641 - acc: 0.9535 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1375  | total loss: \u001b[1m\u001b[32m0.51010\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1375 | loss: 0.51010 - acc: 0.9582 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1376  | total loss: \u001b[1m\u001b[32m0.48625\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1376 | loss: 0.48625 - acc: 0.9623 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1377  | total loss: \u001b[1m\u001b[32m0.46461\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1377 | loss: 0.46461 - acc: 0.9661 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1378  | total loss: \u001b[1m\u001b[32m0.44494\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1378 | loss: 0.44494 - acc: 0.9695 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1379  | total loss: \u001b[1m\u001b[32m0.42705\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1379 | loss: 0.42705 - acc: 0.9725 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1380  | total loss: \u001b[1m\u001b[32m1.05712\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1380 | loss: 1.05712 - acc: 0.8753 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1381  | total loss: \u001b[1m\u001b[32m0.97784\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1381 | loss: 0.97784 - acc: 0.8878 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1382  | total loss: \u001b[1m\u001b[32m0.90649\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1382 | loss: 0.90649 - acc: 0.8990 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1383  | total loss: \u001b[1m\u001b[32m0.84224\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1383 | loss: 0.84224 - acc: 0.9091 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1384  | total loss: \u001b[1m\u001b[32m0.78438\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1384 | loss: 0.78438 - acc: 0.9182 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1385  | total loss: \u001b[1m\u001b[32m0.73224\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1385 | loss: 0.73224 - acc: 0.9264 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1386  | total loss: \u001b[1m\u001b[32m0.68523\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1386 | loss: 0.68523 - acc: 0.9337 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1387  | total loss: \u001b[1m\u001b[32m0.64282\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1387 | loss: 0.64282 - acc: 0.9404 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1388  | total loss: \u001b[1m\u001b[32m0.60455\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1388 | loss: 0.60455 - acc: 0.9463 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1389  | total loss: \u001b[1m\u001b[32m0.56999\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1389 | loss: 0.56999 - acc: 0.9517 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1390  | total loss: \u001b[1m\u001b[32m0.53875\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1390 | loss: 0.53875 - acc: 0.9565 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1391  | total loss: \u001b[1m\u001b[32m0.51049\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1391 | loss: 0.51049 - acc: 0.9609 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1392  | total loss: \u001b[1m\u001b[32m0.48491\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1392 | loss: 0.48491 - acc: 0.9648 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1393  | total loss: \u001b[1m\u001b[32m0.46174\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1393 | loss: 0.46174 - acc: 0.9683 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1394  | total loss: \u001b[1m\u001b[32m0.44072\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1394 | loss: 0.44072 - acc: 0.9715 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1395  | total loss: \u001b[1m\u001b[32m0.42163\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1395 | loss: 0.42163 - acc: 0.9743 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1396  | total loss: \u001b[1m\u001b[32m0.40428\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1396 | loss: 0.40428 - acc: 0.9769 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1397  | total loss: \u001b[1m\u001b[32m0.38849\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1397 | loss: 0.38849 - acc: 0.9792 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1398  | total loss: \u001b[1m\u001b[32m0.37411\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1398 | loss: 0.37411 - acc: 0.9813 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1399  | total loss: \u001b[1m\u001b[32m0.36098\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1399 | loss: 0.36098 - acc: 0.9832 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1400  | total loss: \u001b[1m\u001b[32m0.34898\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1400 | loss: 0.34898 - acc: 0.9848 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1401  | total loss: \u001b[1m\u001b[32m0.33799\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1401 | loss: 0.33799 - acc: 0.9864 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1402  | total loss: \u001b[1m\u001b[32m0.89653\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1402 | loss: 0.89653 - acc: 0.8968 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1403  | total loss: \u001b[1m\u001b[32m0.83060\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1403 | loss: 0.83060 - acc: 0.9071 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1404  | total loss: \u001b[1m\u001b[32m0.77124\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1404 | loss: 0.77124 - acc: 0.9164 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1405  | total loss: \u001b[1m\u001b[32m0.71778\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1405 | loss: 0.71778 - acc: 0.9248 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1406  | total loss: \u001b[1m\u001b[32m1.07336\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1406 | loss: 1.07336 - acc: 0.8414 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1407  | total loss: \u001b[1m\u001b[32m0.98972\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1407 | loss: 0.98972 - acc: 0.8572 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1408  | total loss: \u001b[1m\u001b[32m0.91449\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1408 | loss: 0.91449 - acc: 0.8715 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1409  | total loss: \u001b[1m\u001b[32m0.84682\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1409 | loss: 0.84682 - acc: 0.8844 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1410  | total loss: \u001b[1m\u001b[32m0.78592\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1410 | loss: 0.78592 - acc: 0.8959 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1411  | total loss: \u001b[1m\u001b[32m0.73111\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1411 | loss: 0.73111 - acc: 0.9063 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1412  | total loss: \u001b[1m\u001b[32m0.68174\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1412 | loss: 0.68174 - acc: 0.9157 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1413  | total loss: \u001b[1m\u001b[32m0.63727\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1413 | loss: 0.63727 - acc: 0.9241 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1414  | total loss: \u001b[1m\u001b[32m0.59718\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1414 | loss: 0.59718 - acc: 0.9317 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1415  | total loss: \u001b[1m\u001b[32m0.56102\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1415 | loss: 0.56102 - acc: 0.9386 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1416  | total loss: \u001b[1m\u001b[32m0.52839\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1416 | loss: 0.52839 - acc: 0.9447 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1417  | total loss: \u001b[1m\u001b[32m0.49892\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1417 | loss: 0.49892 - acc: 0.9502 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1418  | total loss: \u001b[1m\u001b[32m0.47229\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1418 | loss: 0.47229 - acc: 0.9552 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1419  | total loss: \u001b[1m\u001b[32m0.44820\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1419 | loss: 0.44820 - acc: 0.9597 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1420  | total loss: \u001b[1m\u001b[32m1.03882\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1420 | loss: 1.03882 - acc: 0.8637 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1421  | total loss: \u001b[1m\u001b[32m0.95800\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1421 | loss: 0.95800 - acc: 0.8773 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1422  | total loss: \u001b[1m\u001b[32m0.88529\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1422 | loss: 0.88529 - acc: 0.8896 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1423  | total loss: \u001b[1m\u001b[32m0.81986\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1423 | loss: 0.81986 - acc: 0.9006 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1424  | total loss: \u001b[1m\u001b[32m0.76095\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1424 | loss: 0.76095 - acc: 0.9106 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1425  | total loss: \u001b[1m\u001b[32m0.70791\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1425 | loss: 0.70791 - acc: 0.9195 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1426  | total loss: \u001b[1m\u001b[32m0.66011\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1426 | loss: 0.66011 - acc: 0.9276 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1427  | total loss: \u001b[1m\u001b[32m0.61704\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1427 | loss: 0.61704 - acc: 0.9348 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1428  | total loss: \u001b[1m\u001b[32m0.57819\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1428 | loss: 0.57819 - acc: 0.9413 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1429  | total loss: \u001b[1m\u001b[32m0.54314\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1429 | loss: 0.54314 - acc: 0.9472 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1430  | total loss: \u001b[1m\u001b[32m0.51149\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1430 | loss: 0.51149 - acc: 0.9525 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1431  | total loss: \u001b[1m\u001b[32m0.48290\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1431 | loss: 0.48290 - acc: 0.9572 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1432  | total loss: \u001b[1m\u001b[32m0.97437\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1432 | loss: 0.97437 - acc: 0.8797 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1433  | total loss: \u001b[1m\u001b[32m0.89939\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1433 | loss: 0.89939 - acc: 0.8917 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1434  | total loss: \u001b[1m\u001b[32m0.83191\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1434 | loss: 0.83191 - acc: 0.9025 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1435  | total loss: \u001b[1m\u001b[32m0.77116\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1435 | loss: 0.77116 - acc: 0.9123 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1436  | total loss: \u001b[1m\u001b[32m0.71645\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1436 | loss: 0.71645 - acc: 0.9211 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1437  | total loss: \u001b[1m\u001b[32m0.66716\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1437 | loss: 0.66716 - acc: 0.9290 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1438  | total loss: \u001b[1m\u001b[32m0.62274\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1438 | loss: 0.62274 - acc: 0.9361 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1439  | total loss: \u001b[1m\u001b[32m0.58268\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1439 | loss: 0.58268 - acc: 0.9425 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1440  | total loss: \u001b[1m\u001b[32m1.10020\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1440 | loss: 1.10020 - acc: 0.8482 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1441  | total loss: \u001b[1m\u001b[32m1.01238\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1441 | loss: 1.01238 - acc: 0.8634 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1442  | total loss: \u001b[1m\u001b[32m0.93339\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1442 | loss: 0.93339 - acc: 0.8771 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1443  | total loss: \u001b[1m\u001b[32m0.86232\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1443 | loss: 0.86232 - acc: 0.8893 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1444  | total loss: \u001b[1m\u001b[32m1.31627\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1444 | loss: 1.31627 - acc: 0.8095 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1445  | total loss: \u001b[1m\u001b[32m1.20707\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1445 | loss: 1.20707 - acc: 0.8286 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1446  | total loss: \u001b[1m\u001b[32m1.10890\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1446 | loss: 1.10890 - acc: 0.8457 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1447  | total loss: \u001b[1m\u001b[32m1.02064\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1447 | loss: 1.02064 - acc: 0.8611 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1448  | total loss: \u001b[1m\u001b[32m0.94127\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1448 | loss: 0.94127 - acc: 0.8750 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1449  | total loss: \u001b[1m\u001b[32m0.86987\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1449 | loss: 0.86987 - acc: 0.8875 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1450  | total loss: \u001b[1m\u001b[32m0.80563\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1450 | loss: 0.80563 - acc: 0.8988 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1451  | total loss: \u001b[1m\u001b[32m0.74780\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1451 | loss: 0.74780 - acc: 0.9089 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1452  | total loss: \u001b[1m\u001b[32m1.15743\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1452 | loss: 1.15743 - acc: 0.8362 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1453  | total loss: \u001b[1m\u001b[32m1.06451\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1453 | loss: 1.06451 - acc: 0.8526 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1454  | total loss: \u001b[1m\u001b[32m0.98097\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1454 | loss: 0.98097 - acc: 0.8673 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1455  | total loss: \u001b[1m\u001b[32m0.90583\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1455 | loss: 0.90583 - acc: 0.8806 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1456  | total loss: \u001b[1m\u001b[32m0.83825\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1456 | loss: 0.83825 - acc: 0.8925 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1457  | total loss: \u001b[1m\u001b[32m0.77743\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1457 | loss: 0.77743 - acc: 0.9033 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1458  | total loss: \u001b[1m\u001b[32m1.25819\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1458 | loss: 1.25819 - acc: 0.8129 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1459  | total loss: \u001b[1m\u001b[32m1.15553\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1459 | loss: 1.15553 - acc: 0.8316 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1460  | total loss: \u001b[1m\u001b[32m1.06326\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1460 | loss: 1.06326 - acc: 0.8485 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1461  | total loss: \u001b[1m\u001b[32m0.98032\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1461 | loss: 0.98032 - acc: 0.8636 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1462  | total loss: \u001b[1m\u001b[32m1.23619\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1462 | loss: 1.23619 - acc: 0.7955 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1463  | total loss: \u001b[1m\u001b[32m1.13619\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1463 | loss: 1.13619 - acc: 0.8159 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1464  | total loss: \u001b[1m\u001b[32m1.04633\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1464 | loss: 1.04633 - acc: 0.8343 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1465  | total loss: \u001b[1m\u001b[32m0.96556\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1465 | loss: 0.96556 - acc: 0.8509 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1466  | total loss: \u001b[1m\u001b[32m0.89294\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1466 | loss: 0.89294 - acc: 0.8658 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1467  | total loss: \u001b[1m\u001b[32m0.82764\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1467 | loss: 0.82764 - acc: 0.8792 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1468  | total loss: \u001b[1m\u001b[32m0.76889\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1468 | loss: 0.76889 - acc: 0.8913 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1469  | total loss: \u001b[1m\u001b[32m0.71601\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1469 | loss: 0.71601 - acc: 0.9022 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1470  | total loss: \u001b[1m\u001b[32m0.66840\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1470 | loss: 0.66840 - acc: 0.9119 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1471  | total loss: \u001b[1m\u001b[32m0.62550\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1471 | loss: 0.62550 - acc: 0.9208 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1472  | total loss: \u001b[1m\u001b[32m0.58683\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1472 | loss: 0.58683 - acc: 0.9287 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1473  | total loss: \u001b[1m\u001b[32m0.55196\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1473 | loss: 0.55196 - acc: 0.9358 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1474  | total loss: \u001b[1m\u001b[32m0.52047\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1474 | loss: 0.52047 - acc: 0.9422 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1475  | total loss: \u001b[1m\u001b[32m0.49204\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1475 | loss: 0.49204 - acc: 0.9480 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1476  | total loss: \u001b[1m\u001b[32m0.46632\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1476 | loss: 0.46632 - acc: 0.9532 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1477  | total loss: \u001b[1m\u001b[32m0.44306\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1477 | loss: 0.44306 - acc: 0.9579 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1478  | total loss: \u001b[1m\u001b[32m0.42198\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1478 | loss: 0.42198 - acc: 0.9621 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1479  | total loss: \u001b[1m\u001b[32m0.40287\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1479 | loss: 0.40287 - acc: 0.9659 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1480  | total loss: \u001b[1m\u001b[32m0.38552\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1480 | loss: 0.38552 - acc: 0.9693 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1481  | total loss: \u001b[1m\u001b[32m0.36975\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1481 | loss: 0.36975 - acc: 0.9724 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1482  | total loss: \u001b[1m\u001b[32m0.35539\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1482 | loss: 0.35539 - acc: 0.9751 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1483  | total loss: \u001b[1m\u001b[32m0.34231\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1483 | loss: 0.34231 - acc: 0.9776 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1484  | total loss: \u001b[1m\u001b[32m0.33037\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1484 | loss: 0.33037 - acc: 0.9799 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1485  | total loss: \u001b[1m\u001b[32m0.31945\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1485 | loss: 0.31945 - acc: 0.9819 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1486  | total loss: \u001b[1m\u001b[32m0.30945\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1486 | loss: 0.30945 - acc: 0.9837 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1487  | total loss: \u001b[1m\u001b[32m0.30028\u001b[0m\u001b[0m | time: 0.024s\n",
      "| Adam | epoch: 1487 | loss: 0.30028 - acc: 0.9853 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1488  | total loss: \u001b[1m\u001b[32m0.62663\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1488 | loss: 0.62663 - acc: 0.9050 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1489  | total loss: \u001b[1m\u001b[32m0.58548\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1489 | loss: 0.58548 - acc: 0.9145 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1490  | total loss: \u001b[1m\u001b[32m0.54834\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1490 | loss: 0.54834 - acc: 0.9230 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1491  | total loss: \u001b[1m\u001b[32m0.51481\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1491 | loss: 0.51481 - acc: 0.9307 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1492  | total loss: \u001b[1m\u001b[32m0.48452\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1492 | loss: 0.48452 - acc: 0.9376 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1493  | total loss: \u001b[1m\u001b[32m0.45714\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1493 | loss: 0.45714 - acc: 0.9439 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1494  | total loss: \u001b[1m\u001b[32m0.43238\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1494 | loss: 0.43238 - acc: 0.9495 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1495  | total loss: \u001b[1m\u001b[32m0.40997\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1495 | loss: 0.40997 - acc: 0.9545 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1496  | total loss: \u001b[1m\u001b[32m0.38966\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1496 | loss: 0.38966 - acc: 0.9591 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1497  | total loss: \u001b[1m\u001b[32m0.37125\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1497 | loss: 0.37125 - acc: 0.9632 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1498  | total loss: \u001b[1m\u001b[32m0.88513\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1498 | loss: 0.88513 - acc: 0.8760 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1499  | total loss: \u001b[1m\u001b[32m0.81707\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1499 | loss: 0.81707 - acc: 0.8884 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1500  | total loss: \u001b[1m\u001b[32m0.75582\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1500 | loss: 0.75582 - acc: 0.8995 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1501  | total loss: \u001b[1m\u001b[32m0.70069\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1501 | loss: 0.70069 - acc: 0.9096 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1502  | total loss: \u001b[1m\u001b[32m1.21324\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1502 | loss: 1.21324 - acc: 0.8186 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1503  | total loss: \u001b[1m\u001b[32m1.11248\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1503 | loss: 1.11248 - acc: 0.8368 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1504  | total loss: \u001b[1m\u001b[32m1.02190\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1504 | loss: 1.02190 - acc: 0.8531 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1505  | total loss: \u001b[1m\u001b[32m0.94046\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1505 | loss: 0.94046 - acc: 0.8678 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1506  | total loss: \u001b[1m\u001b[32m0.86721\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1506 | loss: 0.86721 - acc: 0.8810 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1507  | total loss: \u001b[1m\u001b[32m0.80133\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1507 | loss: 0.80133 - acc: 0.8929 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1508  | total loss: \u001b[1m\u001b[32m0.74204\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1508 | loss: 0.74204 - acc: 0.9036 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1509  | total loss: \u001b[1m\u001b[32m0.68868\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1509 | loss: 0.68868 - acc: 0.9132 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1510  | total loss: \u001b[1m\u001b[32m0.64064\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1510 | loss: 0.64064 - acc: 0.9219 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1511  | total loss: \u001b[1m\u001b[32m0.59736\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1511 | loss: 0.59736 - acc: 0.9297 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1512  | total loss: \u001b[1m\u001b[32m0.55836\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1512 | loss: 0.55836 - acc: 0.9368 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1513  | total loss: \u001b[1m\u001b[32m0.52320\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1513 | loss: 0.52320 - acc: 0.9431 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1514  | total loss: \u001b[1m\u001b[32m0.49148\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1514 | loss: 0.49148 - acc: 0.9488 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1515  | total loss: \u001b[1m\u001b[32m0.46285\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1515 | loss: 0.46285 - acc: 0.9539 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1516  | total loss: \u001b[1m\u001b[32m0.81317\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1516 | loss: 0.81317 - acc: 0.8676 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1517  | total loss: \u001b[1m\u001b[32m0.75229\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1517 | loss: 0.75229 - acc: 0.8808 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1518  | total loss: \u001b[1m\u001b[32m0.69750\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1518 | loss: 0.69750 - acc: 0.8928 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1519  | total loss: \u001b[1m\u001b[32m0.64817\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1519 | loss: 0.64817 - acc: 0.9035 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1520  | total loss: \u001b[1m\u001b[32m0.60374\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1520 | loss: 0.60374 - acc: 0.9131 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1521  | total loss: \u001b[1m\u001b[32m0.56370\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1521 | loss: 0.56370 - acc: 0.9218 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1522  | total loss: \u001b[1m\u001b[32m0.52761\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1522 | loss: 0.52761 - acc: 0.9296 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1523  | total loss: \u001b[1m\u001b[32m0.49505\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1523 | loss: 0.49505 - acc: 0.9367 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1524  | total loss: \u001b[1m\u001b[32m0.46567\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1524 | loss: 0.46567 - acc: 0.9430 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1525  | total loss: \u001b[1m\u001b[32m0.43914\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1525 | loss: 0.43914 - acc: 0.9487 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1526  | total loss: \u001b[1m\u001b[32m0.41516\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1526 | loss: 0.41516 - acc: 0.9538 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1527  | total loss: \u001b[1m\u001b[32m0.39348\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1527 | loss: 0.39348 - acc: 0.9585 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1528  | total loss: \u001b[1m\u001b[32m0.99561\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1528 | loss: 0.99561 - acc: 0.8626 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1529  | total loss: \u001b[1m\u001b[32m0.91585\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1529 | loss: 0.91585 - acc: 0.8763 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1530  | total loss: \u001b[1m\u001b[32m0.84410\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1530 | loss: 0.84410 - acc: 0.8887 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1531  | total loss: \u001b[1m\u001b[32m0.77956\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1531 | loss: 0.77956 - acc: 0.8998 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1532  | total loss: \u001b[1m\u001b[32m1.12391\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1532 | loss: 1.12391 - acc: 0.8371 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1533  | total loss: \u001b[1m\u001b[32m1.03151\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1533 | loss: 1.03151 - acc: 0.8534 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1534  | total loss: \u001b[1m\u001b[32m1.31016\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1534 | loss: 1.31016 - acc: 0.7772 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1535  | total loss: \u001b[1m\u001b[32m1.19938\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1535 | loss: 1.19938 - acc: 0.7994 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1536  | total loss: \u001b[1m\u001b[32m1.09980\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1536 | loss: 1.09980 - acc: 0.8195 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1537  | total loss: \u001b[1m\u001b[32m1.01028\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1537 | loss: 1.01028 - acc: 0.8376 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1538  | total loss: \u001b[1m\u001b[32m0.92979\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1538 | loss: 0.92979 - acc: 0.8538 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1539  | total loss: \u001b[1m\u001b[32m0.85740\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1539 | loss: 0.85740 - acc: 0.8684 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1540  | total loss: \u001b[1m\u001b[32m0.79227\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1540 | loss: 0.79227 - acc: 0.8816 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1541  | total loss: \u001b[1m\u001b[32m0.73367\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1541 | loss: 0.73367 - acc: 0.8934 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1542  | total loss: \u001b[1m\u001b[32m0.68091\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1542 | loss: 0.68091 - acc: 0.9041 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1543  | total loss: \u001b[1m\u001b[32m0.63340\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1543 | loss: 0.63340 - acc: 0.9137 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1544  | total loss: \u001b[1m\u001b[32m1.15146\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1544 | loss: 1.15146 - acc: 0.8405 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1545  | total loss: \u001b[1m\u001b[32m1.05699\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1545 | loss: 1.05699 - acc: 0.8564 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1546  | total loss: \u001b[1m\u001b[32m0.97207\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1546 | loss: 0.97207 - acc: 0.8708 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1547  | total loss: \u001b[1m\u001b[32m0.89571\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1547 | loss: 0.89571 - acc: 0.8837 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1548  | total loss: \u001b[1m\u001b[32m0.82704\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1548 | loss: 0.82704 - acc: 0.8953 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1549  | total loss: \u001b[1m\u001b[32m0.76526\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1549 | loss: 0.76526 - acc: 0.9058 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1550  | total loss: \u001b[1m\u001b[32m0.70967\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1550 | loss: 0.70967 - acc: 0.9152 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1551  | total loss: \u001b[1m\u001b[32m0.65962\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1551 | loss: 0.65962 - acc: 0.9237 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1552  | total loss: \u001b[1m\u001b[32m0.61455\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1552 | loss: 0.61455 - acc: 0.9313 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1553  | total loss: \u001b[1m\u001b[32m0.57394\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1553 | loss: 0.57394 - acc: 0.9382 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1554  | total loss: \u001b[1m\u001b[32m0.53734\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1554 | loss: 0.53734 - acc: 0.9444 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1555  | total loss: \u001b[1m\u001b[32m0.50433\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1555 | loss: 0.50433 - acc: 0.9499 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1556  | total loss: \u001b[1m\u001b[32m0.97671\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1556 | loss: 0.97671 - acc: 0.8640 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1557  | total loss: \u001b[1m\u001b[32m0.89974\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1557 | loss: 0.89974 - acc: 0.8776 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1558  | total loss: \u001b[1m\u001b[32m0.83052\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1558 | loss: 0.83052 - acc: 0.8899 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1559  | total loss: \u001b[1m\u001b[32m0.76824\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1559 | loss: 0.76824 - acc: 0.9009 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1560  | total loss: \u001b[1m\u001b[32m0.97200\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1560 | loss: 0.97200 - acc: 0.8381 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1561  | total loss: \u001b[1m\u001b[32m0.89563\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1561 | loss: 0.89563 - acc: 0.8543 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1562  | total loss: \u001b[1m\u001b[32m0.82692\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1562 | loss: 0.82692 - acc: 0.8688 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1563  | total loss: \u001b[1m\u001b[32m0.76510\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1563 | loss: 0.76510 - acc: 0.8820 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1564  | total loss: \u001b[1m\u001b[32m0.70945\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1564 | loss: 0.70945 - acc: 0.8938 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1565  | total loss: \u001b[1m\u001b[32m0.65935\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1565 | loss: 0.65935 - acc: 0.9044 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1566  | total loss: \u001b[1m\u001b[32m0.61421\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1566 | loss: 0.61421 - acc: 0.9139 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1567  | total loss: \u001b[1m\u001b[32m0.57354\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1567 | loss: 0.57354 - acc: 0.9225 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1568  | total loss: \u001b[1m\u001b[32m0.53687\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1568 | loss: 0.53687 - acc: 0.9303 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1569  | total loss: \u001b[1m\u001b[32m0.50379\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1569 | loss: 0.50379 - acc: 0.9373 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1570  | total loss: \u001b[1m\u001b[32m1.04812\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1570 | loss: 1.04812 - acc: 0.8435 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1571  | total loss: \u001b[1m\u001b[32m0.96390\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1571 | loss: 0.96390 - acc: 0.8592 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1572  | total loss: \u001b[1m\u001b[32m0.88815\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1572 | loss: 0.88815 - acc: 0.8733 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1573  | total loss: \u001b[1m\u001b[32m0.82000\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1573 | loss: 0.82000 - acc: 0.8859 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1574  | total loss: \u001b[1m\u001b[32m0.75867\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1574 | loss: 0.75867 - acc: 0.8973 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1575  | total loss: \u001b[1m\u001b[32m0.70346\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1575 | loss: 0.70346 - acc: 0.9076 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1576  | total loss: \u001b[1m\u001b[32m0.65375\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1576 | loss: 0.65375 - acc: 0.9168 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1577  | total loss: \u001b[1m\u001b[32m0.60896\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1577 | loss: 0.60896 - acc: 0.9252 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1578  | total loss: \u001b[1m\u001b[32m0.56859\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1578 | loss: 0.56859 - acc: 0.9326 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1579  | total loss: \u001b[1m\u001b[32m0.53220\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1579 | loss: 0.53220 - acc: 0.9394 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1580  | total loss: \u001b[1m\u001b[32m0.94331\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1580 | loss: 0.94331 - acc: 0.8545 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1581  | total loss: \u001b[1m\u001b[32m0.86942\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1581 | loss: 0.86942 - acc: 0.8691 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1582  | total loss: \u001b[1m\u001b[32m0.80295\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1582 | loss: 0.80295 - acc: 0.8822 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1583  | total loss: \u001b[1m\u001b[32m0.74313\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1583 | loss: 0.74313 - acc: 0.8940 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1584  | total loss: \u001b[1m\u001b[32m0.68930\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1584 | loss: 0.68930 - acc: 0.9046 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1585  | total loss: \u001b[1m\u001b[32m0.64082\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1585 | loss: 0.64082 - acc: 0.9141 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1586  | total loss: \u001b[1m\u001b[32m0.59716\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1586 | loss: 0.59716 - acc: 0.9227 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1587  | total loss: \u001b[1m\u001b[32m0.55781\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1587 | loss: 0.55781 - acc: 0.9304 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1588  | total loss: \u001b[1m\u001b[32m0.52234\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1588 | loss: 0.52234 - acc: 0.9374 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1589  | total loss: \u001b[1m\u001b[32m0.49034\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1589 | loss: 0.49034 - acc: 0.9436 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1590  | total loss: \u001b[1m\u001b[32m1.01166\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1590 | loss: 1.01166 - acc: 0.8584 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1591  | total loss: \u001b[1m\u001b[32m0.93071\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1591 | loss: 0.93071 - acc: 0.8725 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1592  | total loss: \u001b[1m\u001b[32m0.85789\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1592 | loss: 0.85789 - acc: 0.8853 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1593  | total loss: \u001b[1m\u001b[32m0.79238\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1593 | loss: 0.79238 - acc: 0.8968 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1594  | total loss: \u001b[1m\u001b[32m1.32120\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1594 | loss: 1.32120 - acc: 0.8162 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1595  | total loss: \u001b[1m\u001b[32m1.20951\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1595 | loss: 1.20951 - acc: 0.8346 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1596  | total loss: \u001b[1m\u001b[32m1.10912\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1596 | loss: 1.10912 - acc: 0.8511 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1597  | total loss: \u001b[1m\u001b[32m1.01887\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1597 | loss: 1.01887 - acc: 0.8660 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1598  | total loss: \u001b[1m\u001b[32m0.93772\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1598 | loss: 0.93772 - acc: 0.8794 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1599  | total loss: \u001b[1m\u001b[32m0.86473\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1599 | loss: 0.86473 - acc: 0.8914 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1600  | total loss: \u001b[1m\u001b[32m0.79907\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1600 | loss: 0.79907 - acc: 0.9023 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1601  | total loss: \u001b[1m\u001b[32m0.73999\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1601 | loss: 0.73999 - acc: 0.9121 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1602  | total loss: \u001b[1m\u001b[32m0.68680\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1602 | loss: 0.68680 - acc: 0.9209 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1603  | total loss: \u001b[1m\u001b[32m0.63891\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1603 | loss: 0.63891 - acc: 0.9288 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1604  | total loss: \u001b[1m\u001b[32m1.13281\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1604 | loss: 1.13281 - acc: 0.8450 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1605  | total loss: \u001b[1m\u001b[32m1.04039\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1605 | loss: 1.04039 - acc: 0.8605 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1606  | total loss: \u001b[1m\u001b[32m0.95728\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1606 | loss: 0.95728 - acc: 0.8744 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1607  | total loss: \u001b[1m\u001b[32m0.88254\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1607 | loss: 0.88254 - acc: 0.8870 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1608  | total loss: \u001b[1m\u001b[32m1.32598\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1608 | loss: 1.32598 - acc: 0.8074 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1609  | total loss: \u001b[1m\u001b[32m1.21457\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1609 | loss: 1.21457 - acc: 0.8267 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1610  | total loss: \u001b[1m\u001b[32m1.58308\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1610 | loss: 1.58308 - acc: 0.7531 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1611  | total loss: \u001b[1m\u001b[32m1.44636\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1611 | loss: 1.44636 - acc: 0.7778 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1612  | total loss: \u001b[1m\u001b[32m1.32355\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1612 | loss: 1.32355 - acc: 0.8000 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1613  | total loss: \u001b[1m\u001b[32m1.21321\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1613 | loss: 1.21321 - acc: 0.8200 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1614  | total loss: \u001b[1m\u001b[32m1.11405\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1614 | loss: 1.11405 - acc: 0.8380 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1615  | total loss: \u001b[1m\u001b[32m1.02494\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1615 | loss: 1.02494 - acc: 0.8542 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1616  | total loss: \u001b[1m\u001b[32m0.94483\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1616 | loss: 0.94483 - acc: 0.8688 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1617  | total loss: \u001b[1m\u001b[32m0.87279\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1617 | loss: 0.87279 - acc: 0.8819 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1618  | total loss: \u001b[1m\u001b[32m1.19380\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1618 | loss: 1.19380 - acc: 0.8210 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1619  | total loss: \u001b[1m\u001b[32m1.09704\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1619 | loss: 1.09704 - acc: 0.8389 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1620  | total loss: \u001b[1m\u001b[32m1.01006\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1620 | loss: 1.01006 - acc: 0.8550 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1621  | total loss: \u001b[1m\u001b[32m0.93186\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1621 | loss: 0.93186 - acc: 0.8695 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1622  | total loss: \u001b[1m\u001b[32m0.86152\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1622 | loss: 0.86152 - acc: 0.8825 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1623  | total loss: \u001b[1m\u001b[32m0.79824\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1623 | loss: 0.79824 - acc: 0.8943 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1624  | total loss: \u001b[1m\u001b[32m0.74128\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1624 | loss: 0.74128 - acc: 0.9049 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1625  | total loss: \u001b[1m\u001b[32m0.68999\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1625 | loss: 0.68999 - acc: 0.9144 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1626  | total loss: \u001b[1m\u001b[32m0.64379\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1626 | loss: 0.64379 - acc: 0.9229 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1627  | total loss: \u001b[1m\u001b[32m0.60216\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1627 | loss: 0.60216 - acc: 0.9306 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1628  | total loss: \u001b[1m\u001b[32m0.56461\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1628 | loss: 0.56461 - acc: 0.9376 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1629  | total loss: \u001b[1m\u001b[32m0.53072\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1629 | loss: 0.53072 - acc: 0.9438 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1630  | total loss: \u001b[1m\u001b[32m0.50013\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1630 | loss: 0.50013 - acc: 0.9494 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1631  | total loss: \u001b[1m\u001b[32m0.47249\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1631 | loss: 0.47249 - acc: 0.9545 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1632  | total loss: \u001b[1m\u001b[32m0.44749\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1632 | loss: 0.44749 - acc: 0.9590 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1633  | total loss: \u001b[1m\u001b[32m0.42486\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1633 | loss: 0.42486 - acc: 0.9631 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1634  | total loss: \u001b[1m\u001b[32m0.40436\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1634 | loss: 0.40436 - acc: 0.9668 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1635  | total loss: \u001b[1m\u001b[32m0.38578\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1635 | loss: 0.38578 - acc: 0.9701 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1636  | total loss: \u001b[1m\u001b[32m0.36890\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1636 | loss: 0.36890 - acc: 0.9731 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1637  | total loss: \u001b[1m\u001b[32m0.35357\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1637 | loss: 0.35357 - acc: 0.9758 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1638  | total loss: \u001b[1m\u001b[32m0.33961\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1638 | loss: 0.33961 - acc: 0.9782 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1639  | total loss: \u001b[1m\u001b[32m0.32690\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1639 | loss: 0.32690 - acc: 0.9804 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1640  | total loss: \u001b[1m\u001b[32m0.31530\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1640 | loss: 0.31530 - acc: 0.9824 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1641  | total loss: \u001b[1m\u001b[32m0.30471\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1641 | loss: 0.30471 - acc: 0.9841 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1642  | total loss: \u001b[1m\u001b[32m0.29501\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1642 | loss: 0.29501 - acc: 0.9857 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1643  | total loss: \u001b[1m\u001b[32m0.28612\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1643 | loss: 0.28612 - acc: 0.9871 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1644  | total loss: \u001b[1m\u001b[32m0.27796\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1644 | loss: 0.27796 - acc: 0.9884 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1645  | total loss: \u001b[1m\u001b[32m0.27046\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1645 | loss: 0.27046 - acc: 0.9896 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1646  | total loss: \u001b[1m\u001b[32m0.26354\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1646 | loss: 0.26354 - acc: 0.9906 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1647  | total loss: \u001b[1m\u001b[32m0.25716\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1647 | loss: 0.25716 - acc: 0.9916 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1648  | total loss: \u001b[1m\u001b[32m0.25126\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1648 | loss: 0.25126 - acc: 0.9924 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1649  | total loss: \u001b[1m\u001b[32m0.24578\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1649 | loss: 0.24578 - acc: 0.9932 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1650  | total loss: \u001b[1m\u001b[32m0.24070\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1650 | loss: 0.24070 - acc: 0.9939 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1651  | total loss: \u001b[1m\u001b[32m0.23596\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1651 | loss: 0.23596 - acc: 0.9945 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1652  | total loss: \u001b[1m\u001b[32m0.23155\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1652 | loss: 0.23155 - acc: 0.9950 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1653  | total loss: \u001b[1m\u001b[32m0.22741\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1653 | loss: 0.22741 - acc: 0.9955 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1654  | total loss: \u001b[1m\u001b[32m0.22354\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1654 | loss: 0.22354 - acc: 0.9960 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1655  | total loss: \u001b[1m\u001b[32m0.21991\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1655 | loss: 0.21991 - acc: 0.9964 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1656  | total loss: \u001b[1m\u001b[32m0.79714\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1656 | loss: 0.79714 - acc: 0.8967 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1657  | total loss: \u001b[1m\u001b[32m0.73600\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1657 | loss: 0.73600 - acc: 0.9071 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1658  | total loss: \u001b[1m\u001b[32m0.68097\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1658 | loss: 0.68097 - acc: 0.9164 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1659  | total loss: \u001b[1m\u001b[32m0.63141\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1659 | loss: 0.63141 - acc: 0.9247 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1660  | total loss: \u001b[1m\u001b[32m0.58679\u001b[0m\u001b[0m | time: 0.015s\n",
      "| Adam | epoch: 1660 | loss: 0.58679 - acc: 0.9322 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1661  | total loss: \u001b[1m\u001b[32m0.54658\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1661 | loss: 0.54658 - acc: 0.9390 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1662  | total loss: \u001b[1m\u001b[32m0.51034\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1662 | loss: 0.51034 - acc: 0.9451 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1663  | total loss: \u001b[1m\u001b[32m0.47767\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1663 | loss: 0.47767 - acc: 0.9506 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1664  | total loss: \u001b[1m\u001b[32m0.44819\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1664 | loss: 0.44819 - acc: 0.9555 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1665  | total loss: \u001b[1m\u001b[32m0.42159\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1665 | loss: 0.42159 - acc: 0.9600 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1666  | total loss: \u001b[1m\u001b[32m0.39757\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1666 | loss: 0.39757 - acc: 0.9640 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1667  | total loss: \u001b[1m\u001b[32m0.37586\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1667 | loss: 0.37586 - acc: 0.9676 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1668  | total loss: \u001b[1m\u001b[32m0.35623\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1668 | loss: 0.35623 - acc: 0.9708 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1669  | total loss: \u001b[1m\u001b[32m0.33847\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1669 | loss: 0.33847 - acc: 0.9738 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1670  | total loss: \u001b[1m\u001b[32m0.32238\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1670 | loss: 0.32238 - acc: 0.9764 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1671  | total loss: \u001b[1m\u001b[32m0.30780\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1671 | loss: 0.30780 - acc: 0.9787 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1672  | total loss: \u001b[1m\u001b[32m0.82247\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1672 | loss: 0.82247 - acc: 0.8809 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1673  | total loss: \u001b[1m\u001b[32m0.75780\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1673 | loss: 0.75780 - acc: 0.8928 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1674  | total loss: \u001b[1m\u001b[32m0.69961\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1674 | loss: 0.69961 - acc: 0.9035 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1675  | total loss: \u001b[1m\u001b[32m0.64724\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1675 | loss: 0.64724 - acc: 0.9132 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1676  | total loss: \u001b[1m\u001b[32m1.01897\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1676 | loss: 1.01897 - acc: 0.8491 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1677  | total loss: \u001b[1m\u001b[32m0.93475\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1677 | loss: 0.93475 - acc: 0.8642 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1678  | total loss: \u001b[1m\u001b[32m1.47809\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1678 | loss: 1.47809 - acc: 0.7778 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1679  | total loss: \u001b[1m\u001b[32m1.34826\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1679 | loss: 1.34826 - acc: 0.8000 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1680  | total loss: \u001b[1m\u001b[32m1.81814\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1680 | loss: 1.81814 - acc: 0.7200 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1681  | total loss: \u001b[1m\u001b[32m1.65482\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1681 | loss: 1.65482 - acc: 0.7480 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1682  | total loss: \u001b[1m\u001b[32m1.50812\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1682 | loss: 1.50812 - acc: 0.7732 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1683  | total loss: \u001b[1m\u001b[32m1.37634\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1683 | loss: 1.37634 - acc: 0.7959 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1684  | total loss: \u001b[1m\u001b[32m1.90156\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1684 | loss: 1.90156 - acc: 0.7345 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1685  | total loss: \u001b[1m\u001b[32m1.73102\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1685 | loss: 1.73102 - acc: 0.7610 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1686  | total loss: \u001b[1m\u001b[32m2.12522\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1686 | loss: 2.12522 - acc: 0.6849 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1687  | total loss: \u001b[1m\u001b[32m1.93313\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1687 | loss: 1.93313 - acc: 0.7164 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1688  | total loss: \u001b[1m\u001b[32m1.76067\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1688 | loss: 1.76067 - acc: 0.7448 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1689  | total loss: \u001b[1m\u001b[32m1.60582\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1689 | loss: 1.60582 - acc: 0.7703 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1690  | total loss: \u001b[1m\u001b[32m1.46678\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1690 | loss: 1.46678 - acc: 0.7933 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1691  | total loss: \u001b[1m\u001b[32m1.34191\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1691 | loss: 1.34191 - acc: 0.8140 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1692  | total loss: \u001b[1m\u001b[32m1.22975\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1692 | loss: 1.22975 - acc: 0.8326 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1693  | total loss: \u001b[1m\u001b[32m1.12900\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1693 | loss: 1.12900 - acc: 0.8493 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1694  | total loss: \u001b[1m\u001b[32m1.56833\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1694 | loss: 1.56833 - acc: 0.7644 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1695  | total loss: \u001b[1m\u001b[32m1.43416\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1695 | loss: 1.43416 - acc: 0.7879 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1696  | total loss: \u001b[1m\u001b[32m1.31366\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1696 | loss: 1.31366 - acc: 0.8091 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1697  | total loss: \u001b[1m\u001b[32m1.20542\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1697 | loss: 1.20542 - acc: 0.8282 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1698  | total loss: \u001b[1m\u001b[32m1.10816\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1698 | loss: 1.10816 - acc: 0.8454 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1699  | total loss: \u001b[1m\u001b[32m1.02074\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1699 | loss: 1.02074 - acc: 0.8609 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1700  | total loss: \u001b[1m\u001b[32m1.34382\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1700 | loss: 1.34382 - acc: 0.8020 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1701  | total loss: \u001b[1m\u001b[32m1.23307\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1701 | loss: 1.23307 - acc: 0.8218 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1702  | total loss: \u001b[1m\u001b[32m1.13351\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1702 | loss: 1.13351 - acc: 0.8397 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1703  | total loss: \u001b[1m\u001b[32m1.04398\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1703 | loss: 1.04398 - acc: 0.8557 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1704  | total loss: \u001b[1m\u001b[32m1.50549\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1704 | loss: 1.50549 - acc: 0.7701 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1705  | total loss: \u001b[1m\u001b[32m1.37903\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1705 | loss: 1.37903 - acc: 0.7931 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1706  | total loss: \u001b[1m\u001b[32m1.26538\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1706 | loss: 1.26538 - acc: 0.8138 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1707  | total loss: \u001b[1m\u001b[32m1.16324\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1707 | loss: 1.16324 - acc: 0.8324 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1708  | total loss: \u001b[1m\u001b[32m1.61414\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1708 | loss: 1.61414 - acc: 0.7583 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1709  | total loss: \u001b[1m\u001b[32m1.47746\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1709 | loss: 1.47746 - acc: 0.7824 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1710  | total loss: \u001b[1m\u001b[32m1.35465\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1710 | loss: 1.35465 - acc: 0.8042 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1711  | total loss: \u001b[1m\u001b[32m1.24428\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1711 | loss: 1.24428 - acc: 0.8238 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1712  | total loss: \u001b[1m\u001b[32m1.14506\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1712 | loss: 1.14506 - acc: 0.8414 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1713  | total loss: \u001b[1m\u001b[32m1.05583\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1713 | loss: 1.05583 - acc: 0.8573 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1714  | total loss: \u001b[1m\u001b[32m0.97557\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1714 | loss: 0.97557 - acc: 0.8715 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1715  | total loss: \u001b[1m\u001b[32m0.90334\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1715 | loss: 0.90334 - acc: 0.8844 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1716  | total loss: \u001b[1m\u001b[32m0.83832\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1716 | loss: 0.83832 - acc: 0.8959 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1717  | total loss: \u001b[1m\u001b[32m0.77976\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1717 | loss: 0.77976 - acc: 0.9063 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1718  | total loss: \u001b[1m\u001b[32m0.72699\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1718 | loss: 0.72699 - acc: 0.9157 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1719  | total loss: \u001b[1m\u001b[32m0.67942\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1719 | loss: 0.67942 - acc: 0.9241 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1720  | total loss: \u001b[1m\u001b[32m0.94800\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1720 | loss: 0.94800 - acc: 0.8408 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1721  | total loss: \u001b[1m\u001b[32m0.87826\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1721 | loss: 0.87826 - acc: 0.8567 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1722  | total loss: \u001b[1m\u001b[32m0.81549\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1722 | loss: 0.81549 - acc: 0.8711 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1723  | total loss: \u001b[1m\u001b[32m0.75897\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1723 | loss: 0.75897 - acc: 0.8840 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1724  | total loss: \u001b[1m\u001b[32m0.70806\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1724 | loss: 0.70806 - acc: 0.8956 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1725  | total loss: \u001b[1m\u001b[32m0.66218\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1725 | loss: 0.66218 - acc: 0.9060 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1726  | total loss: \u001b[1m\u001b[32m0.62079\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1726 | loss: 0.62079 - acc: 0.9154 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1727  | total loss: \u001b[1m\u001b[32m0.58345\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1727 | loss: 0.58345 - acc: 0.9239 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1728  | total loss: \u001b[1m\u001b[32m0.54973\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1728 | loss: 0.54973 - acc: 0.9315 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1729  | total loss: \u001b[1m\u001b[32m0.51925\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1729 | loss: 0.51925 - acc: 0.9383 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1730  | total loss: \u001b[1m\u001b[32m0.49169\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1730 | loss: 0.49169 - acc: 0.9445 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1731  | total loss: \u001b[1m\u001b[32m0.46674\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1731 | loss: 0.46674 - acc: 0.9500 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1732  | total loss: \u001b[1m\u001b[32m0.44413\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1732 | loss: 0.44413 - acc: 0.9550 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1733  | total loss: \u001b[1m\u001b[32m0.42362\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1733 | loss: 0.42362 - acc: 0.9595 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1734  | total loss: \u001b[1m\u001b[32m0.40499\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1734 | loss: 0.40499 - acc: 0.9636 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1735  | total loss: \u001b[1m\u001b[32m0.38806\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1735 | loss: 0.38806 - acc: 0.9672 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1736  | total loss: \u001b[1m\u001b[32m0.37265\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1736 | loss: 0.37265 - acc: 0.9705 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1737  | total loss: \u001b[1m\u001b[32m0.35860\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1737 | loss: 0.35860 - acc: 0.9735 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1738  | total loss: \u001b[1m\u001b[32m0.34578\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1738 | loss: 0.34578 - acc: 0.9761 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1739  | total loss: \u001b[1m\u001b[32m0.33405\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1739 | loss: 0.33405 - acc: 0.9785 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1740  | total loss: \u001b[1m\u001b[32m0.32332\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1740 | loss: 0.32332 - acc: 0.9806 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1741  | total loss: \u001b[1m\u001b[32m0.31348\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1741 | loss: 0.31348 - acc: 0.9826 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1742  | total loss: \u001b[1m\u001b[32m0.81651\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1742 | loss: 0.81651 - acc: 0.8843 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1743  | total loss: \u001b[1m\u001b[32m0.75713\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1743 | loss: 0.75713 - acc: 0.8959 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1744  | total loss: \u001b[1m\u001b[32m0.70363\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1744 | loss: 0.70363 - acc: 0.9063 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1745  | total loss: \u001b[1m\u001b[32m0.65543\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1745 | loss: 0.65543 - acc: 0.9157 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1746  | total loss: \u001b[1m\u001b[32m0.61196\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1746 | loss: 0.61196 - acc: 0.9241 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1747  | total loss: \u001b[1m\u001b[32m0.57276\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1747 | loss: 0.57276 - acc: 0.9317 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1748  | total loss: \u001b[1m\u001b[32m1.09518\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1748 | loss: 1.09518 - acc: 0.8385 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1749  | total loss: \u001b[1m\u001b[32m1.00763\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1749 | loss: 1.00763 - acc: 0.8547 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1750  | total loss: \u001b[1m\u001b[32m0.92888\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1750 | loss: 0.92888 - acc: 0.8692 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1751  | total loss: \u001b[1m\u001b[32m0.85803\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1751 | loss: 0.85803 - acc: 0.8823 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1752  | total loss: \u001b[1m\u001b[32m0.79426\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1752 | loss: 0.79426 - acc: 0.8941 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1753  | total loss: \u001b[1m\u001b[32m0.73686\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1753 | loss: 0.73686 - acc: 0.9047 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1754  | total loss: \u001b[1m\u001b[32m0.68517\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1754 | loss: 0.68517 - acc: 0.9142 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1755  | total loss: \u001b[1m\u001b[32m0.63859\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1755 | loss: 0.63859 - acc: 0.9228 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1756  | total loss: \u001b[1m\u001b[32m0.59661\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1756 | loss: 0.59661 - acc: 0.9305 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1757  | total loss: \u001b[1m\u001b[32m0.55875\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1757 | loss: 0.55875 - acc: 0.9374 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1758  | total loss: \u001b[1m\u001b[32m0.52459\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1758 | loss: 0.52459 - acc: 0.9437 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1759  | total loss: \u001b[1m\u001b[32m0.49375\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1759 | loss: 0.49375 - acc: 0.9493 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1760  | total loss: \u001b[1m\u001b[32m0.95693\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1760 | loss: 0.95693 - acc: 0.8635 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1761  | total loss: \u001b[1m\u001b[32m0.88278\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1761 | loss: 0.88278 - acc: 0.8771 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1762  | total loss: \u001b[1m\u001b[32m0.81606\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1762 | loss: 0.81606 - acc: 0.8894 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1763  | total loss: \u001b[1m\u001b[32m0.75600\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1763 | loss: 0.75600 - acc: 0.9005 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1764  | total loss: \u001b[1m\u001b[32m1.16473\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1764 | loss: 1.16473 - acc: 0.8286 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1765  | total loss: \u001b[1m\u001b[32m1.06987\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1765 | loss: 1.06987 - acc: 0.8458 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1766  | total loss: \u001b[1m\u001b[32m0.98457\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1766 | loss: 0.98457 - acc: 0.8612 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1767  | total loss: \u001b[1m\u001b[32m0.90786\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1767 | loss: 0.90786 - acc: 0.8751 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1768  | total loss: \u001b[1m\u001b[32m0.83884\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1768 | loss: 0.83884 - acc: 0.8876 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1769  | total loss: \u001b[1m\u001b[32m0.77673\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1769 | loss: 0.77673 - acc: 0.8988 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1770  | total loss: \u001b[1m\u001b[32m0.72082\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1770 | loss: 0.72082 - acc: 0.9089 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1771  | total loss: \u001b[1m\u001b[32m0.67047\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1771 | loss: 0.67047 - acc: 0.9180 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1772  | total loss: \u001b[1m\u001b[32m0.62512\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1772 | loss: 0.62512 - acc: 0.9262 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1773  | total loss: \u001b[1m\u001b[32m0.58424\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1773 | loss: 0.58424 - acc: 0.9336 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1774  | total loss: \u001b[1m\u001b[32m0.54737\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1774 | loss: 0.54737 - acc: 0.9402 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1775  | total loss: \u001b[1m\u001b[32m0.51411\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1775 | loss: 0.51411 - acc: 0.9462 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1776  | total loss: \u001b[1m\u001b[32m0.98333\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1776 | loss: 0.98333 - acc: 0.8698 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1777  | total loss: \u001b[1m\u001b[32m0.90644\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1777 | loss: 0.90644 - acc: 0.8828 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1778  | total loss: \u001b[1m\u001b[32m1.24955\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1778 | loss: 1.24955 - acc: 0.8036 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1779  | total loss: \u001b[1m\u001b[32m1.14623\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1779 | loss: 1.14623 - acc: 0.8232 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1780  | total loss: \u001b[1m\u001b[32m1.05337\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1780 | loss: 1.05337 - acc: 0.8409 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1781  | total loss: \u001b[1m\u001b[32m0.96989\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1781 | loss: 0.96989 - acc: 0.8568 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1782  | total loss: \u001b[1m\u001b[32m0.89482\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1782 | loss: 0.89482 - acc: 0.8711 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1783  | total loss: \u001b[1m\u001b[32m0.82731\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1783 | loss: 0.82731 - acc: 0.8840 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1784  | total loss: \u001b[1m\u001b[32m0.76657\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1784 | loss: 0.76657 - acc: 0.8956 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1785  | total loss: \u001b[1m\u001b[32m0.71190\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1785 | loss: 0.71190 - acc: 0.9061 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1786  | total loss: \u001b[1m\u001b[32m0.66267\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1786 | loss: 0.66267 - acc: 0.9155 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1787  | total loss: \u001b[1m\u001b[32m0.61833\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1787 | loss: 0.61833 - acc: 0.9239 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1788  | total loss: \u001b[1m\u001b[32m0.57837\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1788 | loss: 0.57837 - acc: 0.9315 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1789  | total loss: \u001b[1m\u001b[32m0.54234\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1789 | loss: 0.54234 - acc: 0.9384 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1790  | total loss: \u001b[1m\u001b[32m0.50983\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1790 | loss: 0.50983 - acc: 0.9445 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1791  | total loss: \u001b[1m\u001b[32m0.48048\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1791 | loss: 0.48048 - acc: 0.9501 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1792  | total loss: \u001b[1m\u001b[32m0.45397\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1792 | loss: 0.45397 - acc: 0.9551 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1793  | total loss: \u001b[1m\u001b[32m0.42999\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1793 | loss: 0.42999 - acc: 0.9596 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1794  | total loss: \u001b[1m\u001b[32m0.83577\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1794 | loss: 0.83577 - acc: 0.8818 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1795  | total loss: \u001b[1m\u001b[32m0.77350\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1795 | loss: 0.77350 - acc: 0.8936 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1796  | total loss: \u001b[1m\u001b[32m0.71745\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1796 | loss: 0.71745 - acc: 0.9043 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1797  | total loss: \u001b[1m\u001b[32m0.66698\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1797 | loss: 0.66698 - acc: 0.9138 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1798  | total loss: \u001b[1m\u001b[32m0.62151\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1798 | loss: 0.62151 - acc: 0.9224 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1799  | total loss: \u001b[1m\u001b[32m0.58053\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1799 | loss: 0.58053 - acc: 0.9302 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1800  | total loss: \u001b[1m\u001b[32m0.54358\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1800 | loss: 0.54358 - acc: 0.9372 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1801  | total loss: \u001b[1m\u001b[32m0.51025\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1801 | loss: 0.51025 - acc: 0.9435 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1802  | total loss: \u001b[1m\u001b[32m0.88520\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1802 | loss: 0.88520 - acc: 0.8491 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1803  | total loss: \u001b[1m\u001b[32m0.81763\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1803 | loss: 0.81763 - acc: 0.8642 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1804  | total loss: \u001b[1m\u001b[32m0.75682\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1804 | loss: 0.75682 - acc: 0.8778 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1805  | total loss: \u001b[1m\u001b[32m0.70207\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1805 | loss: 0.70207 - acc: 0.8900 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1806  | total loss: \u001b[1m\u001b[32m0.65276\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1806 | loss: 0.65276 - acc: 0.9010 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1807  | total loss: \u001b[1m\u001b[32m0.60834\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1807 | loss: 0.60834 - acc: 0.9109 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1808  | total loss: \u001b[1m\u001b[32m0.56829\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1808 | loss: 0.56829 - acc: 0.9198 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1809  | total loss: \u001b[1m\u001b[32m0.53218\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1809 | loss: 0.53218 - acc: 0.9278 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1810  | total loss: \u001b[1m\u001b[32m0.49960\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1810 | loss: 0.49960 - acc: 0.9350 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1811  | total loss: \u001b[1m\u001b[32m0.47019\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1811 | loss: 0.47019 - acc: 0.9415 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1812  | total loss: \u001b[1m\u001b[32m0.44361\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1812 | loss: 0.44361 - acc: 0.9474 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1813  | total loss: \u001b[1m\u001b[32m0.41959\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1813 | loss: 0.41959 - acc: 0.9527 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1814  | total loss: \u001b[1m\u001b[32m0.39786\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1814 | loss: 0.39786 - acc: 0.9574 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1815  | total loss: \u001b[1m\u001b[32m0.37818\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1815 | loss: 0.37818 - acc: 0.9616 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1816  | total loss: \u001b[1m\u001b[32m0.95630\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1816 | loss: 0.95630 - acc: 0.8655 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1817  | total loss: \u001b[1m\u001b[32m0.88071\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1817 | loss: 0.88071 - acc: 0.8789 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1818  | total loss: \u001b[1m\u001b[32m0.81272\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1818 | loss: 0.81272 - acc: 0.8910 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1819  | total loss: \u001b[1m\u001b[32m0.75154\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1819 | loss: 0.75154 - acc: 0.9019 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1820  | total loss: \u001b[1m\u001b[32m1.24505\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1820 | loss: 1.24505 - acc: 0.8117 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1821  | total loss: \u001b[1m\u001b[32m1.14078\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1821 | loss: 1.14078 - acc: 0.8306 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1822  | total loss: \u001b[1m\u001b[32m1.04706\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1822 | loss: 1.04706 - acc: 0.8475 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1823  | total loss: \u001b[1m\u001b[32m0.96280\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1823 | loss: 0.96280 - acc: 0.8628 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1824  | total loss: \u001b[1m\u001b[32m1.36655\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1824 | loss: 1.36655 - acc: 0.7765 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1825  | total loss: \u001b[1m\u001b[32m1.25059\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1825 | loss: 1.25059 - acc: 0.7988 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1826  | total loss: \u001b[1m\u001b[32m1.14637\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1826 | loss: 1.14637 - acc: 0.8190 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1827  | total loss: \u001b[1m\u001b[32m1.05269\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1827 | loss: 1.05269 - acc: 0.8371 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1828  | total loss: \u001b[1m\u001b[32m0.96846\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1828 | loss: 0.96846 - acc: 0.8534 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1829  | total loss: \u001b[1m\u001b[32m0.89271\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1829 | loss: 0.89271 - acc: 0.8680 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1830  | total loss: \u001b[1m\u001b[32m0.82457\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1830 | loss: 0.82457 - acc: 0.8812 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1831  | total loss: \u001b[1m\u001b[32m0.76326\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1831 | loss: 0.76326 - acc: 0.8931 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1832  | total loss: \u001b[1m\u001b[32m0.70807\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1832 | loss: 0.70807 - acc: 0.9038 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1833  | total loss: \u001b[1m\u001b[32m0.65838\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1833 | loss: 0.65838 - acc: 0.9134 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1834  | total loss: \u001b[1m\u001b[32m1.18545\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1834 | loss: 1.18545 - acc: 0.8221 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1835  | total loss: \u001b[1m\u001b[32m1.08809\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1835 | loss: 1.08809 - acc: 0.8399 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1836  | total loss: \u001b[1m\u001b[32m1.46381\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1836 | loss: 1.46381 - acc: 0.7741 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1837  | total loss: \u001b[1m\u001b[32m1.33887\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1837 | loss: 1.33887 - acc: 0.7966 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1838  | total loss: \u001b[1m\u001b[32m1.76241\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1838 | loss: 1.76241 - acc: 0.7170 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1839  | total loss: \u001b[1m\u001b[32m1.60803\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1839 | loss: 1.60803 - acc: 0.7453 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1840  | total loss: \u001b[1m\u001b[32m1.46934\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1840 | loss: 1.46934 - acc: 0.7708 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1841  | total loss: \u001b[1m\u001b[32m1.34471\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1841 | loss: 1.34471 - acc: 0.7937 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1842  | total loss: \u001b[1m\u001b[32m1.23271\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1842 | loss: 1.23271 - acc: 0.8143 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1843  | total loss: \u001b[1m\u001b[32m1.13204\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1843 | loss: 1.13204 - acc: 0.8329 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1844  | total loss: \u001b[1m\u001b[32m1.04154\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1844 | loss: 1.04154 - acc: 0.8496 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1845  | total loss: \u001b[1m\u001b[32m0.96015\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1845 | loss: 0.96015 - acc: 0.8646 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1846  | total loss: \u001b[1m\u001b[32m0.88693\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1846 | loss: 0.88693 - acc: 0.8782 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1847  | total loss: \u001b[1m\u001b[32m0.82106\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1847 | loss: 0.82106 - acc: 0.8904 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1848  | total loss: \u001b[1m\u001b[32m0.76177\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1848 | loss: 0.76177 - acc: 0.9013 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1849  | total loss: \u001b[1m\u001b[32m0.70838\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1849 | loss: 0.70838 - acc: 0.9112 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1850  | total loss: \u001b[1m\u001b[32m0.66029\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1850 | loss: 0.66029 - acc: 0.9201 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1851  | total loss: \u001b[1m\u001b[32m0.61694\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1851 | loss: 0.61694 - acc: 0.9281 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1852  | total loss: \u001b[1m\u001b[32m1.24550\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1852 | loss: 1.24550 - acc: 0.8353 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1853  | total loss: \u001b[1m\u001b[32m1.14368\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1853 | loss: 1.14368 - acc: 0.8517 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1854  | total loss: \u001b[1m\u001b[32m1.59938\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1854 | loss: 1.59938 - acc: 0.7666 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1855  | total loss: \u001b[1m\u001b[32m1.46250\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1855 | loss: 1.46250 - acc: 0.7899 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1856  | total loss: \u001b[1m\u001b[32m1.82437\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1856 | loss: 1.82437 - acc: 0.7200 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1857  | total loss: \u001b[1m\u001b[32m1.66553\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1857 | loss: 1.66553 - acc: 0.7480 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1858  | total loss: \u001b[1m\u001b[32m1.52287\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1858 | loss: 1.52287 - acc: 0.7732 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1859  | total loss: \u001b[1m\u001b[32m1.39472\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1859 | loss: 1.39472 - acc: 0.7959 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1860  | total loss: \u001b[1m\u001b[32m1.27959\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1860 | loss: 1.27959 - acc: 0.8163 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1861  | total loss: \u001b[1m\u001b[32m1.17612\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1861 | loss: 1.17612 - acc: 0.8347 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1862  | total loss: \u001b[1m\u001b[32m1.67828\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1862 | loss: 1.67828 - acc: 0.7512 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1863  | total loss: \u001b[1m\u001b[32m1.53537\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1863 | loss: 1.53537 - acc: 0.7761 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1864  | total loss: \u001b[1m\u001b[32m1.40699\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1864 | loss: 1.40699 - acc: 0.7985 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1865  | total loss: \u001b[1m\u001b[32m1.29165\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1865 | loss: 1.29165 - acc: 0.8186 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1866  | total loss: \u001b[1m\u001b[32m1.18801\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1866 | loss: 1.18801 - acc: 0.8368 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1867  | total loss: \u001b[1m\u001b[32m1.09484\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1867 | loss: 1.09484 - acc: 0.8531 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1868  | total loss: \u001b[1m\u001b[32m1.01108\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1868 | loss: 1.01108 - acc: 0.8678 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1869  | total loss: \u001b[1m\u001b[32m0.93574\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1869 | loss: 0.93574 - acc: 0.8810 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1870  | total loss: \u001b[1m\u001b[32m0.86796\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1870 | loss: 0.86796 - acc: 0.8929 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1871  | total loss: \u001b[1m\u001b[32m0.80694\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1871 | loss: 0.80694 - acc: 0.9036 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1872  | total loss: \u001b[1m\u001b[32m1.09677\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1872 | loss: 1.09677 - acc: 0.8314 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1873  | total loss: \u001b[1m\u001b[32m1.01292\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1873 | loss: 1.01292 - acc: 0.8483 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1874  | total loss: \u001b[1m\u001b[32m0.93751\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1874 | loss: 0.93751 - acc: 0.8635 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1875  | total loss: \u001b[1m\u001b[32m0.86966\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1875 | loss: 0.86966 - acc: 0.8771 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1876  | total loss: \u001b[1m\u001b[32m0.80859\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1876 | loss: 0.80859 - acc: 0.8894 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1877  | total loss: \u001b[1m\u001b[32m0.75360\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1877 | loss: 0.75360 - acc: 0.9005 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1878  | total loss: \u001b[1m\u001b[32m0.70405\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1878 | loss: 0.70405 - acc: 0.9104 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1879  | total loss: \u001b[1m\u001b[32m0.65938\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1879 | loss: 0.65938 - acc: 0.9194 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1880  | total loss: \u001b[1m\u001b[32m0.61909\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1880 | loss: 0.61909 - acc: 0.9274 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1881  | total loss: \u001b[1m\u001b[32m0.58272\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1881 | loss: 0.58272 - acc: 0.9347 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1882  | total loss: \u001b[1m\u001b[32m0.54987\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1882 | loss: 0.54987 - acc: 0.9412 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1883  | total loss: \u001b[1m\u001b[32m0.52017\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1883 | loss: 0.52017 - acc: 0.9471 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1884  | total loss: \u001b[1m\u001b[32m0.49330\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1884 | loss: 0.49330 - acc: 0.9524 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1885  | total loss: \u001b[1m\u001b[32m0.46897\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1885 | loss: 0.46897 - acc: 0.9572 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1886  | total loss: \u001b[1m\u001b[32m0.44691\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1886 | loss: 0.44691 - acc: 0.9614 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1887  | total loss: \u001b[1m\u001b[32m0.42688\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1887 | loss: 0.42688 - acc: 0.9653 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1888  | total loss: \u001b[1m\u001b[32m0.93046\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1888 | loss: 0.93046 - acc: 0.8688 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1889  | total loss: \u001b[1m\u001b[32m0.86193\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1889 | loss: 0.86193 - acc: 0.8819 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1890  | total loss: \u001b[1m\u001b[32m1.39544\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1890 | loss: 1.39544 - acc: 0.7937 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1891  | total loss: \u001b[1m\u001b[32m1.28057\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1891 | loss: 1.28057 - acc: 0.8143 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1892  | total loss: \u001b[1m\u001b[32m1.17731\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1892 | loss: 1.17731 - acc: 0.8329 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1893  | total loss: \u001b[1m\u001b[32m1.08448\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1893 | loss: 1.08448 - acc: 0.8496 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1894  | total loss: \u001b[1m\u001b[32m1.00100\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1894 | loss: 1.00100 - acc: 0.8646 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1895  | total loss: \u001b[1m\u001b[32m0.92591\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1895 | loss: 0.92591 - acc: 0.8782 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1896  | total loss: \u001b[1m\u001b[32m0.85833\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1896 | loss: 0.85833 - acc: 0.8904 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1897  | total loss: \u001b[1m\u001b[32m0.79749\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1897 | loss: 0.79749 - acc: 0.9013 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1898  | total loss: \u001b[1m\u001b[32m0.74270\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1898 | loss: 0.74270 - acc: 0.9112 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1899  | total loss: \u001b[1m\u001b[32m0.69333\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1899 | loss: 0.69333 - acc: 0.9201 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1900  | total loss: \u001b[1m\u001b[32m0.64882\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1900 | loss: 0.64882 - acc: 0.9281 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1901  | total loss: \u001b[1m\u001b[32m0.60867\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1901 | loss: 0.60867 - acc: 0.9353 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1902  | total loss: \u001b[1m\u001b[32m0.57244\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1902 | loss: 0.57244 - acc: 0.9417 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1903  | total loss: \u001b[1m\u001b[32m0.53971\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1903 | loss: 0.53971 - acc: 0.9476 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1904  | total loss: \u001b[1m\u001b[32m0.99469\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1904 | loss: 0.99469 - acc: 0.8619 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1905  | total loss: \u001b[1m\u001b[32m0.91965\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1905 | loss: 0.91965 - acc: 0.8757 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1906  | total loss: \u001b[1m\u001b[32m0.85213\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1906 | loss: 0.85213 - acc: 0.8881 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1907  | total loss: \u001b[1m\u001b[32m0.79136\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1907 | loss: 0.79136 - acc: 0.8993 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1908  | total loss: \u001b[1m\u001b[32m0.73663\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1908 | loss: 0.73663 - acc: 0.9094 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1909  | total loss: \u001b[1m\u001b[32m0.68734\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1909 | loss: 0.68734 - acc: 0.9185 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1910  | total loss: \u001b[1m\u001b[32m0.64290\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1910 | loss: 0.64290 - acc: 0.9266 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1911  | total loss: \u001b[1m\u001b[32m0.60283\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1911 | loss: 0.60283 - acc: 0.9339 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1912  | total loss: \u001b[1m\u001b[32m0.56667\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1912 | loss: 0.56667 - acc: 0.9406 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1913  | total loss: \u001b[1m\u001b[32m0.53401\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1913 | loss: 0.53401 - acc: 0.9465 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1914  | total loss: \u001b[1m\u001b[32m0.50450\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1914 | loss: 0.50450 - acc: 0.9518 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1915  | total loss: \u001b[1m\u001b[32m0.47781\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1915 | loss: 0.47781 - acc: 0.9567 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1916  | total loss: \u001b[1m\u001b[32m0.89395\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1916 | loss: 0.89395 - acc: 0.8701 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1917  | total loss: \u001b[1m\u001b[32m0.82818\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1917 | loss: 0.82818 - acc: 0.8831 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1918  | total loss: \u001b[1m\u001b[32m1.28411\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1918 | loss: 1.28411 - acc: 0.8039 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1919  | total loss: \u001b[1m\u001b[32m1.17945\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1919 | loss: 1.17945 - acc: 0.8235 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1920  | total loss: \u001b[1m\u001b[32m1.08536\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1920 | loss: 1.08536 - acc: 0.8411 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1921  | total loss: \u001b[1m\u001b[32m1.00073\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1921 | loss: 1.00073 - acc: 0.8570 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1922  | total loss: \u001b[1m\u001b[32m0.92461\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1922 | loss: 0.92461 - acc: 0.8713 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1923  | total loss: \u001b[1m\u001b[32m0.85611\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1923 | loss: 0.85611 - acc: 0.8842 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1924  | total loss: \u001b[1m\u001b[32m1.28469\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1924 | loss: 1.28469 - acc: 0.8049 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1925  | total loss: \u001b[1m\u001b[32m1.18030\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1925 | loss: 1.18030 - acc: 0.8244 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1926  | total loss: \u001b[1m\u001b[32m1.08644\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1926 | loss: 1.08644 - acc: 0.8419 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1927  | total loss: \u001b[1m\u001b[32m1.00204\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1927 | loss: 1.00204 - acc: 0.8577 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1928  | total loss: \u001b[1m\u001b[32m0.92611\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1928 | loss: 0.92611 - acc: 0.8720 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1929  | total loss: \u001b[1m\u001b[32m0.85778\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1929 | loss: 0.85778 - acc: 0.8848 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1930  | total loss: \u001b[1m\u001b[32m1.21907\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1930 | loss: 1.21907 - acc: 0.8054 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1931  | total loss: \u001b[1m\u001b[32m1.12153\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1931 | loss: 1.12153 - acc: 0.8248 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1932  | total loss: \u001b[1m\u001b[32m1.03381\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1932 | loss: 1.03381 - acc: 0.8424 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1933  | total loss: \u001b[1m\u001b[32m0.95490\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1933 | loss: 0.95490 - acc: 0.8581 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1934  | total loss: \u001b[1m\u001b[32m1.29526\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1934 | loss: 1.29526 - acc: 0.7996 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1935  | total loss: \u001b[1m\u001b[32m1.19032\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1935 | loss: 1.19032 - acc: 0.8196 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1936  | total loss: \u001b[1m\u001b[32m1.09595\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1936 | loss: 1.09595 - acc: 0.8377 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1937  | total loss: \u001b[1m\u001b[32m1.01107\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1937 | loss: 1.01107 - acc: 0.8539 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1938  | total loss: \u001b[1m\u001b[32m1.35842\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1938 | loss: 1.35842 - acc: 0.7867 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1939  | total loss: \u001b[1m\u001b[32m1.24746\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1939 | loss: 1.24746 - acc: 0.8080 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1940  | total loss: \u001b[1m\u001b[32m1.55719\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1940 | loss: 1.55719 - acc: 0.7454 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1941  | total loss: \u001b[1m\u001b[32m1.42671\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1941 | loss: 1.42671 - acc: 0.7709 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1942  | total loss: \u001b[1m\u001b[32m1.30947\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1942 | loss: 1.30947 - acc: 0.7938 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1943  | total loss: \u001b[1m\u001b[32m1.20411\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1943 | loss: 1.20411 - acc: 0.8144 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1944  | total loss: \u001b[1m\u001b[32m1.61797\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1944 | loss: 1.61797 - acc: 0.7420 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1945  | total loss: \u001b[1m\u001b[32m1.48212\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1945 | loss: 1.48212 - acc: 0.7678 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1946  | total loss: \u001b[1m\u001b[32m1.36005\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1946 | loss: 1.36005 - acc: 0.7911 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1947  | total loss: \u001b[1m\u001b[32m1.25033\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1947 | loss: 1.25033 - acc: 0.8120 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1948  | total loss: \u001b[1m\u001b[32m1.15170\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1948 | loss: 1.15170 - acc: 0.8308 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1949  | total loss: \u001b[1m\u001b[32m1.06301\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1949 | loss: 1.06301 - acc: 0.8477 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1950  | total loss: \u001b[1m\u001b[32m0.98322\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1950 | loss: 0.98322 - acc: 0.8629 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1951  | total loss: \u001b[1m\u001b[32m0.91143\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1951 | loss: 0.91143 - acc: 0.8766 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1952  | total loss: \u001b[1m\u001b[32m0.84679\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1952 | loss: 0.84679 - acc: 0.8890 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1953  | total loss: \u001b[1m\u001b[32m0.78858\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1953 | loss: 0.78858 - acc: 0.9001 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1954  | total loss: \u001b[1m\u001b[32m0.73613\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1954 | loss: 0.73613 - acc: 0.9101 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1955  | total loss: \u001b[1m\u001b[32m0.68885\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1955 | loss: 0.68885 - acc: 0.9191 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1956  | total loss: \u001b[1m\u001b[32m0.64619\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1956 | loss: 0.64619 - acc: 0.9271 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1957  | total loss: \u001b[1m\u001b[32m0.60768\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1957 | loss: 0.60768 - acc: 0.9344 -- iter: 11/11\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1958  | total loss: \u001b[1m\u001b[32m0.57290\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1958 | loss: 0.57290 - acc: 0.9410 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1959  | total loss: \u001b[1m\u001b[32m0.54145\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1959 | loss: 0.54145 - acc: 0.9469 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1960  | total loss: \u001b[1m\u001b[32m0.51300\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1960 | loss: 0.51300 - acc: 0.9522 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1961  | total loss: \u001b[1m\u001b[32m0.48724\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1961 | loss: 0.48724 - acc: 0.9570 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1962  | total loss: \u001b[1m\u001b[32m0.46389\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1962 | loss: 0.46389 - acc: 0.9613 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1963  | total loss: \u001b[1m\u001b[32m0.44270\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1963 | loss: 0.44270 - acc: 0.9652 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1964  | total loss: \u001b[1m\u001b[32m0.42345\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1964 | loss: 0.42345 - acc: 0.9686 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1965  | total loss: \u001b[1m\u001b[32m0.40595\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1965 | loss: 0.40595 - acc: 0.9718 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1966  | total loss: \u001b[1m\u001b[32m0.39001\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1966 | loss: 0.39001 - acc: 0.9746 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1967  | total loss: \u001b[1m\u001b[32m0.37547\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1967 | loss: 0.37547 - acc: 0.9771 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1968  | total loss: \u001b[1m\u001b[32m0.36220\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1968 | loss: 0.36220 - acc: 0.9794 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1969  | total loss: \u001b[1m\u001b[32m0.35006\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1969 | loss: 0.35006 - acc: 0.9815 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1970  | total loss: \u001b[1m\u001b[32m0.68412\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1970 | loss: 0.68412 - acc: 0.9015 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1971  | total loss: \u001b[1m\u001b[32m0.63952\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1971 | loss: 0.63952 - acc: 0.9114 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1972  | total loss: \u001b[1m\u001b[32m0.59929\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1972 | loss: 0.59929 - acc: 0.9202 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1973  | total loss: \u001b[1m\u001b[32m0.56299\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1973 | loss: 0.56299 - acc: 0.9282 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1974  | total loss: \u001b[1m\u001b[32m1.03199\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1974 | loss: 1.03199 - acc: 0.8354 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1975  | total loss: \u001b[1m\u001b[32m0.95235\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1975 | loss: 0.95235 - acc: 0.8518 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1976  | total loss: \u001b[1m\u001b[32m0.88070\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1976 | loss: 0.88070 - acc: 0.8667 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1977  | total loss: \u001b[1m\u001b[32m0.81620\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1977 | loss: 0.81620 - acc: 0.8800 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1978  | total loss: \u001b[1m\u001b[32m0.75813\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1978 | loss: 0.75813 - acc: 0.8920 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1979  | total loss: \u001b[1m\u001b[32m0.70583\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1979 | loss: 0.70583 - acc: 0.9028 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1980  | total loss: \u001b[1m\u001b[32m0.65871\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1980 | loss: 0.65871 - acc: 0.9125 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1981  | total loss: \u001b[1m\u001b[32m0.61622\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1981 | loss: 0.61622 - acc: 0.9213 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1982  | total loss: \u001b[1m\u001b[32m0.57790\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1982 | loss: 0.57790 - acc: 0.9291 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1983  | total loss: \u001b[1m\u001b[32m0.54331\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1983 | loss: 0.54331 - acc: 0.9362 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1984  | total loss: \u001b[1m\u001b[32m0.87411\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1984 | loss: 0.87411 - acc: 0.8426 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1985  | total loss: \u001b[1m\u001b[32m0.80980\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1985 | loss: 0.80980 - acc: 0.8583 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1986  | total loss: \u001b[1m\u001b[32m0.75190\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1986 | loss: 0.75190 - acc: 0.8725 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1987  | total loss: \u001b[1m\u001b[32m0.69976\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1987 | loss: 0.69976 - acc: 0.8853 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1988  | total loss: \u001b[1m\u001b[32m0.65278\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1988 | loss: 0.65278 - acc: 0.8967 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1989  | total loss: \u001b[1m\u001b[32m0.61043\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1989 | loss: 0.61043 - acc: 0.9071 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1990  | total loss: \u001b[1m\u001b[32m0.57223\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1990 | loss: 0.57223 - acc: 0.9164 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1991  | total loss: \u001b[1m\u001b[32m0.53776\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1991 | loss: 0.53776 - acc: 0.9247 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1992  | total loss: \u001b[1m\u001b[32m1.06421\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1992 | loss: 1.06421 - acc: 0.8413 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1993  | total loss: \u001b[1m\u001b[32m0.98050\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1993 | loss: 0.98050 - acc: 0.8572 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1994  | total loss: \u001b[1m\u001b[32m1.42248\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1994 | loss: 1.42248 - acc: 0.7806 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1995  | total loss: \u001b[1m\u001b[32m1.30314\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1995 | loss: 1.30314 - acc: 0.8025 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1996  | total loss: \u001b[1m\u001b[32m1.60692\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1996 | loss: 1.60692 - acc: 0.7314 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1997  | total loss: \u001b[1m\u001b[32m1.46948\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1997 | loss: 1.46948 - acc: 0.7582 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1998  | total loss: \u001b[1m\u001b[32m1.34595\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1998 | loss: 1.34595 - acc: 0.7824 -- iter: 11/11\n",
      "--\n",
      "Training Step: 1999  | total loss: \u001b[1m\u001b[32m1.23491\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1999 | loss: 1.23491 - acc: 0.8042 -- iter: 11/11\n",
      "--\n",
      "Training Step: 2000  | total loss: \u001b[1m\u001b[32m1.54641\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2000 | loss: 1.54641 - acc: 0.7328 -- iter: 11/11\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "tf.reset_default_graph()\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 20)\n",
    "net = tflearn.fully_connected(net, 20)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs') \n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=2000, batch_size=20, show_metric=True)#n_epoch is the number of times network sees the data\n",
    "model.save('model.tflearn')\n",
    "import pickle\n",
    "pickle.dump({'words':words, 'classes':classes,'train_x':train_x,'train_y':train_y},open( \"training_data\", \"wb\" ))\n",
    "\n",
    "\n",
    "\n",
    "# restore all of our data structures\n",
    "import pickle\n",
    "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
    "words = data['words']\n",
    "classes = data['classes']\n",
    "train_x = data['train_x']\n",
    "train_y = data['train_y']\n",
    "import json\n",
    "with open('docman.json') as jd:\n",
    "    intents = json.load(jd)\n",
    "    \n",
    "# load our saved model\n",
    "model.load('./model.tflearn')\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Processes    Burst Time     Waiting Time    Turn-Around Time\n",
      "  1 \t\t 10 \t\t 13 \t\t 23\n",
      "  2 \t\t 5 \t\t 10 \t\t 15\n",
      "  3 \t\t 8 \t\t 13 \t\t 21\n",
      "\n",
      "Average waiting time = 12.00000 \n",
      "Average turn around time = 19.66667 \n",
      "2\n",
      "Processes    Burst Time     Waiting Time    Turn-Around Time\n",
      "  1 \t\t 10 \t\t 13 \t\t 23\n",
      "  2 \t\t 5 \t\t 10 \t\t 15\n",
      "  3 \t\t 8 \t\t 13 \t\t 21\n",
      "\n",
      "Average waiting time = 12.00000 \n",
      "Average turn around time = 19.66667 \n"
     ]
    }
   ],
   "source": [
    "##Priority Scheduling LAgorith,\n",
    "  # process id's \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "r=sr.Recognizer()\n",
    "\n",
    "# initialisation \n",
    "engine = pyttsx3.init() \n",
    "engine.setProperty('rate', 150)\n",
    "voices=engine.getProperty('voices')\n",
    "engine.setProperty('voice', voices[1].id)\n",
    "\n",
    "\n",
    "def register():\n",
    "    global register_screen\n",
    "    register_screen = Toplevel(root)\n",
    "    #register_screen.configure(background=\"#c71435\")\n",
    "    register_screen.title(\"Register\")\n",
    "    register_screen.geometry(\"320x550+400+50\")\n",
    " \n",
    "    global username\n",
    "    global password\n",
    "    global name\n",
    "    global age\n",
    "    global height\n",
    "    global weight\n",
    "    global sex\n",
    "    global contact\n",
    "    global username_entry\n",
    "    global password_entry\n",
    "    global name_entry\n",
    "    global age_entry\n",
    "    global height_entry\n",
    "    global weight_entry\n",
    "    global sex_entry\n",
    "    global contact_entry\n",
    "    username = StringVar()\n",
    "    password = StringVar()\n",
    "    name = StringVar()\n",
    "    age = StringVar()\n",
    "    height = StringVar()\n",
    "    weight = StringVar()\n",
    "    sex = StringVar()\n",
    "    contact = StringVar()\n",
    "    Label(register_screen, text=\"\").pack()\n",
    "    Label(register_screen, text=\"Please enter the details below\", font=\"bold 15\").pack()\n",
    "    Label(register_screen, text=\"\").pack()\n",
    "    username_lable = Label(register_screen, text=\"Email id * \")\n",
    "    username_lable.pack()\n",
    "    username_entry = Entry(register_screen, textvariable=username)\n",
    "    username_entry.pack()\n",
    "    password_lable = Label(register_screen, text=\"Password * \")\n",
    "    password_lable.pack()\n",
    "    password_entry = Entry(register_screen, textvariable=password, show='*')\n",
    "    password_entry.pack()\n",
    "    name_lable = Label(register_screen, text=\"Name  \")\n",
    "    name_lable.pack()\n",
    "    name_entry = Entry(register_screen, textvariable=name)\n",
    "    name_entry.pack()\n",
    "    age_lable = Label(register_screen, text=\"Age * \")\n",
    "    age_lable.pack()\n",
    "    age_entry = Entry(register_screen, textvariable=age)\n",
    "    age_entry.pack()\n",
    "    height_lable = Label(register_screen, text=\"Height (in cms) * \")\n",
    "    height_lable.pack()\n",
    "    height_entry = Entry(register_screen, textvariable=height)\n",
    "    height_entry.pack()\n",
    "    weight_lable = Label(register_screen, text=\"Weight (in kgs) * \")\n",
    "    weight_lable.pack()\n",
    "    weight_entry = Entry(register_screen, textvariable=weight)\n",
    "    weight_entry.pack()\n",
    "    sex_lable = Label(register_screen, text=\"Sex (M/F) * \")\n",
    "    sex_lable.pack()\n",
    "    sex_entry = Entry(register_screen, textvariable=sex)\n",
    "    sex_entry.pack()\n",
    "    contact_lable = Label(register_screen, text=\"Contact No. * \")\n",
    "    contact_lable.pack()\n",
    "    contact_entry = Entry(register_screen, textvariable=contact)\n",
    "    contact_entry.pack()\n",
    "    Label(register_screen, text=\"\").pack()\n",
    "    Button(register_screen, text=\"Register\", width=10, height=1, bg=\"#cf2929\", fg=\"white\",command = register_user).pack()\n",
    "    Label(register_screen,text=\"\").pack()\n",
    "    Button(register_screen, text=\"Back\", width=10, height=1, bg=\"black\", fg=\"white\",command = delete_register_user).pack()\n",
    "\n",
    "def delete_register_user():\n",
    "    register_screen.destroy()\n",
    " \n",
    " \n",
    "# Designing window for login \n",
    " \n",
    "def login():\n",
    "    global login_screen\n",
    "    login_screen = Toplevel(root)\n",
    "    #login_screen.configure(background=\"white\")\n",
    "    login_screen.title(\"Login\")\n",
    "    login_screen.geometry(\"320x550+400+50\")\n",
    "    Label(login_screen, text=\"\").pack()\n",
    "    Label(login_screen, text=\"\").pack()\n",
    "    Label(login_screen, text=\"\").pack()\n",
    "    Label(login_screen, text=\"Please enter the details below to login\",font=\"bold 12\").pack()\n",
    "    Label(login_screen, text=\"\").pack()\n",
    " \n",
    "    global username_verify\n",
    "    global password_verify\n",
    " \n",
    "    username_verify = StringVar()\n",
    "    password_verify = StringVar()\n",
    "    \n",
    " \n",
    "    global username_login_entry\n",
    "    global password_login_entry\n",
    " \n",
    "    Label(login_screen, text=\"Email id * \").pack()\n",
    "    username_login_entry = Entry(login_screen, textvariable=username_verify)\n",
    "    username_login_entry.pack()\n",
    "    Label(login_screen, text=\"\").pack()\n",
    "    Label(login_screen, text=\"Password * \").pack()\n",
    "    password_login_entry = Entry(login_screen, textvariable=password_verify, show= '*')\n",
    "    password_login_entry.pack()\n",
    "    Label(login_screen, text=\"\").pack()\n",
    "    Button(login_screen, text=\"Login\", width=10, height=1, command = login_verify).pack()\n",
    "\n",
    "#Implementing event on register button\n",
    " \n",
    "def register_user():\n",
    " \n",
    "    username_info = username.get()\n",
    "    password_info = password.get()\n",
    "    current_id=0\n",
    "\n",
    "    global name_info\n",
    "    name_info= name.get()\n",
    "    age_info = age.get()\n",
    "    height_info = height.get()\n",
    "    weight_info = weight.get()\n",
    "    sex_info = sex.get()\n",
    "    contact_info = contact.get()\n",
    " \n",
    "    file = open(username_info, \"w\")\n",
    "    file.write(username_info + \"\\n\")\n",
    "    file.write(password_info + \"\\n\")\n",
    "    file.write(name_info + \"\\n\")\n",
    "    file.write(age_info + \"\\n\")\n",
    "    file.write(height_info + \"\\n\")\n",
    "    file.write(weight_info + \"\\n\")\n",
    "    file.write(sex_info + \"\\n\")\n",
    "    file.write(contact_info + \"\\n\")\n",
    "    file.write(str(current_id) + \"\\n\")\n",
    "\n",
    "\n",
    "    file.close()\n",
    " \n",
    "    username_entry.delete(0, END)\n",
    "    password_entry.delete(0, END)\n",
    "    name_entry.delete(0,END)\n",
    "    age_entry.delete(0,END)\n",
    "    height_entry.delete(0,END)\n",
    "    weight_entry.delete(0,END)\n",
    "    sex_entry.delete(0,END)\n",
    "    contact_entry.delete(0,END)\n",
    " \n",
    "    Label(register_screen, text=\"Registration Successful\", fg=\"green\", font=(\"calibri\", 11,\"bold\")).pack()\n",
    "    \n",
    "# Implementing event on login button \n",
    " \n",
    "def login_verify():\n",
    "    global username1\n",
    "    username1 = username_verify.get()\n",
    "    password1 = password_verify.get()\n",
    "    username_login_entry.delete(0, END)\n",
    "    password_login_entry.delete(0, END)\n",
    "    global user\n",
    "    user=\"friend\"\n",
    "    list_of_files = os.listdir()\n",
    "    if username1 in list_of_files:\n",
    "        file1 = open(username1, \"r\")\n",
    "        verify = file1.read().splitlines()\n",
    "        if password1 in verify:\n",
    "            user=verify[2]\n",
    "            login_sucess()\n",
    " \n",
    "        else:\n",
    "            password_not_recognised()\n",
    " \n",
    "    else:\n",
    "        user_not_found()\n",
    "# Designing popup for login success\n",
    " \n",
    "def login_sucess():\n",
    "    global login_success_screen\n",
    "    login_success_screen = Toplevel(login_screen)\n",
    "    login_success_screen.title(\"Success\")\n",
    "    login_success_screen.geometry(\"150x100+480+270\")\n",
    "    Label(login_success_screen, text=\"Login Success!\",fg=\"green\",font=\"bold 13\").pack()\n",
    "    Button(login_success_screen, text=\"OK\", command=delete_login_success).pack()\n",
    "# Designing popup for login invalid password\n",
    " \n",
    "def password_not_recognised():\n",
    "    global password_not_recog_screen\n",
    "    password_not_recog_screen = Toplevel(login_screen)\n",
    "    password_not_recog_screen.title(\"Success\")\n",
    "    password_not_recog_screen.geometry(\"150x100+480+270\")\n",
    "    Label(password_not_recog_screen, text=\"Invalid Password\",fg=\"red\",font=\"bold 13\").pack()\n",
    "    Button(password_not_recog_screen, text=\"OK\", command=delete_password_not_recognised).pack()\n",
    "# Designing popup for user not found\n",
    " \n",
    "def user_not_found():\n",
    "    global user_not_found_screen\n",
    "    user_not_found_screen = Toplevel(login_screen)\n",
    "    user_not_found_screen.title(\"Success\")\n",
    "    user_not_found_screen.geometry(\"150x100+480+270\")\n",
    "    Label(user_not_found_screen, text=\"User Not Found!!\",fg=\"red\",font=\"bold 13\").pack()\n",
    "    Button(user_not_found_screen, text=\"OK\", command=delete_user_not_found_screen).pack()\n",
    "# Deleting popups\n",
    " \n",
    "def delete_login_success():\n",
    "    login_success_screen.destroy()\n",
    "    login_screen.destroy()\n",
    "    home_screen()\n",
    " \n",
    " \n",
    "def delete_password_not_recognised():\n",
    "    password_not_recog_screen.destroy()\n",
    " \n",
    " \n",
    "def delete_user_not_found_screen():\n",
    "    user_not_found_screen.destroy()\n",
    "    \n",
    "def home_screen():\n",
    "    global home_screen\n",
    "    home_screen = Toplevel(root)\n",
    "    home_screen.title(\"HOME\")\n",
    "    home_screen.geometry(\"320x550+400+50\")\n",
    "    home_screen.configure(background='white')\n",
    "    Label(home_screen,text=\"Welcome\", bg=\"white\",fg=\"#cf2929\" ,width=\"300\", height=\"2\", font=(\"Calibri\", 18)).pack()\n",
    "    Label(home_screen,text=\"\").pack()\n",
    "    Button(home_screen, text=\"About\", width=15, fg=\"white\",height=1, bg=\"#cf2929\",font=\"bold 12\" ,command = about).pack()\n",
    "    Label(home_screen,text=\"\").pack()\n",
    "    Button(home_screen, text=\"Enter Your ID\", width=15, fg=\"white\",height=1, bg=\"#cf2929\", font=\"bold 12\" ,command = reports).pack()\n",
    "    Label(home_screen,text=\"\").pack()\n",
    "    Button(home_screen, text=\"Support\", width=15, fg=\"white\",height=1, bg=\"#cf2929\",font=\"bold 12\" , command = chatbot).pack()\n",
    "    Label(home_screen,text=\"\").pack()\n",
    "    Button(home_screen, text=\"Instructions\", width=15, fg=\"white\",height=1, bg=\"#cf2929\",font=\"bold 12\" , command = instructions).pack()\n",
    "    Label(home_screen,text=\"\").pack()\n",
    "    Button(home_screen, text=\"Contact\", width=15, fg=\"white\",height=1, bg=\"#cf2929\", font=\"bold 12\" ,command = contact).pack()\n",
    "    Label(home_screen,text=\"\").pack()\n",
    "    Button(home_screen, text=\"Log Out\", width=15, fg=\"white\",height=1, bg=\"black\", font=\"bold 12\" ,command = delete_home_screen).pack()\n",
    "    Label(home_screen,text=\"\").pack()\n",
    "    \n",
    "def delete_home_screen():\n",
    "    home_screen.destroy()\n",
    "    #root.destroy()\n",
    "    \n",
    "\n",
    "    \n",
    "def bot_speaking(message): \n",
    "    # testing \n",
    "    #rate = engine.getProperty('rate')   # getting details of current speaking rate\n",
    "    #print (rate)\n",
    "    engine.say(message)\n",
    "    engine.runAndWait() \n",
    "    \n",
    "def about():\n",
    "    global about_screen\n",
    "    about_screen = Toplevel(home_screen)\n",
    "    about_screen.title(\"ABOUT\")\n",
    "    about_screen.geometry(\"320x550+400+50\")\n",
    "    about_screen.configure(background='white')\n",
    "    Label(about_screen,text=\"DocMan\", bg=\"white\",fg=\"#c71435\" ,width=\"300\", height=\"2\", font=(\"Calibri\", 18)).pack()\n",
    "    Label(about_screen,text=\"\").pack()\n",
    "    \n",
    "    '''img1 = Image.open(\"R:\\\\PROJECTS\\\\DocMan -- Snoring Analysis\\\\bg.png\") \n",
    "    img = img1.resize((500, 300), Image.ANTIALIAS)\n",
    "    photo1=ImageTk.PhotoImage(img)\n",
    "    lab=Label(about_screen,image=photo1).pack()#.place(x=200,y=30)'''\n",
    "    description=\"Welcome to DocMan!\"+\"\\n\"+\"If you are a person who is suffering with snoring, then youve come to the right place at the right time.\"+\"\\n\"+\"Sleep illnesses are often left undiagnosed as their symptoms go unnoticed, unless you have a sleeping partner.\"+\"\\n\"+\"Snoring is not a merely a sound produced due to nose block, there are numerous other reasons behind a persons snoring.\"+\"\\n\"+\" Some can be extremely malicious to our body, if left unchecked it may lead to a much horrifying doom called death.\"+\"\\n\"+\" Hence, it has become a necessity to know the underlying cause of snoring.\"+\"\\n\"+\"People who complain of snoring are often diagnosed with Obstructive Sleep Apnea.\"+\"\\n\"+\" Obstructive sleep apnea (also called as OSA) is a condition which causes pauses in breathing while the person is sleeping.\"+\"\\n\"+\"This leads to lack of oxygen in the body.\"+\"\\n\"+\"DocMan app helps you record your snoring and analyses it and determines if you are suffering with OSA.\"+\"\\n\"+\" If you are suffering with OSA it also tells us if OSA is mild, moderate or severe.\"+\"\\n\"+\"If you are a new user please visit instructions tab provided on home page.\"+\"\\n\"+\"Thank you.\"\n",
    "    #Label(about_screen,text=description,width=\"150\", height=\"14\", font=(\"Calibri\", 12)).pack()\n",
    "    scrollbar = Scrollbar(about_screen, orient='horizontal',width='5')\n",
    "    scrollbar.pack(fill=X)\n",
    "    listbox = Listbox(about_screen,xscrollcommand=scrollbar.set,width='90',height='20')\n",
    "    listbox.insert(END,\"\")\n",
    "    listbox.insert(END,\"\")\n",
    "    listbox.insert(END,\"  Welcome to Smart Diagnostic Centre\")\n",
    "    listbox.insert(END,\"  If you are a new user please visit instructions tab provided on home page.\")\n",
    "    listbox.insert(END,\"  Thank you.\")\n",
    "    listbox.pack( fill=BOTH)\n",
    "    Label(about_screen,text=\"\").pack()\n",
    "    Label(about_screen,text=\"\").pack()\n",
    "    scrollbar.config(command=listbox.xview)\n",
    "    Button(about_screen,text=\"Listen\", height=\"1\", width=\"10\",fg=\"white\", bg=\"#cf2929\", command = speak_about).pack()\n",
    "    Label(about_screen,text=\"\").pack()\n",
    "    Button(about_screen, text=\"Back\", width=10, height=1, bg=\"black\", fg=\"white\",command = delete_about_screen).pack()\n",
    "\n",
    "def delete_about_screen():\n",
    "    about_screen.destroy()\n",
    "    \n",
    "    \n",
    "def speak_about():\n",
    "    \n",
    "    '''list_of_files = os.listdir()\n",
    "    if username1 in list_of_files:\n",
    "        file1 = open(username1, \"r\")\n",
    "        verify = file1.read().splitlines()\n",
    "        global user\n",
    "        user=verify[2]\n",
    "        #print(user)'''\n",
    "    bot_speaking(\"hello\"+user)\n",
    "    bot_speaking(\"I am your personal assistant\")\n",
    "    bot_speaking(\"I will walk you through the process of Using the Application\")\n",
    "    bot_speaking(\"If you are a new user please visit instructions tab provided on home page\")\n",
    "    bot_speaking(\"Thank you\")\n",
    "    \n",
    "n = 3\n",
    "wt = [0] * n \n",
    "def reports():\n",
    "    global report_screen\n",
    "    report_screen = Toplevel(home_screen)\n",
    "    report_screen.title(\"Enter Your ID\")\n",
    "    report_screen.geometry(\"320x550+400+50\")\n",
    "    report_screen.configure(background='white')\n",
    "    Label(report_screen,text=\"Report\",width=\"300\", height=\"2\",fg=\"#cf2929\" ,font=(\"Calibri\", 12,\"bold\")).pack()\n",
    "    Label(report_screen,text=\"\").pack()\n",
    "    import math\n",
    "    proc = [1, 2, 3] \n",
    "   \n",
    "    burst_time = [10, 5, 8]    \n",
    "    quantum = 2; \n",
    "    \n",
    "    def findWaitingTime(processes, n, bt,  wt, quantum):  \n",
    "        rem_bt = [0] * n \n",
    "\n",
    "        # Copy the burst time into rt[]  \n",
    "        for i in range(n):  \n",
    "            rem_bt[i] = bt[i] \n",
    "        t = 0 # Current time  \n",
    "\n",
    "        # Keep traversing processes in round  \n",
    "        # robin manner until all of them are \n",
    "        # not done.  \n",
    "        while(1): \n",
    "            done = True\n",
    "\n",
    "            # Traverse all processes one by \n",
    "            # one repeatedly  \n",
    "            for i in range(n): \n",
    "\n",
    "                # If burst time of a process is greater  \n",
    "                # than 0 then only need to process further  \n",
    "                if (rem_bt[i] > 0) : \n",
    "                    done = False # There is a pending process \n",
    "\n",
    "                    if (rem_bt[i] > quantum) : \n",
    "\n",
    "                        # Increase the value of t i.e. shows  \n",
    "                        # how much time a process has been processed  \n",
    "                        t += quantum  \n",
    "\n",
    "                        # Decrease the burst_time of current  \n",
    "                        # process by quantum  \n",
    "                        rem_bt[i] -= quantum  \n",
    "\n",
    "                    # If burst time is smaller than or equal   \n",
    "                    # to quantum. Last cycle for this process  \n",
    "                    else: \n",
    "\n",
    "                        # Increase the value of t i.e. shows  \n",
    "                        # how much time a process has been processed  \n",
    "                        t = t + rem_bt[i]  \n",
    "\n",
    "                        # Waiting time is current time minus  \n",
    "                        # time used by this process  \n",
    "                        wt[i] = t - bt[i]  \n",
    "\n",
    "                        # As the process gets fully executed  \n",
    "                        # make its remaining burst time = 0  \n",
    "                        rem_bt[i] = 0\n",
    "\n",
    "            # If all processes are done  \n",
    "            if (done == True): \n",
    "                break\n",
    "\n",
    "    # Function to calculate turn around time  \n",
    "    def findTurnAroundTime(processes, n, bt, wt, tat): \n",
    "\n",
    "        # Calculating turnaround time  \n",
    "        for i in range(n): \n",
    "            tat[i] = bt[i] + wt[i]  \n",
    "\n",
    "\n",
    "    # Function to calculate average waiting  \n",
    "    # and turn-around times.  \n",
    "    def findavgTime(processes, n, bt, quantum):  \n",
    "        \n",
    "        tat = [0] * n  \n",
    "\n",
    "        # Function to find waiting time \n",
    "        # of all processes  \n",
    "        findWaitingTime(processes, n, bt,  \n",
    "                             wt, quantum)  \n",
    "\n",
    "        # Function to find turn around time \n",
    "        # for all processes  \n",
    "        findTurnAroundTime(processes, n, bt, \n",
    "                                    wt, tat)  \n",
    "\n",
    "        # Display processes along with all details  \n",
    "        print(\"PatientID    ConsultationTime     Waiting\",  \n",
    "                         \"Time    Turn-Around Time\") \n",
    "        total_wt = 0\n",
    "        total_tat = 0\n",
    "        for i in range(n): \n",
    "\n",
    "            total_wt = total_wt + wt[i]  \n",
    "            total_tat = total_tat + tat[i]  \n",
    "            print(\" \", i + 1, \"\\t\\t\", bt[i],  \n",
    "                  \"\\t\\t\", wt[i], \"\\t\\t\", tat[i]) \n",
    "\n",
    "        print(\"\\nAverage waiting time = %.5f \"%(total_wt /n) ) \n",
    "        print(\"Average turn around time = %.5f \"% (total_tat / n)) \n",
    "        return math.floor((total_wt)/n)\n",
    "    def find(x):\n",
    "        return wt[x]\n",
    "   \n",
    "    \n",
    "    bot_speaking(\"enter your id\")\n",
    "    x=int(input())\n",
    "    x=x-1\n",
    "    a=findavgTime(proc, n, burst_time, quantum)\n",
    "    \n",
    "    b=find(x)\n",
    "    bot_speaking(\"Your Waiting time is\"+str(b)+\"minutes\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    Label(report_screen,text=\"Average waiting\"+ \"timeis \",width=\"300\", height=\"2\",fg=\"black\", font=(\"Calibri\", 14)).pack()\n",
    "    description= a\n",
    "    bot_speaking(\"Average Waiting time is \"+str(a)+\"minuts\")\n",
    "  \n",
    "    Label(report_screen,text=\"\").pack()\n",
    "    Button(report_screen,text=\"Listen\", height=\"1\", width=\"20\",fg=\"white\", bg=\"#cf2929\", command = speak_report).pack()\n",
    "    Label(report_screen,text=\"\").pack()\n",
    "    Button(report_screen, text=\"Back\", width=10, height=1, bg=\"black\", fg=\"white\",command = delete_report_screen).pack()\n",
    "\n",
    "def speak_report():\n",
    "    bot_speaking(\"hey\"+user)\n",
    "    bot_speaking(\"you are in \"+\"Moderate\"+\" stage\")\n",
    "    bot_speaking(\"dont worry we suggest you some precautions\")\n",
    "    \n",
    "def delete_report_screen():\n",
    "    report_screen.destroy()\n",
    " \n",
    "    \n",
    "    \n",
    "\n",
    "def instructions():\n",
    "    global instructions_screen\n",
    "    instructions_screen = Toplevel(home_screen)\n",
    "    instructions_screen.title(\"Instructions\")\n",
    "    instructions_screen.geometry(\"320x550+400+50\")\n",
    "    instructions_screen.configure(background='white')\n",
    "    Label(instructions_screen,text=\"Instructions\", bg=\"white\",fg=\"#c71435\" ,width=\"300\", height=\"2\", font=(\"Calibri\", 18)).pack()\n",
    "    Label(instructions_screen,text=\"\").pack()\n",
    "    Label(instructions_screen,text=\"\").pack()\n",
    "\n",
    "    listbox = Listbox(instructions_screen,width='80',height='15')\n",
    "    listbox.insert(END,\"\")\n",
    "    listbox.insert(END,\"\")\n",
    "    listbox.insert(END,\" Please carry your Previous Reports \")\n",
    "    listbox.insert(END,\"  Book a prior appointment\")\n",
    "    listbox.insert(END,\"  Consultation fee is 500 \")\n",
    "\n",
    "    listbox.insert(END,\"  For any queries use our chat assistance.\")\n",
    "    listbox.pack( fill=BOTH)\n",
    "    Label(instructions_screen,text=\"\").pack()\n",
    "    Button(instructions_screen,text=\"Listen\", height=\"1\", width=\"10\",fg=\"white\", bg=\"#cf2929\", command = speak_instructions).pack()\n",
    "    Label(instructions_screen,text=\"\").pack()\n",
    "    Button(instructions_screen, text=\"Back\", width=\"10\", height=\"1\", bg=\"black\", fg=\"white\",command = delete_instructions_screen).pack()\n",
    "    \n",
    "\n",
    "def speak_instructions():\n",
    "    bot_speaking(\"hello\"+user)\n",
    "    bot_speaking(\"Instructions to be followed are \")\n",
    "    bot_speaking(\" Please carry your Previous Reports \")\n",
    "    bot_speaking(\"Book a prior appointment\")\n",
    "    bot_speaking(\"  Consultation fee is 500 \")\n",
    "   \n",
    "    bot_speaking(\"for queries use our chat assistance\")\n",
    "    bot_speaking(\"Thank you\")\n",
    "    \n",
    "    \n",
    "def delete_instructions_screen():\n",
    "    instructions_screen.destroy()\n",
    "    \n",
    "    \n",
    "def contact():\n",
    "    global contact_screen\n",
    "    contact_screen = Toplevel(home_screen)\n",
    "    contact_screen.title(\"Contact\")\n",
    "    contact_screen.geometry(\"320x550+400+50\")\n",
    "    contact_screen.configure(background='white')\n",
    "    Label(contact_screen,text=\"Contact\", bg=\"white\",fg=\"black\" ,width=\"300\", height=\"1\", font=(\"Calibri\", 14)).pack()\n",
    "    Label(contact_screen,text=\"\").pack()\n",
    "    Label(contact_screen,text=\"smartcare@gmail.com\", bg=\"white\",fg=\"black\" ,width=\"300\", height=\"1\", font=(\"Calibri\", 12)).pack()\n",
    "    Label(contact_screen,text=\"\").pack()\n",
    "    Button(contact_screen, text=\"Back\", width=10, height=1, bg=\"black\", fg=\"white\",command = delete_contact_screen).pack()\n",
    "\n",
    "def delete_contact_screen():\n",
    "    contact_screen.destroy()\n",
    "    \n",
    "    \n",
    "def health_journal():\n",
    "    global var1\n",
    "    global var2\n",
    "    global var3\n",
    "    global a\n",
    "    global b\n",
    "    global c\n",
    "    global today\n",
    "    global journal_screen\n",
    "    \n",
    "    journal_screen = Toplevel(home_screen)\n",
    "    journal_screen.title(\"Health Journal\")\n",
    "    journal_screen.geometry(\"320x550+400+50\")\n",
    "    journal_screen.configure(background='white')\n",
    "    Label(journal_screen,text=\"Health Journal\",width=\"300\", height=\"2\",fg=\"#cf2929\" ,font=(\"Calibri\", 18,\"bold\")).pack()\n",
    "    Label(journal_screen,text=\"\").pack()\n",
    "    Label(journal_screen,text=\"1.Did you experience a sore or dirty\",width=\"300\", height=\"1\",font=(\"Calibri\", 9)).pack()\n",
    "    Label(journal_screen,text=\"throat this morning?(YES/NO)\", height=\"1\",font=(\"Calibri\", 9)).pack()\n",
    "    Label(journal_screen,text=\"\").pack()\n",
    "    today = str(date.today())\n",
    "    var1=StringVar()\n",
    "    a=Entry(journal_screen, textvariable=var1)\n",
    "    a.pack()\n",
    "    Label(journal_screen,text=\"\").pack()\n",
    "    Label(journal_screen,text=\"2.Were you feeling drowsy and tried\",width=\"300\", height=\"1\",font=(\"Calibri\", 9)).pack()\n",
    "    Label(journal_screen,text=\"throughout the day?(YES/NO)\", height=\"1\",font=(\"Calibri\", 9)).pack()\n",
    "    Label(journal_screen,text=\"\").pack()\n",
    "    \n",
    "    \n",
    "    var2=StringVar()\n",
    "    b=Entry(journal_screen, textvariable=var2)\n",
    "    b.pack()\n",
    "    Label(journal_screen,text=\"\").pack()\n",
    "    Label(journal_screen,text=\"3.Tell us any other symptoms\", height=\"1\",font=(\"Calibri\", 9)).pack()\n",
    "    Label(journal_screen,text=\"\").pack()\n",
    "    var3=StringVar()\n",
    "    c=Entry(journal_screen, textvariable=var3)\n",
    "    c.pack()\n",
    "    Label(journal_screen,text=\"\").pack()\n",
    "    Label(journal_screen,text=\"\").pack()\n",
    "    Label(journal_screen,text=\"\").pack()\n",
    "    Button(journal_screen, text=\"Submit\", width=10, height=1, bg=\"green\", fg=\"white\",command = journal_submit_screen).pack()\n",
    "    Label(journal_screen,text=\"\").pack()\n",
    "    Label(journal_screen,text=\"\").pack()\n",
    "    Button(journal_screen, text=\"Back\", width=10, height=1, bg=\"black\", fg=\"white\",command = delete_journal_screen).pack()\n",
    "\n",
    "def delete_journal_screen():\n",
    "    journal_screen.destroy()\n",
    "def journal_submit_screen():\n",
    "    resp1=var1.get()\n",
    "    resp2=var2.get()\n",
    "    resp3=var3.get()\n",
    "    file = open(username1+\" journal\", \"w\")\n",
    "    file.write(\"Health Jounral Responses \"+ today+\"\\n\")\n",
    "    \n",
    "    a.delete(0,END)\n",
    "    b.delete(0,END)\n",
    "    c.delete(0,END)\n",
    " \n",
    "    Label(journal_screen, text=\"Success!\", fg=\"green\", font=(\"calibri\", 11,\"bold\")).pack()\n",
    "    \n",
    "        \n",
    "    file.write('Fisrt respose : '+resp1 + \"\\n\")\n",
    "    file.write('Second respose : '+resp2 + \"\\n\")\n",
    "    file.write('Third respose : '+resp3 + \"\\n\")\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "def cchat(event):\n",
    "    output.config(text=\"BOT: I am  your Personal Digital Assistant. What can I do for you!\")\n",
    "    output.config(text=\"BOT : Hey mate. How can I help you?\")\n",
    "    #bot_speaking(\"Hey mate how can I help you?\")\n",
    "        #user_text = user_input.get()\n",
    "\n",
    "    inp=user_input.get()  #input(\"YOU : \")\n",
    " \n",
    "        \n",
    "    results = model.predict([bow(inp, words)])[0]\n",
    "    results_index=numpy.argmax(results)\n",
    "    tag=classes[results_index]\n",
    "    if(results[results_index]>0.65):\n",
    "        for tg in intents['intents']:\n",
    "            if tg['tag']==tag:\n",
    "                response=tg['responses']\n",
    "        ms=random.choice(response)\n",
    "\n",
    "        output.config(text=\"BOT : \" +ms)\n",
    "        bot_speaking(ms)\n",
    "    else:\n",
    "        output.config(text=\"BOT :  I did not understand you! Try again\")\n",
    "        bot_speaking(\"I did not understand you! Try again\")\n",
    "    \n",
    "def cb():\n",
    "    Label(chatbot_screen,text=\"Honie speaking :D\",width='20',height=\"1\", font=(\"Calibri\", 9)).pack()\n",
    "    \n",
    "    #Label(chatbot_screen,text=\"BOT : Hey mate. How can I help you?\",height=\"1\", font=(\"Calibri\", 9)).pack()\n",
    "    output.config(text=\"BOT: I am  your Personal Digital Assistant. What can I do for you!\")\n",
    "    output.config(text=\"BOT : Hey mate. How can I help you?\")\n",
    "    bot_speaking(\"Hey mate how can I help you?\")\n",
    "    \n",
    "        #user_text = user_input.get()\n",
    "\n",
    "    inp=str(get_input())   #input(\"YOU : \")\n",
    "    \n",
    "    results = model.predict([bow(inp, words)])[0]\n",
    "    results_index=numpy.argmax(results)\n",
    "    tag=classes[results_index]\n",
    "    if(results[results_index]>0.65):\n",
    "        for tg in intents['intents']:\n",
    "            if tg['tag']==tag:\n",
    "                response=tg['responses']\n",
    "        ms=random.choice(response)\n",
    "\n",
    "        output.config(text=\"BOT : \" +ms)\n",
    "        bot_speaking(ms)\n",
    "    else:\n",
    "        output.config(text=\"BOT :  I did not understand you! Try again\")\n",
    "        bot_speaking(\"I did not understand you! Try again\")\n",
    "    \n",
    "def chatbot():\n",
    "    global chatbot_screen\n",
    "    chatbot_screen = Toplevel(home_screen)\n",
    "    chatbot_screen.title(\"BOT\")\n",
    "    chatbot_screen.geometry(\"320x550+400+50\")\n",
    "    chatbot_screen.configure(background='white')\n",
    "    img1  = Image.open(\"C:\\\\Users\\\\kumar\\\\a1.PROJECTS\\\\DocMan_MLR\\\\DocMan\\\\bg.png\") \n",
    "    img = img1.resize((700, 250), Image.ANTIALIAS)\n",
    "    photo=ImageTk.PhotoImage(img)\n",
    "    lab=Label(image=photo).pack()\n",
    "    Label(chatbot_screen,text=\"Chat Assistant\", bg=\"white\",fg=\"#cf2929\" ,width=\"300\", height=\"2\", font=(\"Calibri\", 18)).pack()\n",
    "    Label(chatbot_screen,text=\"\").pack()\n",
    "    #.place(x=200,y=30)\n",
    "    Label(chatbot_screen,text=\"Any queires? Ask me : \", bg=\"white\",fg=\"black\" ,width=\"200\", height=\"1\", font=(\"Calibri\", 13)).pack()\n",
    "    Label(chatbot_screen,text=\"\").pack()\n",
    "    \n",
    "    global user_input\n",
    "    global greetings\n",
    "    global question\n",
    "    global responses\n",
    "    global huh\n",
    "    global output\n",
    "    \n",
    "    \n",
    "    user_input = tk.Entry(chatbot_screen)\n",
    "    user_input.pack()\n",
    "    \n",
    "    #greetings = ['hola', 'hello', 'hi', 'Hi', 'hey!', 'hey']\n",
    "    #question = ['How are you?', 'How are you doing?']\n",
    "    #responses = ['Okay', \"I'm fine\"]\n",
    "    huh = \"I did not understand what you said\"\n",
    "    user_input.bind(\"<Return>\", cchat)\n",
    "    Label(chatbot_screen,text=\"\").pack()\n",
    "    Label(chatbot_screen,text=\"\").pack()\n",
    "    output = tk.Label(chatbot_screen, text='')\n",
    "    output.pack()\n",
    "    Label(chatbot_screen,text=\"\").pack()\n",
    "    Button(chatbot_screen, text=\"Speak with bot..\", width=20, height=1, bg=\"black\", fg=\"white\",command = cb).pack()\n",
    "    Label(chatbot_screen,text=\"\").pack()\n",
    "    Label(chatbot_screen,text=\"\").pack()\n",
    "    Label(chatbot_screen,text=\"\").pack()\n",
    "    Button(chatbot_screen, text=\"Back\", width=10, height=1, bg=\"black\", fg=\"white\",command = delete_chatbot_screen).pack()\n",
    "\n",
    "def delete_chatbot_screen():\n",
    "    chatbot_screen.destroy()\n",
    "\n",
    "\n",
    "def delete_root():\n",
    "    root.destroy()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    global main_screen\n",
    "    global root\n",
    "    global current_id\n",
    "    \n",
    "    root = Tk() \n",
    "    root.geometry(\"320x550+400+50\")\n",
    "    root.title(\"Smart Diagnostic Centre\") \n",
    "      \n",
    "    # set the background colour of GUI window \n",
    "    root.configure(background='white')\n",
    "    img1  = Image.open(\"C:\\\\Users\\\\kumar\\\\a1.PROJECTS\\\\SCHEDULING ALGO\\\\IMAGES\\\\main.jpg\") \n",
    "    img = img1.resize((700, 250), Image.ANTIALIAS)\n",
    "    photo=ImageTk.PhotoImage(img)\n",
    "    lab=Label(image=photo).pack()#.place(x=200,y=30)\n",
    "    Label(text=\"Select Your Choice\", bg=\"white\", width=\"300\", height=\"2\", font=(\"Calibri\", 13)).pack()\n",
    "    Label(text=\"\").pack()\n",
    "    Button(text=\"Login\", height=\"2\", width=\"30\",font=\"bold 10\", command = login).pack()\n",
    "    Label(text=\"\").pack()\n",
    "    Button(text=\"Register\", height=\"2\", width=\"30\", font=\"bold 10\",command=register).pack()\n",
    "    Label(text=\"\").pack()\n",
    "    Label(text=\"\").pack()\n",
    "    Button(text=\"Close\", height=\"2\", width=\"30\",bg=\"black\",fg=\"white\",font=\"bold 10\",command=delete_root).pack()\n",
    "  \n",
    "    \n",
    "    root.mainloop()\n",
    "    \n",
    "    #root=Tk()\n",
    "    #root.mainloop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
